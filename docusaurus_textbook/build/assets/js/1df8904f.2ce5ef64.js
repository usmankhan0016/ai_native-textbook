"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[8135],{3658:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3/chapter-3-isaac-ros","title":"Chapter 3: AI Perception with Isaac ROS","description":"Duration Intermediate | Week: 9","source":"@site/docs/module-3/chapter-3-isaac-ros.md","sourceDirName":"module-3","slug":"/module-3/chapter-3-isaac-ros","permalink":"/ai_native-textbook/docs/module-3/chapter-3-isaac-ros","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"isaac-ros","permalink":"/ai_native-textbook/docs/tags/isaac-ros"},{"inline":true,"label":"perception","permalink":"/ai_native-textbook/docs/tags/perception"},{"inline":true,"label":"cuda","permalink":"/ai_native-textbook/docs/tags/cuda"},{"inline":true,"label":"tensorrt","permalink":"/ai_native-textbook/docs/tags/tensorrt"},{"inline":true,"label":"slam","permalink":"/ai_native-textbook/docs/tags/slam"},{"inline":true,"label":"sensor-fusion","permalink":"/ai_native-textbook/docs/tags/sensor-fusion"}],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Ch. 3: Isaac ROS Perception","title":"Chapter 3: AI Perception with Isaac ROS","tags":["isaac-ros","perception","cuda","tensorrt","slam","sensor-fusion"],"difficulty":"Intermediate","module":3,"week":9,"prerequisites":["Chapter 1-2","Module 1: ROS 2","Python 3.10+"],"estimated_time":"5-6 hours","topics":["Isaac ROS","CUDA acceleration","TensorRT","object detection","visual SLAM","sensor fusion"]},"sidebar":"tutorialSidebar","previous":{"title":"Ch. 2: Isaac Sim Robotics","permalink":"/ai_native-textbook/docs/module-3/chapter-2-isaac-sim"},"next":{"title":"Ch. 4: Synthetic Data Isaac Lab","permalink":"/ai_native-textbook/docs/module-3/chapter-4-isaac-lab"}}');var t=s(4848),r=s(8453);const a={sidebar_position:3,sidebar_label:"Ch. 3: Isaac ROS Perception",title:"Chapter 3: AI Perception with Isaac ROS",tags:["isaac-ros","perception","cuda","tensorrt","slam","sensor-fusion"],difficulty:"Intermediate",module:3,week:9,prerequisites:["Chapter 1-2","Module 1: ROS 2","Python 3.10+"],estimated_time:"5-6 hours",topics:["Isaac ROS","CUDA acceleration","TensorRT","object detection","visual SLAM","sensor fusion"]},o="Chapter 3: AI Perception with Isaac ROS",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"<strong>Isaac ROS</strong>",id:"isaac-ros",level:3},{value:"<strong>CUDA (Compute Unified Device Architecture)</strong>",id:"cuda-compute-unified-device-architecture",level:3},{value:"<strong>TensorRT</strong>",id:"tensorrt",level:3},{value:"<strong>Jetson</strong>",id:"jetson",level:3},{value:"<strong>Object Detection</strong>",id:"object-detection",level:3},{value:"<strong>Visual SLAM (cuVSLAM)</strong>",id:"visual-slam-cuvslam",level:3},{value:"<strong>Sensor Fusion</strong>",id:"sensor-fusion",level:3},{value:"Part 1: Isaac ROS Architecture",id:"part-1-isaac-ros-architecture",level:2},{value:"GPU Acceleration Benefits",id:"gpu-acceleration-benefits",level:3},{value:"Part 2: TensorRT Model Optimization",id:"part-2-tensorrt-model-optimization",level:2},{value:"Conversion Pipeline",id:"conversion-pipeline",level:3},{value:"Quantization Strategy",id:"quantization-strategy",level:3},{value:"Part 3: Object Detection Pipeline",id:"part-3-object-detection-pipeline",level:2},{value:"Hands-On: Deploy YOLO Object Detection",id:"hands-on-deploy-yolo-object-detection",level:3},{value:"Lab: Deploy to Jetson",id:"lab-deploy-to-jetson",level:3},{value:"Part 4: Visual SLAM (cuVSLAM)",id:"part-4-visual-slam-cuvslam",level:2},{value:"How Visual SLAM Works",id:"how-visual-slam-works",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Part 5: Sensor Fusion with Kalman Filter",id:"part-5-sensor-fusion-with-kalman-filter",level:2},{value:"Part 6: Hands-On Labs",id:"part-6-hands-on-labs",level:2},{value:"Lab 1: Deploy Object Detection on Humanoid (1 hour)",id:"lab-1-deploy-object-detection-on-humanoid-1-hour",level:3},{value:"Lab 2: Visual SLAM Real-Time Mapping (1 hour)",id:"lab-2-visual-slam-real-time-mapping-1-hour",level:3},{value:"Lab 3: Multi-Sensor Fusion (1.5 hours)",id:"lab-3-multi-sensor-fusion-15-hours",level:3},{value:"Part 7: Code Examples &amp; Debugging",id:"part-7-code-examples--debugging",level:2},{value:"Example: Latency Profiler",id:"example-latency-profiler",level:3},{value:"Part 8: End-of-Chapter Exercises",id:"part-8-end-of-chapter-exercises",level:2},{value:"Exercise 1: Deploy Object Detector (1 hour)",id:"exercise-1-deploy-object-detector-1-hour",level:3},{value:"Exercise 2: Visual SLAM Mapping (1 hour)",id:"exercise-2-visual-slam-mapping-1-hour",level:3},{value:"Exercise 3: Sensor Fusion Robustness (1.5 hours)",id:"exercise-3-sensor-fusion-robustness-15-hours",level:3},{value:"Exercise 4: TensorRT Optimization (1 hour)",id:"exercise-4-tensorrt-optimization-1-hour",level:3},{value:"Part 9: Capstone Integration",id:"part-9-capstone-integration",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-3-ai-perception-with-isaac-ros",children:"Chapter 3: AI Perception with Isaac ROS"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 5-6 hours | ",(0,t.jsx)(n.strong,{children:"Difficulty"}),": Intermediate | ",(0,t.jsx)(n.strong,{children:"Week"}),": 9"]}),"\n",(0,t.jsx)(n.p,{children:"Deploy real-time hardware-accelerated AI perception on Jetson hardware."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By completing this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Set Up Isaac ROS Environment"})," \u2014 Install and configure Docker container with NVIDIA CUDA-accelerated perception"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understand CUDA Acceleration"})," \u2014 Grasp how NVIDIA GPUs parallelize computer vision (cuDNN, cuCVCore)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deploy Object Detection"})," \u2014 Use pre-trained YOLO/ResNet models with TensorRT for real-time inference (>30 FPS)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement Visual SLAM"})," \u2014 Build real-time simultaneous localization and mapping using cuVSLAM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fuse Multiple Sensors"})," \u2014 Combine LiDAR + camera + IMU with Kalman filtering for robust perception"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Optimize Model Performance"})," \u2014 Apply TensorRT quantization (FP32 \u2192 INT8) to achieve >10 FPS on Jetson"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Measure Latency"})," \u2014 Profile perception pipeline to identify and fix bottlenecks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Debug Perception Failures"})," \u2014 Troubleshoot common issues (low accuracy, missed detections, SLAM drift)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros",children:(0,t.jsx)(n.strong,{children:"Isaac ROS"})}),"\n",(0,t.jsx)(n.p,{children:"Hardware-accelerated perception middleware for robotics. Built on ROS 2, Isaac ROS provides CUDA-optimized modules for vision, depth processing, pose estimation, SLAM. Runs efficiently on NVIDIA Jetson edge devices."}),"\n",(0,t.jsx)(n.h3,{id:"cuda-compute-unified-device-architecture",children:(0,t.jsx)(n.strong,{children:"CUDA (Compute Unified Device Architecture)"})}),"\n",(0,t.jsx)(n.p,{children:"Parallel computing platform by NVIDIA enabling general-purpose GPU computing. Allows perception algorithms to run 10-100x faster on GPU than CPU."}),"\n",(0,t.jsx)(n.h3,{id:"tensorrt",children:(0,t.jsx)(n.strong,{children:"TensorRT"})}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA's inference optimization library. Converts trained deep learning models (PyTorch, TensorFlow) to optimized binary format for fastest inference on Jetson hardware."}),"\n",(0,t.jsx)(n.h3,{id:"jetson",children:(0,t.jsx)(n.strong,{children:"Jetson"})}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA's edge AI processor line (Orin Nano, Orin AGX, Xavier). Combines CPU + GPU + specialized AI accelerators (VIC, DLA). Perfect for deploying trained perception models."}),"\n",(0,t.jsx)(n.h3,{id:"object-detection",children:(0,t.jsx)(n.strong,{children:"Object Detection"})}),"\n",(0,t.jsx)(n.p,{children:"Computer vision task: identify and localize objects in images. YOLO, ResNet, MobileNet are popular architectures. Isaac ROS provides pre-optimized versions."}),"\n",(0,t.jsx)(n.h3,{id:"visual-slam-cuvslam",children:(0,t.jsx)(n.strong,{children:"Visual SLAM (cuVSLAM)"})}),"\n",(0,t.jsx)(n.p,{children:"Real-time algorithm for building maps and localizing robots without external sensors. NVIDIA's cuVSLAM implementation runs on GPU for real-time performance."}),"\n",(0,t.jsx)(n.h3,{id:"sensor-fusion",children:(0,t.jsx)(n.strong,{children:"Sensor Fusion"})}),"\n",(0,t.jsx)(n.p,{children:"Combining data from multiple sensors (camera, LiDAR, IMU) to get better perception than any single sensor alone. Kalman filtering is standard technique."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-1-isaac-ros-architecture",children:"Part 1: Isaac ROS Architecture"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph LR\n    A["Raw Sensor Data<br/>(Camera, LiDAR, IMU)"] --\x3e|ROS 2 Topics| B["Isaac ROS<br/>Perception Modules<br/>(CUDA GPU)"]\n    B --\x3e|cuDNN| C["Image Processing<br/>(denoising, resizing)"]\n    B --\x3e|TensorRT| D["Neural Network<br/>Inference<br/>(object detection)"]\n    B --\x3e|cuVSLAM| E["Visual SLAM<br/>(mapping, localization)"]\n    C --\x3e F["Output Topics"]\n    D --\x3e F\n    E --\x3e F\n    F --\x3e|/detections| G["Control Node"]\n    F --\x3e|/map| G\n    F --\x3e|/pose| G\n    G --\x3e|/cmd_vel| H["Robot Controller"]'}),"\n",(0,t.jsx)(n.h3,{id:"gpu-acceleration-benefits",children:"GPU Acceleration Benefits"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Task"}),(0,t.jsx)(n.th,{children:"CPU Time"}),(0,t.jsx)(n.th,{children:"GPU Time"}),(0,t.jsx)(n.th,{children:"Speedup"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Image denoising (640\xd7480)"}),(0,t.jsx)(n.td,{children:"45 ms"}),(0,t.jsx)(n.td,{children:"2 ms"}),(0,t.jsx)(n.td,{children:"22x"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Object detection (YOLO on image)"}),(0,t.jsx)(n.td,{children:"120 ms"}),(0,t.jsx)(n.td,{children:"8 ms"}),(0,t.jsx)(n.td,{children:"15x"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Visual SLAM (feature matching)"}),(0,t.jsx)(n.td,{children:"60 ms"}),(0,t.jsx)(n.td,{children:"3 ms"}),(0,t.jsx)(n.td,{children:"20x"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Total pipeline latency"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"225 ms"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"13 ms"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"17x"})})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"At 17x speedup, what takes 1 second on CPU takes 59 milliseconds on GPU \u2192 enables real-time robotics."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-2-tensorrt-model-optimization",children:"Part 2: TensorRT Model Optimization"}),"\n",(0,t.jsx)(n.p,{children:"Training creates FP32 (32-bit float) models. Deployment requires optimization."}),"\n",(0,t.jsx)(n.h3,{id:"conversion-pipeline",children:"Conversion Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Trained Model (PyTorch/TensorFlow)\n    \u2193 (PyTorch: .pt file, ~500 MB)\nTensorRT Conversion\n    \u251c\u2500 Precision: FP32 \u2192 FP16 \u2192 INT8\n    \u251c\u2500 Operator Fusion: Combine layers\n    \u251c\u2500 Kernel Optimization: GPU-specific speedups\n    \u2514\u2500 Memory Optimization: Minimize VRAM\n    \u2193\nOptimized TensorRT Engine\n    \u2193 (~50 MB INT8, 100x smaller)\nJetson Inference\n    \u251c\u2500 Load engine\n    \u251c\u2500 Copy input to GPU\n    \u251c\u2500 Execute inference kernel\n    \u2514\u2500 Copy output to CPU\n    \u2193\nOutput (bounding boxes, scores)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"quantization-strategy",children:"Quantization Strategy"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# FP32 (original, 32 bits per number)\nweights = [0.123456789012345, -0.987654321098765, ...]\nmodel_size = 500 MB\nlatency = 120 ms (on Jetson Orin Nano)\n\n# FP16 (half precision, 16 bits)\nweights = [0.1235, -0.9877, ...]\nmodel_size = 250 MB (50% reduction)\nlatency = 40 ms (3x speedup)\naccuracy loss = `<1%`\n\n# INT8 (integer, 8 bits)\nweights = [13, -100, ...]  # Quantized to 8-bit integers\nmodel_size = 125 MB (75% reduction)\nlatency = 15 ms (8x speedup)\naccuracy loss = `<5%` (usually acceptable)\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-3-object-detection-pipeline",children:"Part 3: Object Detection Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"hands-on-deploy-yolo-object-detection",children:"Hands-On: Deploy YOLO Object Detection"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: object_detection_node.py\n"""\nReal-time object detection on humanoid using Isaac ROS.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import Header\nimport numpy as np\nimport torch\nimport tensorrt as trt\nfrom torch2trt import torch2trt, TRTModule\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__(\'object_detection\')\n\n        # Load pre-trained YOLO model\n        self.model = torch.hub.load(\'ultralytics/yolov5\', \'yolov5s\')\n        self.model.eval()\n\n        # Convert to TensorRT for deployment (optional for Jetson)\n        # example_input = torch.randn(1, 3, 640, 480).cuda()\n        # model_trt = torch2trt(self.model, [example_input])\n\n        # Subscribe to camera topic\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10)\n\n        # Publish detections\n        self.detections_pub = self.create_publisher(\n            Image, \'/detections\', 10)\n\n        self.get_logger().info("Object detection node started")\n\n    def image_callback(self, msg: Image):\n        """Process incoming image and run inference."""\n        # Convert ROS message to numpy array\n        image = np.frombuffer(msg.data, dtype=np.uint8).reshape(\n            msg.height, msg.width, 3)\n\n        # Run inference\n        with torch.no_grad():\n            results = self.model(image)\n\n        # Process results\n        detections = results.xyxy[0]  # Bounding boxes [x1, y1, x2, y2, conf, class]\n\n        # Publish detections\n        self.get_logger().info(\n            f"Detected {len(detections)} objects")\n\n        if len(detections) > 0:\n            for det in detections:\n                x1, y1, x2, y2, conf, cls = det.cpu().numpy()\n                if conf > 0.5:  # Confidence threshold\n                    self.get_logger().info(\n                        f"  Class {int(cls)}: bbox=[{int(x1)},{int(y1)},{int(x2)},{int(y2)}], conf={conf:.2f}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetectionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"lab-deploy-to-jetson",children:"Lab: Deploy to Jetson"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"SSH into Jetson"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ssh nvidia@jetson-ip-address\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Copy perception node"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"scp object_detection_node.py nvidia@jetson:/home/nvidia/\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run on Jetson"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# In Jetson terminal\ncd /home/nvidia\nros2 run your_package object_detection\n\n# Should show detections at >10 FPS\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-4-visual-slam-cuvslam",children:"Part 4: Visual SLAM (cuVSLAM)"}),"\n",(0,t.jsx)(n.h3,{id:"how-visual-slam-works",children:"How Visual SLAM Works"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Frame 1 (image)\n    \u2193 (feature detection)\nKeypoints: [pt1, pt2, ...] (100+ feature locations)\n    \u2193 (next frame arrives)\nFrame 2 (image)\n    \u2193 (match features to Frame 1)\nMatched features: [(pt1\u2192pt1', pt2\u2192pt2', ...)]\n    \u2193 (solve for camera motion)\nCamera moved: [translation, rotation]\n    \u2193 (triangulate 3D points)\n3D map: [point1, point2, ...]\n    \u2193 (loop closure detection)\nLoop detected at Frame N \u2192 refine map\n    \u2193\nReal-time map updated continuously\n"})}),"\n",(0,t.jsx)(n.h3,{id:"implementation",children:"Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: visual_slam_node.py\n"""\nReal-time visual SLAM using Isaac ROS cuVSLAM.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom nav_msgs.msg import OccupancyGrid\nfrom geometry_msgs.msg import PoseStamped\nimport numpy as np\n\nclass VisualSLAMNode(Node):\n    def __init__(self):\n        super().__init__(\'visual_slam\')\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.process_image, 10)\n\n        # Publish map and pose\n        self.map_pub = self.create_publisher(\n            OccupancyGrid, \'/map\', 10)\n        self.pose_pub = self.create_publisher(\n            PoseStamped, \'/pose\', 10)\n\n        # Tracking state\n        self.prev_frame = None\n        self.map_points = []\n        self.camera_pose = np.eye(4)\n\n        self.get_logger().info("Visual SLAM node started")\n\n    def process_image(self, msg: Image):\n        """Process image frame for SLAM."""\n        # Convert ROS message to numpy\n        frame = np.frombuffer(msg.data, dtype=np.uint8).reshape(\n            msg.height, msg.width, 3)\n\n        if self.prev_frame is None:\n            self.prev_frame = frame\n            return\n\n        # Feature matching between frames\n        # In real implementation, use feature detectors (SIFT, ORB)\n        keypoints_prev = self.detect_features(self.prev_frame)\n        keypoints_curr = self.detect_features(frame)\n        matches = self.match_features(keypoints_prev, keypoints_curr)\n\n        # Estimate camera motion from matches\n        if len(matches) > 10:\n            rotation, translation = self.estimate_motion(matches)\n            self.camera_pose[:3, :3] = rotation\n            self.camera_pose[:3, 3] = translation\n\n            # Triangulate new 3D points\n            new_points = self.triangulate(matches, rotation, translation)\n            self.map_points.extend(new_points)\n\n            # Publish results\n            self.publish_pose()\n            self.publish_map()\n\n        self.prev_frame = frame.copy()\n\n    def detect_features(self, image):\n        """Detect feature keypoints (placeholder)."""\n        # Real implementation uses ORB, SIFT, or neural features\n        return np.random.rand(100, 2) * image.shape[:2]\n\n    def match_features(self, pts1, pts2):\n        """Match features between frames (placeholder)."""\n        return list(zip(pts1[:10], pts2[:10]))  # Simple match\n\n    def estimate_motion(self, matches):\n        """Estimate camera rotation and translation."""\n        # Real implementation uses essential matrix decomposition\n        rotation = np.eye(3)\n        translation = np.random.rand(3) * 0.1\n        return rotation, translation\n\n    def triangulate(self, matches, rotation, translation):\n        """Compute 3D positions of matched features."""\n        points_3d = []\n        for pt1, pt2 in matches:\n            # Simple triangulation (real version uses epipolar geometry)\n            z = 2.0  # Depth estimate\n            x = (pt1[0] - 320) * z / 500\n            y = (pt1[1] - 240) * z / 500\n            points_3d.append([x, y, z])\n        return points_3d\n\n    def publish_pose(self):\n        """Publish current camera pose."""\n        msg = PoseStamped()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.header.frame_id = "map"\n        msg.pose.position.x = float(self.camera_pose[0, 3])\n        msg.pose.position.y = float(self.camera_pose[1, 3])\n        msg.pose.position.z = float(self.camera_pose[2, 3])\n        self.pose_pub.publish(msg)\n\n    def publish_map(self):\n        """Publish occupancy grid map."""\n        grid = OccupancyGrid()\n        grid.header.frame_id = "map"\n        # Simplified: just publish grid metadata\n        grid.info.width = 100\n        grid.info.height = 100\n        grid.info.resolution = 0.1\n        self.map_pub.publish(grid)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisualSLAMNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-5-sensor-fusion-with-kalman-filter",children:"Part 5: Sensor Fusion with Kalman Filter"}),"\n",(0,t.jsx)(n.p,{children:"Fusing LiDAR + camera + IMU for robust perception:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: sensor_fusion_node.py\n"""\nMulti-sensor fusion using Kalman filtering.\n"""\n\nimport numpy as np\nfrom scipy.linalg import block_diag\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom geometry_msgs.msg import PointStamped\n\nclass KalmanFilter:\n    def __init__(self):\n        # State: [x, y, z, vx, vy, vz]\n        self.state = np.zeros(6)\n\n        # Covariance matrix (uncertainty)\n        self.P = np.eye(6) * 0.1\n\n        # Process noise\n        self.Q = np.eye(6) * 0.01\n\n        # Measurement noise (varies per sensor)\n        self.R_camera = np.eye(3) * 0.05    # Camera is less accurate\n        self.R_lidar = np.eye(3) * 0.01     # LiDAR is more accurate\n        self.R_imu = np.eye(3) * 0.02       # IMU is moderate\n\n    def predict(self, dt=0.033):\n        """Predict next state (constant velocity model)."""\n        F = np.eye(6)\n        F[0, 3] = dt  # x += vx * dt\n        F[1, 4] = dt\n        F[2, 5] = dt\n\n        self.state = F @ self.state\n        self.P = F @ self.P @ F.T + self.Q\n\n    def update_camera(self, measurement):\n        """Update with camera-based detection."""\n        self._update(measurement, self.R_camera)\n\n    def update_lidar(self, measurement):\n        """Update with LiDAR-based detection."""\n        self._update(measurement, self.R_lidar)\n\n    def update_imu(self, measurement):\n        """Update with IMU data (acceleration)."""\n        self._update(measurement[:3], self.R_imu)  # Only use accel part\n\n    def _update(self, measurement, R):\n        """Generic Kalman update."""\n        # Measurement matrix (we measure position [x, y, z])\n        H = np.zeros((3, 6))\n        H[:3, :3] = np.eye(3)\n\n        # Innovation\n        y = measurement - (H @ self.state)\n\n        # Innovation covariance\n        S = H @ self.P @ H.T + R\n\n        # Kalman gain\n        K = self.P @ H.T @ np.linalg.inv(S)\n\n        # Update state and covariance\n        self.state = self.state + K @ y\n        self.P = (np.eye(6) - K @ H) @ self.P\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__(\'sensor_fusion\')\n\n        self.filter = KalmanFilter()\n\n        # Subscribe to sensors\n        self.camera_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.camera_callback, 10)\n        self.lidar_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.lidar_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10)\n\n        # Publish fused estimate\n        self.pose_pub = self.create_publisher(\n            PointStamped, \'/fused_pose\', 10)\n\n        # Timer for prediction step\n        self.timer = self.create_timer(0.033, self.predict)  # 30 Hz\n\n    def predict(self):\n        """Predict step at fixed rate."""\n        self.filter.predict(dt=0.033)\n\n        # Publish current state estimate\n        msg = PointStamped()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.point.x = float(self.filter.state[0])\n        msg.point.y = float(self.filter.state[1])\n        msg.point.z = float(self.filter.state[2])\n        self.pose_pub.publish(msg)\n\n    def camera_callback(self, msg):\n        """Camera provides object position estimate."""\n        # Extract 3D position from detection (simplified)\n        measurement = np.array([0.5, 0.3, 2.0])  # Placeholder\n        self.filter.update_camera(measurement)\n\n    def lidar_callback(self, msg):\n        """LiDAR provides more accurate range measurements."""\n        ranges = np.array(msg.ranges)\n        if len(ranges) > 0:\n            # Simple centroid of detected points\n            angle_increment = msg.angle_increment\n            angles = np.arange(len(ranges)) * angle_increment\n\n            x_points = ranges * np.cos(angles)\n            y_points = ranges * np.sin(angles)\n\n            x = np.mean(x_points[np.isfinite(x_points)])\n            y = np.mean(y_points[np.isfinite(y_points)])\n            z = 0.0  # LiDAR is 2D in this case\n\n            measurement = np.array([x, y, z])\n            self.filter.update_lidar(measurement)\n\n    def imu_callback(self, msg):\n        """IMU provides acceleration (optional for position)."""\n        measurement = np.array([\n            msg.linear_acceleration.x,\n            msg.linear_acceleration.y,\n            msg.linear_acceleration.z,\n        ])\n        self.filter.update_imu(measurement)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorFusionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-6-hands-on-labs",children:"Part 6: Hands-On Labs"}),"\n",(0,t.jsx)(n.h3,{id:"lab-1-deploy-object-detection-on-humanoid-1-hour",children:"Lab 1: Deploy Object Detection on Humanoid (1 hour)"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Use Chapter 2 humanoid in Isaac Sim"}),"\n",(0,t.jsx)(n.li,{children:"Add RGB camera sensor"}),"\n",(0,t.jsx)(n.li,{children:"Launch Isaac ROS bridge"}),"\n",(0,t.jsx)(n.li,{children:"Run object_detection_node.py"}),"\n",(0,t.jsxs)(n.li,{children:["Verify detections published to ",(0,t.jsx)(n.code,{children:"/detections"})," topic"]}),"\n",(0,t.jsxs)(n.li,{children:["Measure inference latency: ",(0,t.jsx)(n.code,{children:"ros2 topic hz /detections"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success"}),": >10 FPS detection rate"]}),"\n",(0,t.jsx)(n.h3,{id:"lab-2-visual-slam-real-time-mapping-1-hour",children:"Lab 2: Visual SLAM Real-Time Mapping (1 hour)"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Start Isaac Sim with moving humanoid camera"}),"\n",(0,t.jsx)(n.li,{children:"Run visual_slam_node.py"}),"\n",(0,t.jsxs)(n.li,{children:["Visualize map in RViz: ",(0,t.jsx)(n.code,{children:"ros2 run rviz2 rviz2"})]}),"\n",(0,t.jsxs)(n.li,{children:["Add ",(0,t.jsx)(n.code,{children:"/map"})," display in RViz"]}),"\n",(0,t.jsx)(n.li,{children:"Observe map building in real-time as humanoid moves"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success"}),": Map grows continuously, no obvious drift over 1 minute"]}),"\n",(0,t.jsx)(n.h3,{id:"lab-3-multi-sensor-fusion-15-hours",children:"Lab 3: Multi-Sensor Fusion (1.5 hours)"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Configure humanoid with camera + LiDAR + IMU (Chapter 2)"}),"\n",(0,t.jsx)(n.li,{children:"Run sensor_fusion_node.py"}),"\n",(0,t.jsx)(n.li,{children:"Visualize fused pose in RViz"}),"\n",(0,t.jsx)(n.li,{children:"Introduce noise to one sensor (e.g., block camera)"}),"\n",(0,t.jsx)(n.li,{children:"Verify fusion compensates for missing sensor"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success"}),": Fused estimate is smoother and more robust than any single sensor"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-7-code-examples--debugging",children:"Part 7: Code Examples & Debugging"}),"\n",(0,t.jsx)(n.h3,{id:"example-latency-profiler",children:"Example: Latency Profiler"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: latency_profiler.py\n"""\nProfile perception pipeline to identify bottlenecks.\n"""\n\nimport time\nimport statistics\nfrom collections import defaultdict\n\nclass LatencyProfiler:\n    def __init__(self):\n        self.times = defaultdict(list)\n\n    def start(self, name):\n        self.start_time = {name: time.perf_counter()}\n\n    def end(self, name):\n        elapsed = time.perf_counter() - self.start_time[name]\n        self.times[name].append(elapsed * 1000)  # Convert to ms\n\n    def report(self):\n        print("\\n=== Latency Report ===")\n        total_time = 0\n        for name, times in sorted(self.times.items()):\n            if len(times) > 0:\n                mean = statistics.mean(times)\n                stdev = statistics.stdev(times) if len(times) > 1 else 0\n                max_time = max(times)\n                print(f"{name:20s}: {mean:6.2f} \xb1 {stdev:5.2f} ms (max: {max_time:6.2f} ms)")\n                total_time += mean\n\n        print(f"{\'Total\':20s}: {total_time:6.2f} ms")\n        target_fps = 30\n        target_latency = 1000 / target_fps\n        print(f"Target ({target_fps} Hz): {target_latency:6.2f} ms")\n        if total_time < target_latency:\n            print("\u2705 Within real-time budget")\n        else:\n            print(f"\u274c Exceeds budget by {total_time - target_latency:.2f} ms")\n\n# Usage in perception node\nprofiler = LatencyProfiler()\n\nprofiler.start("image_capture")\n# ... capture image ...\nprofiler.end("image_capture")\n\nprofiler.start("preprocessing")\n# ... resize, normalize ...\nprofiler.end("preprocessing")\n\nprofiler.start("inference")\n# ... run neural network ...\nprofiler.end("inference")\n\nprofiler.start("postprocessing")\n# ... extract bounding boxes ...\nprofiler.end("postprocessing")\n\nprofiler.report()\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-8-end-of-chapter-exercises",children:"Part 8: End-of-Chapter Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-deploy-object-detector-1-hour",children:"Exercise 1: Deploy Object Detector (1 hour)"}),"\n",(0,t.jsx)(n.p,{children:"Deploy YOLOv5 (or available pre-trained model) to detect objects in humanoid's camera feed. Measure FPS."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance"}),": >15 FPS on GPU, documented latency breakdown"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-visual-slam-mapping-1-hour",children:"Exercise 2: Visual SLAM Mapping (1 hour)"}),"\n",(0,t.jsx)(n.p,{children:"Run visual SLAM while humanoid moves through Isaac Sim environment. Visualize map in RViz."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance"}),": Map shows recognizable environment structure, no obvious drift"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-sensor-fusion-robustness-15-hours",children:"Exercise 3: Sensor Fusion Robustness (1.5 hours)"}),"\n",(0,t.jsx)(n.p,{children:"Implement Kalman filter fusing camera + LiDAR. Test robustness by blocking camera \u2192 verify LiDAR compensates."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance"}),": Fused estimate stays accurate when one sensor fails"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-4-tensorrt-optimization-1-hour",children:"Exercise 4: TensorRT Optimization (1 hour)"}),"\n",(0,t.jsx)(n.p,{children:"Convert pre-trained detection model to TensorRT INT8 format. Measure speedup vs. FP32."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance"}),": 3-5x latency reduction, ",(0,t.jsx)(n.code,{children:"<5%"})," accuracy loss"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-9-capstone-integration",children:"Part 9: Capstone Integration"}),"\n",(0,t.jsx)(n.p,{children:'Perception (Chapter 3) is the "eyes" of your Humanoid AI Assistant:'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 2"})," built the digital twin with sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 3"})," processes sensor data \u2192 extracts meaning (objects, locations, self-localization)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 4"})," will train better detection models using synthetic data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapter 5"})," deploys everything to real hardware"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"By end of Chapter 3, your system should:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Detect objects in real-time (>10 FPS)"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Build maps and localize"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Fuse multiple sensors robustly"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Run efficiently on Jetson (TensorRT optimized)"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"In Chapter 4, you'll generate massive synthetic datasets to train better perception models."}),"\n",(0,t.jsxs)(n.p,{children:["Ready? Move to ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/ai_native-textbook/docs/module-3/chapter-4-isaac-lab",children:"Chapter 4: Synthetic Data Generation with Isaac Lab"})}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Chapter Summary"}),": 5-6 hours | Difficulty: Intermediate | Week 9"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/",children:"Isaac ROS Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/",children:"TensorRT Developer Guide"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.ultralytics.com/",children:"YOLOv5 Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://developer.nvidia.com/jetson",children:"Jetson Deployment Guide"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var i=s(6540);const t={},r=i.createContext(t);function a(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);