"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[6420],{2925:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4/chapter-4-vla-control-architecture","title":"Chapter 4: VLA Control Architecture & Deployment","description":"Module 13 (Part 1) | Difficulty 8\u201310 hours","source":"@site/docs/module-4/chapter-4-vla-control-architecture.md","sourceDirName":"module-4","slug":"/module-4/chapter-4-vla-control-architecture","permalink":"/ai_native-textbook/docs/module-4/chapter-4-vla-control-architecture","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Language Planning with Whisper & Large Language Models","permalink":"/ai_native-textbook/docs/module-4/chapter-3-language-planning-whisper-llm"},"next":{"title":"Week 13: VLA Capstone Sprint & Integration","permalink":"/ai_native-textbook/docs/module-4/week-13"}}');var s=t(4848),o=t(8453);const r={},a="Chapter 4: VLA Control Architecture & Deployment",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Part 1: ROS 2 Control Fundamentals",id:"part-1-ros-2-control-fundamentals",level:2},{value:"Action Servers",id:"action-servers",level:3},{value:"Nav2 Navigation Stack",id:"nav2-navigation-stack",level:3},{value:"MoveIt Manipulation",id:"moveit-manipulation",level:3},{value:"Part 2: VLA Pipeline Integration",id:"part-2-vla-pipeline-integration",level:2},{value:"Complete VLA Orchestrator",id:"complete-vla-orchestrator",level:3},{value:"Part 3: Safety &amp; Failure Recovery",id:"part-3-safety--failure-recovery",level:2},{value:"Safety Mechanisms",id:"safety-mechanisms",level:3},{value:"Failure Recovery",id:"failure-recovery",level:3},{value:"Part 4: Deployment &amp; Optimization",id:"part-4-deployment--optimization",level:2},{value:"Deployment Design Considerations",id:"deployment-design-considerations",level:3},{value:"Jetson Deployment",id:"jetson-deployment",level:3},{value:"Labs &amp; Exercises",id:"labs--exercises",level:2},{value:"Lab 1: Build Perception-Aware Navigation (1.5 hours)",id:"lab-1-build-perception-aware-navigation-15-hours",level:3},{value:"Lab 2: Integrate LLM Planning with Control (1.5 hours)",id:"lab-2-integrate-llm-planning-with-control-15-hours",level:3},{value:"Lab 3: Implement Safety Watchdogs (1 hour)",id:"lab-3-implement-safety-watchdogs-1-hour",level:3},{value:"Lab 4: Design Deployment Optimization Strategy (1.5 hours)",id:"lab-4-design-deployment-optimization-strategy-15-hours",level:3},{value:"Capstone Integration",id:"capstone-integration",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-4-vla-control-architecture--deployment",children:"Chapter 4: VLA Control Architecture & Deployment"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Module"}),": 4 | ",(0,s.jsx)(n.strong,{children:"Week"}),": 13 (Part 1) | ",(0,s.jsx)(n.strong,{children:"Difficulty"}),": Advanced | ",(0,s.jsx)(n.strong,{children:"Estimated Time"}),": 8\u201310 hours"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Design and implement"})," ROS 2 control nodes for navigation (Nav2) and manipulation (MoveIt)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integrate perception \u2192 planning \u2192 control"})," into a unified VLA orchestrator"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement safety mechanisms"})," including watchdogs, emergency stops, and collision avoidance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Design deployment strategies"})," for real hardware including model optimization and latency budgeting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Explain failure recovery mechanisms"})," and replanning for robust task execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deploy optimized VLA systems"})," to resource-constrained platforms (Jetson)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Action Servers"}),": Asynchronous, goal-oriented communication for long-running tasks (navigation, manipulation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Nav2"}),": ROS 2 navigation stack providing path planning and autonomous mobile manipulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"MoveIt"}),": Motion planning library for arm manipulation with collision avoidance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VLA Orchestrator"}),": Central node that coordinates perception, planning, and action"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Behavior Trees"}),": Hierarchical task control with conditional logic and error recovery"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Watchdogs"}),": Monitoring systems that enforce timing constraints and kill switches"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deployment Optimization"}),": Model quantization, TensorRT inference, edge processing"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-1-ros-2-control-fundamentals",children:"Part 1: ROS 2 Control Fundamentals"}),"\n",(0,s.jsx)(n.h3,{id:"action-servers",children:"Action Servers"}),"\n",(0,s.jsxs)(n.p,{children:["ROS 2 actions provide ",(0,s.jsx)(n.strong,{children:"goal-oriented communication"})," for long-running tasks:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Client (LLM Planner) \u2192 Goal \u2192 Action Server (Navigator/Manipulator) \u2192 Feedback/Result\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key differences from topics/services"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Topics"}),": One-way streaming (no synchronization)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Services"}),": Request/response (synchronous, blocks)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Actions"}),": Goal \u2192 feedback loop \u2192 result (asynchronous)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example: Navigation Action"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: navigation_action_server.py\nimport rclpy\nfrom rclpy.action import ActionServer\nfrom rclpy.node import Node\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\nimport math\n\nclass NavigationActionServer(Node):\n    def __init__(self):\n        super().__init__('nav_action_server')\n\n        self._action_server = ActionServer(\n            self, NavigateToPose, 'navigate_to_pose', self.execute_callback\n        )\n\n        self.get_logger().info('Navigation Action Server started')\n\n    def execute_callback(self, goal_handle):\n        \"\"\"Execute navigation to target pose.\"\"\"\n        goal = goal_handle.request\n        self.get_logger().info(f'Received goal: {goal.pose.pose.position}')\n\n        # Simulate navigation (in real implementation, call Nav2)\n        for i in range(10):\n            if goal_handle.is_cancel_requested:\n                goal_handle.canceled()\n                return NavigateToPose.Result(error_code=1)\n\n            # Publish feedback\n            feedback = NavigateToPose.Feedback()\n            feedback.distance_remaining = 1.0 - (i / 10.0)  # Decrease distance\n            goal_handle.publish_feedback(feedback)\n\n            self.get_logger().info(f'Distance remaining: {feedback.distance_remaining:.2f}')\n            import time\n            time.sleep(0.5)\n\n        # Return result\n        goal_handle.succeed()\n        result = NavigateToPose.Result()\n        result.error_code = 0  # Success\n        return result\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NavigationActionServer()\n    rclpy.spin(node)\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"nav2-navigation-stack",children:"Nav2 Navigation Stack"}),"\n",(0,s.jsx)(n.p,{children:"Nav2 provides production-ready autonomous navigation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: nav2_client.py\nimport rclpy\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\nimport math\n\nclass Nav2Client:\n    def __init__(self, node):\n        self._action_client = ActionClient(node, NavigateToPose, \'navigate_to_pose\')\n\n    def navigate_to(self, x, y, theta, frame_id=\'map\'):\n        """\n        Navigate to target pose.\n\n        Args:\n            x, y, theta: Target position and orientation\n            frame_id: Coordinate frame\n        """\n        # Create goal\n        goal = NavigateToPose.Goal()\n        goal.pose = PoseStamped()\n        goal.pose.header.frame_id = frame_id\n        goal.pose.pose.position.x = float(x)\n        goal.pose.pose.position.y = float(y)\n        goal.pose.pose.orientation.z = math.sin(theta / 2.0)\n        goal.pose.pose.orientation.w = math.cos(theta / 2.0)\n\n        # Send goal\n        self._action_client.wait_for_server()\n        future = self._action_client.send_goal_async(goal)\n        return future\n'})}),"\n",(0,s.jsx)(n.h3,{id:"moveit-manipulation",children:"MoveIt Manipulation"}),"\n",(0,s.jsx)(n.p,{children:"MoveIt enables arm motion planning with collision avoidance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: moveit_manipulation.py\nfrom moveit_commander import MoveGroupCommander, RobotCommander\nimport geometry_msgs.msg as geometry_msgs\n\nclass ManipulationPlanner:\n    def __init__(self):\n        self.robot = RobotCommander()\n        self.arm_group = MoveGroupCommander(\'right_arm\')\n\n        # Set planning constraints\n        self.arm_group.set_goal_position_tolerance(0.01)  # 1cm\n        self.arm_group.set_max_acceleration_scaling_factor(0.5)\n        self.arm_group.set_max_velocity_scaling_factor(0.5)\n\n    def plan_to_position(self, x, y, z):\n        """Plan arm motion to 3D position."""\n        target_pose = geometry_msgs.Pose()\n        target_pose.position.x = x\n        target_pose.position.y = y\n        target_pose.position.z = z\n\n        # Set orientation (gripper points down)\n        target_pose.orientation.x = 1.0\n        target_pose.orientation.y = 0.0\n        target_pose.orientation.z = 0.0\n        target_pose.orientation.w = 0.0\n\n        self.arm_group.set_pose_target(target_pose)\n        plan = self.arm_group.plan()\n\n        return plan\n\n    def execute_plan(self, plan):\n        """Execute planned motion."""\n        self.arm_group.execute(plan, wait=True)\n\n    def open_gripper(self, gripper_force=0.0):\n        """Open gripper."""\n        # Command gripper to open (0 force = fully open)\n        pass\n\n    def close_gripper(self, gripper_force=1.0):\n        """Close gripper with specified force."""\n        # Command gripper to close with force\n        pass\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-2-vla-pipeline-integration",children:"Part 2: VLA Pipeline Integration"}),"\n",(0,s.jsx)(n.h3,{id:"complete-vla-orchestrator",children:"Complete VLA Orchestrator"}),"\n",(0,s.jsx)(n.p,{children:"The orchestrator coordinates perception \u2192 planning \u2192 control:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# File: vla_orchestrator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nimport json\nimport time\n\nclass VLAOrchestrator(Node):\n    \"\"\"\n    Main VLA system orchestrator that:\n    1. Receives voice commands\n    2. Runs perception pipeline\n    3. Calls LLM for task decomposition\n    4. Executes ROS 2 actions (Nav2, MoveIt)\n    5. Monitors execution and replans on failure\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('vla_orchestrator')\n\n        # Subscribe to inputs\n        self.command_subscription = self.create_subscription(\n            String, '/user/voice_command', self.command_callback, 10\n        )\n\n        # State tracking\n        self.current_task = None\n        self.execution_state = 'idle'  # idle, executing, failed\n\n        # Initialize sub-components\n        self.nav_client = self._create_nav_client()\n        self.manipulation = self._create_manipulation_client()\n        self.llm = self._create_llm_client()\n\n        self.get_logger().info('VLA Orchestrator initialized')\n\n    def command_callback(self, msg):\n        \"\"\"Process user command.\"\"\"\n        command = msg.data\n        self.get_logger().info(f'Received command: {command}')\n\n        try:\n            # 1. Get scene understanding from perception\n            scene = self._get_scene_understanding()\n\n            # 2. Call LLM to decompose task\n            task_plan = self._decompose_task(command, scene)\n\n            # 3. Execute task plan with error recovery\n            self._execute_task_plan(task_plan)\n\n            self.get_logger().info('Task completed successfully')\n\n        except Exception as e:\n            self.get_logger().error(f'Task failed: {e}')\n            self.execution_state = 'failed'\n\n    def _get_scene_understanding(self):\n        \"\"\"Get current scene from perception module.\"\"\"\n        # Wait for latest perception message\n        # (In real implementation, subscribe and cache)\n        scene = {\n            'objects': [\n                {'id': 'cup_0', 'class': 'cup', 'position': [0.5, 0.3, 0.8]},\n                {'id': 'table_0', 'class': 'table', 'position': [0.5, 0.0, 0.0]}\n            ]\n        }\n        return scene\n\n    def _decompose_task(self, command, scene):\n        \"\"\"Call LLM to decompose task.\"\"\"\n        prompt = f\"\"\"\n        Robot scene: {json.dumps(scene)}\n        User command: {command}\n        Decompose into robot skills.\n        \"\"\"\n\n        # Call LLM API (mock for now)\n        task_plan = {\n            'steps': [\n                {'skill': 'search', 'params': {'object': 'red cup'}},\n                {'skill': 'moveto', 'params': {'position': [0.5, 0.3]}},\n                {'skill': 'pick', 'params': {'gripper_force': 'moderate'}},\n                {'skill': 'moveto', 'params': {'position': [0.8, 1.0]}},\n                {'skill': 'place', 'params': {'release_speed': 'slow'}}\n            ]\n        }\n        return task_plan\n\n    def _execute_task_plan(self, task_plan):\n        \"\"\"Execute task plan step-by-step.\"\"\"\n        for i, step in enumerate(task_plan['steps']):\n            skill = step['skill']\n            params = step['params']\n\n            self.get_logger().info(f'Step {i+1}: {skill} {params}')\n\n            try:\n                # Execute skill\n                if skill == 'moveto':\n                    self._execute_moveto(params['position'])\n                elif skill == 'pick':\n                    self._execute_pick(params.get('gripper_force', 'moderate'))\n                elif skill == 'place':\n                    self._execute_place(params.get('release_speed', 'moderate'))\n                elif skill == 'search':\n                    self._execute_search(params['object'])\n\n                time.sleep(0.1)  # Brief delay between steps\n\n            except Exception as e:\n                self.get_logger().error(f'Step {i+1} failed: {e}')\n                # Trigger replanning (simplified)\n                self.get_logger().info('Replanning...')\n                raise\n\n    def _execute_moveto(self, position):\n        \"\"\"Execute navigation.\"\"\"\n        self.get_logger().info(f'Navigating to {position}')\n        # Call Nav2 action client\n\n    def _execute_pick(self, gripper_force):\n        \"\"\"Execute grasping.\"\"\"\n        self.get_logger().info(f'Picking with force {gripper_force}')\n        # Call MoveIt + gripper\n\n    def _execute_place(self, release_speed):\n        \"\"\"Execute placement.\"\"\"\n        self.get_logger().info(f'Placing with speed {release_speed}')\n        # Call MoveIt + gripper\n\n    def _execute_search(self, object_name):\n        \"\"\"Execute search (perception + navigation).\"\"\"\n        self.get_logger().info(f'Searching for {object_name}')\n        # Spin in place, capture images, detect object\n\n    def _create_nav_client(self):\n        # Create Nav2 client\n        return None\n\n    def _create_manipulation_client(self):\n        # Create MoveIt client\n        return None\n\n    def _create_llm_client(self):\n        # Create LLM API client\n        return None\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VLAOrchestrator()\n    rclpy.spin(node)\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-3-safety--failure-recovery",children:"Part 3: Safety & Failure Recovery"}),"\n",(0,s.jsx)(n.h3,{id:"safety-mechanisms",children:"Safety Mechanisms"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Watchdogs"}),": Monitor control loop timing and enforce emergency stops."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# File: safety_watchdog.py\nimport rclpy\nfrom rclpy.node import Node\nimport time\nimport threading\n\nclass SafetyWatchdog(Node):\n    """Monitor robot safety and enforce watchdog constraints."""\n\n    def __init__(self, timeout_seconds=2.0):\n        super().__init__(\'safety_watchdog\')\n\n        self.timeout = timeout_seconds\n        self.last_heartbeat = time.time()\n        self.emergency_stop_active = False\n\n        # Subscription to monitor activity\n        self.monitor_subscription = self.create_subscription(\n            String, \'/vla/status\', self.status_callback, 10\n        )\n\n        # Start watchdog thread\n        self.watchdog_thread = threading.Thread(target=self._watchdog_loop, daemon=True)\n        self.watchdog_thread.start()\n\n    def status_callback(self, msg):\n        """Update heartbeat on activity."""\n        self.last_heartbeat = time.time()\n\n    def _watchdog_loop(self):\n        """Watchdog loop: check for timeout."""\n        while True:\n            time.sleep(0.1)\n\n            elapsed = time.time() - self.last_heartbeat\n            if elapsed > self.timeout and not self.emergency_stop_active:\n                self.get_logger().warning(f\'Watchdog timeout after {elapsed:.2f}s\')\n                self._activate_emergency_stop()\n\n    def _activate_emergency_stop(self):\n        """Activate emergency stop (halt all motors)."""\n        self.emergency_stop_active = True\n        self.get_logger().error(\'EMERGENCY STOP ACTIVATED\')\n        # Publish emergency stop command to all actuators\n'})}),"\n",(0,s.jsx)(n.h3,{id:"failure-recovery",children:"Failure Recovery"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Replanning"}),": When a skill fails, generate an alternative plan."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def execute_with_replanning(task_plan, max_retries=2):\n    """Execute task plan with replanning on failure."""\n    for step_idx, step in enumerate(task_plan[\'steps\']):\n        retries = 0\n\n        while retries < max_retries:\n            try:\n                execute_skill(step)\n                break  # Success, move to next step\n\n            except SkillFailureException as e:\n                retries += 1\n                logger.warning(f"Step {step_idx} failed: {e}")\n\n                if retries < max_retries:\n                    # Attempt to replan from this point\n                    logger.info(f"Replanning from step {step_idx}...")\n                    remaining_steps = task_plan[\'steps\'][step_idx:]\n                    replanned = replan_from_failure(remaining_steps, current_state)\n                    task_plan[\'steps\'] = task_plan[\'steps\'][:step_idx] + replanned\n                else:\n                    raise  # Give up after max retries\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"part-4-deployment--optimization",children:"Part 4: Deployment & Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"deployment-design-considerations",children:"Deployment Design Considerations"}),"\n",(0,s.jsx)(n.p,{children:"For physical robot deployment:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Optimization"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Quantization (FP32 \u2192 INT8): 4x memory reduction, minimal accuracy loss"}),"\n",(0,s.jsx)(n.li,{children:"Pruning: Remove unused weights (~30-40% size reduction)"}),"\n",(0,s.jsx)(n.li,{children:"Distillation: Train smaller model from larger model"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Inference Optimization"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"TensorRT (NVIDIA): Optimized inference engine (~2-3x speedup)"}),"\n",(0,s.jsx)(n.li,{children:"ONNX: Cross-platform model format with optimizations"}),"\n",(0,s.jsx)(n.li,{children:"Edge devices: Jetson platforms provide GPU-accelerated inference"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Latency Budgeting"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Perception: ",(0,s.jsx)(n.code,{children:"<1"}),"00ms` (YOLO inference)"]}),"\n",(0,s.jsxs)(n.li,{children:["Planning: ",(0,s.jsx)(n.code,{children:"<1"}),"s` (LLM reasoning)"]}),"\n",(0,s.jsxs)(n.li,{children:["Control: ",(0,s.jsx)(n.code,{children:"<5"}),"0ms` (ROS 2 action execution)"]}),"\n",(0,s.jsxs)(n.li,{children:["Total cycle: ",(0,s.jsx)(n.code,{children:"<3"}),"3ms` for real-time control (~30 FPS)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example Deployment Profile"})," (Jetson Orin Nano):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception"}),": YOLO + depth (100ms, GPU)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM"}),": Cloud API call (1-2s latency acceptable for planning)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Control"}),": ROS 2 action (10-20ms per step, CPU)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Total cycle time"}),": ~2.5 seconds per high-level command (acceptable for household tasks)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"jetson-deployment",children:"Jetson Deployment"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install ROS 2 on Jetson\nsudo apt-get update\nsudo apt-get install -y ros-humble-desktop\n\n# Install perception libraries\npip install ultralytics opencv-python torch torchvision\n\n# Install control libraries\npip install rclpy MoveIt2 Nav2\n\n# Deploy VLA system\n# Copy ROS 2 packages and configuration to Jetson\n# Run: ros2 launch vla_system full_pipeline.launch.py\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"labs--exercises",children:"Labs & Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"lab-1-build-perception-aware-navigation-15-hours",children:"Lab 1: Build Perception-Aware Navigation (1.5 hours)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Navigate while avoiding obstacles detected by perception."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example code structure"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class PerceptionAwareNavigator:\n    def navigate_avoiding_obstacles(self, goal_position, obstacle_threshold=0.3):\n        # Get perception data\n        scene = self.get_scene()\n\n        # Check for obstacles in path\n        obstacles = [obj for obj in scene['objects'] if obj['distance'] < obstacle_threshold]\n\n        if obstacles:\n            # Replan around obstacles\n            alternative_path = self.compute_alternative_path(goal_position, obstacles)\n            return self.nav_client.navigate(alternative_path)\n        else:\n            return self.nav_client.navigate(goal_position)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"lab-2-integrate-llm-planning-with-control-15-hours",children:"Lab 2: Integrate LLM Planning with Control (1.5 hours)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Connect LLM task decomposition to ROS 2 execution."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Evaluation"}),": Successfully execute a multi-step household task from voice command."]}),"\n",(0,s.jsx)(n.h3,{id:"lab-3-implement-safety-watchdogs-1-hour",children:"Lab 3: Implement Safety Watchdogs (1 hour)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Build a safety monitoring system that halts execution on timeout."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor control loop frequency"}),"\n",(0,s.jsx)(n.li,{children:"Detect missed deadlines"}),"\n",(0,s.jsx)(n.li,{children:"Activate emergency stop on failure"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"lab-4-design-deployment-optimization-strategy-15-hours",children:"Lab 4: Design Deployment Optimization Strategy (1.5 hours)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Objective"}),": Analyze latency, create optimization plan, estimate performance gains."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Deliverables"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Latency breakdown (perception, planning, control)"}),"\n",(0,s.jsx)(n.li,{children:"Optimization strategy (which components to quantize/optimize)"}),"\n",(0,s.jsx)(n.li,{children:"Estimated speedups"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"capstone-integration",children:"Capstone Integration"}),"\n",(0,s.jsxs)(n.p,{children:["This chapter brings together ",(0,s.jsx)(n.strong,{children:"all VLA pillars"}),": perception (Ch 2), planning (Ch 3), and control (Ch 4). Your capstone project will:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Deploy a complete VLA system in simulation"}),"\n",(0,s.jsx)(n.li,{children:"Execute household tasks from natural language commands"}),"\n",(0,s.jsx)(n.li,{children:"Demonstrate replanning and failure recovery"}),"\n",(0,s.jsx)(n.li,{children:"Discuss deployment to physical hardware"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"VLA control architecture requires tight integration of perception, planning, and action. Key takeaways:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 actions"})," provide goal-oriented communication for long-running tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Nav2 + MoveIt"})," offer production-ready navigation and manipulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VLA orchestrators"})," coordinate all components into seamless task execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety watchdogs"})," and ",(0,s.jsx)(n.strong,{children:"replanning"})," enable robust, fault-tolerant behavior"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deployment optimization"})," makes VLA practical on resource-constrained platforms"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:'Macenski, S., et al. (2020). "Nav2: A Customizable Autonomous Navigation System." arXiv:2010.04738'}),"\n",(0,s.jsx)(n.li,{children:'Sucan, I., & Chitta, S. (2023). "MoveIt2: A Modern C++ Motion Planning Framework." IEEE RA-L'}),"\n",(0,s.jsx)(n.li,{children:'Sierra, B., et al. (2024). "Jetson Deployment Best Practices." NVIDIA Developer Blog'}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next Section"}),": ",(0,s.jsx)(n.a,{href:"/ai_native-textbook/docs/module-4/week-13",children:"Week 13: Capstone Sprint & Final Integration"})]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const s={},o=i.createContext(s);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);