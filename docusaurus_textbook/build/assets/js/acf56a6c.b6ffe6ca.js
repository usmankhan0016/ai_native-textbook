"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[6130],{3365:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>t,contentTitle:()=>o,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-2/chapter-3-sensor-simulation","title":"Chapter 3: Sensor Simulation (LiDAR, Depth Camera, IMU)","description":"Estimated time Intermediate | Week: 7, Days 1-3","source":"@site/docs/module-2/chapter-3-sensor-simulation.md","sourceDirName":"module-2","slug":"/module-2/chapter-3-sensor-simulation","permalink":"/ai_native-textbook/docs/module-2/chapter-3-sensor-simulation","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"sensors","permalink":"/ai_native-textbook/docs/tags/sensors"},{"inline":true,"label":"lidar","permalink":"/ai_native-textbook/docs/tags/lidar"},{"inline":true,"label":"depth-camera","permalink":"/ai_native-textbook/docs/tags/depth-camera"},{"inline":true,"label":"imu","permalink":"/ai_native-textbook/docs/tags/imu"},{"inline":true,"label":"perception","permalink":"/ai_native-textbook/docs/tags/perception"},{"inline":true,"label":"week-7","permalink":"/ai_native-textbook/docs/tags/week-7"}],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"sidebar_label":"Ch.3: Sensor Simulation","title":"Chapter 3: Sensor Simulation (LiDAR, Depth Camera, IMU)","tags":["sensors","lidar","depth-camera","imu","perception","week-7"],"difficulty":"Intermediate","module":2,"week":7,"prerequisites":["Chapter 2","ROS 2 Humble"],"estimated_time":"4-5 hours","topics":["lidar-simulation","camera-simulation","imu-simulation","sensor-noise","ros2-topics"]},"sidebar":"tutorialSidebar","previous":{"title":"Ch.2: Gazebo Physics","permalink":"/ai_native-textbook/docs/module-2/chapter-2-gazebo-physics-simulation"},"next":{"title":"Ch.4: Unity High-Fidelity","permalink":"/ai_native-textbook/docs/module-2/chapter-4-unity-high-fidelity-simulation"}}');var a=i(4848),r=i(8453);const l={sidebar_position:3,sidebar_label:"Ch.3: Sensor Simulation",title:"Chapter 3: Sensor Simulation (LiDAR, Depth Camera, IMU)",tags:["sensors","lidar","depth-camera","imu","perception","week-7"],difficulty:"Intermediate",module:2,week:7,prerequisites:["Chapter 2","ROS 2 Humble"],estimated_time:"4-5 hours",topics:["lidar-simulation","camera-simulation","imu-simulation","sensor-noise","ros2-topics"]},o="Chapter 3: Sensor Simulation (LiDAR, Depth Camera, IMU)",t={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Perception Simulation Fundamentals",id:"perception-simulation-fundamentals",level:2},{value:"Why Sensor Simulation Matters",id:"why-sensor-simulation-matters",level:3},{value:"Noise Modeling",id:"noise-modeling",level:3},{value:"LiDAR Simulation in Gazebo",id:"lidar-simulation-in-gazebo",level:2},{value:"2D LiDAR (Scanning Laser)",id:"2d-lidar-scanning-laser",level:3},{value:"3D LiDAR (Point Cloud)",id:"3d-lidar-point-cloud",level:3},{value:"Camera Simulation",id:"camera-simulation",level:2},{value:"RGB Camera",id:"rgb-camera",level:3},{value:"Depth Camera (RealSense D435-style)",id:"depth-camera-realsense-d435-style",level:3},{value:"IMU Sensor Simulation",id:"imu-sensor-simulation",level:2},{value:"IMU with Realistic Noise",id:"imu-with-realistic-noise",level:3},{value:"Sensor Fusion: Combining Multiple Sensors",id:"sensor-fusion-combining-multiple-sensors",level:2},{value:"Validating Simulated Sensors",id:"validating-simulated-sensors",level:2},{value:"Comparison to Real Hardware Specs",id:"comparison-to-real-hardware-specs",level:3},{value:"Validation Script",id:"validation-script",level:3},{value:"Hands-On Labs",id:"hands-on-labs",level:2},{value:"Lab 1: Simulate LiDAR and Visualize Point Clouds",id:"lab-1-simulate-lidar-and-visualize-point-clouds",level:3},{value:"Lab 2: Depth Camera with Noise",id:"lab-2-depth-camera-with-noise",level:3},{value:"Lab 3: IMU Noise Modeling",id:"lab-3-imu-noise-modeling",level:3},{value:"End-of-Chapter Exercises",id:"end-of-chapter-exercises",level:2},{value:"Exercise 1: LiDAR Navigation",id:"exercise-1-lidar-navigation",level:3},{value:"Exercise 2: Depth Camera Noise Analysis",id:"exercise-2-depth-camera-noise-analysis",level:3},{value:"Exercise 3: IMU Calibration",id:"exercise-3-imu-calibration",level:3},{value:"Exercise 4: Sensor Fusion Implementation",id:"exercise-4-sensor-fusion-implementation",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-3-sensor-simulation-lidar-depth-camera-imu",children:"Chapter 3: Sensor Simulation (LiDAR, Depth Camera, IMU)"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Estimated time"}),": 4-5 hours | ",(0,a.jsx)(n.strong,{children:"Difficulty"}),": Intermediate | ",(0,a.jsx)(n.strong,{children:"Week"}),": 7, Days 1-3"]}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Explain why sensor simulation is essential"})," for developing robust perception algorithms"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simulate LiDAR sensors"})," (2D scans and 3D point clouds) with realistic range and accuracy"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Configure RGB and depth cameras"})," with intrinsic parameters and noise models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simulate stereo camera systems"})," with baseline and disparity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Configure IMU sensors"})," with accelerometer, gyroscope, and magnetometer noise models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Add sensor plugins to robots"})," in SDF format"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Publish sensor data"})," over ROS 2 topics in standard message formats"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visualize and validate"})," sensor output using RViz and Python analysis scripts"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor fidelity"}),": How accurately simulation matches real-world sensor behavior (noise, range, accuracy)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"LiDAR"}),": Light Detection and Ranging\u2014emits laser pulses, measures reflections to create point clouds"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth camera"}),": Captures RGB image + depth map (distance to each pixel)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Stereo vision"}),": Two cameras with baseline separation compute depth via triangulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"IMU"}),": Inertial Measurement Unit\u2014measures acceleration and angular velocity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor noise"}),": Realistic Gaussian noise and bias that match real hardware specifications"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"perception-simulation-fundamentals",children:"Perception Simulation Fundamentals"}),"\n",(0,a.jsx)(n.h3,{id:"why-sensor-simulation-matters",children:"Why Sensor Simulation Matters"}),"\n",(0,a.jsx)(n.p,{children:"Real sensors have imperfections:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise"}),": Random fluctuations (Gaussian noise)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bias"}),": Systematic error (e.g., IMU always reads 0.1m/s\xb2 high)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range limits"}),": Sensors work within min/max distances"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Field of view (FOV)"}),": Limited angle of perception"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution"}),": Discrete measurements (not continuous)"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Without simulating these, algorithms trained in perfect simulation fail on real hardware."}),"\n",(0,a.jsx)(n.h3,{id:"noise-modeling",children:"Noise Modeling"}),"\n",(0,a.jsxs)(n.p,{children:["Realistic sensor noise follows a ",(0,a.jsx)(n.strong,{children:"Gaussian distribution"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"measurement = true_value + noise\nnoise ~ N(mean=0, std_dev=\u03c3)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example: LiDAR with \u03c3=0.01m adds \xb10.01m random error to each measurement."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"lidar-simulation-in-gazebo",children:"LiDAR Simulation in Gazebo"}),"\n",(0,a.jsx)(n.h3,{id:"2d-lidar-scanning-laser",children:"2D LiDAR (Scanning Laser)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- File: humanoid_with_sensors.sdf (in humanoid head link) --\x3e\n<sensor name="laser_scanner" type="ray">\n  <pose>0 0 0.3 0 0 0</pose>  \x3c!-- On head, facing forward --\x3e\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>360</samples>  \x3c!-- 360 points per scan --\x3e\n        <resolution>1.0</resolution>  \x3c!-- One per degree --\x3e\n        <min_angle>0</min_angle>\n        <max_angle>6.283</max_angle>  \x3c!-- 2\u03c0 radians = 360\xb0 --\x3e\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>  \x3c!-- Minimum range: 10cm --\x3e\n      <max>30.0</max>  \x3c!-- Maximum range: 30m --\x3e\n      <resolution>0.01</resolution>  \x3c!-- 1cm resolution --\x3e\n    </range>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.05</stddev>  \x3c!-- \xb15cm noise --\x3e\n    </noise>\n  </ray>\n  <always_on>1</always_on>\n  <update_rate>30</update_rate>  \x3c!-- 30 Hz --\x3e\n  <visualize>true</visualize>\n  <plugin name="laser_controller" filename="libgazebo_ros_ray_sensor.so">\n    <ros>\n      <remapping>~/out:=scan</remapping>\n    </ros>\n    <output_type>sensor_msgs/LaserScan</output_type>\n    <frame_name>laser_link</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Verify in ROS 2"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /scan\n# Output: angles, ranges, intensities\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3d-lidar-point-cloud",children:"3D LiDAR (Point Cloud)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Velodyne-style 3D LiDAR --\x3e\n<sensor name="lidar_3d" type="ray">\n  <pose>0 0 0.5 0 0 0</pose>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>400</samples>  \x3c!-- 400 points per scan --\x3e\n        <min_angle>0</min_angle>\n        <max_angle>6.283</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>16</samples>  \x3c!-- 16 vertical lines --\x3e\n        <min_angle>-0.2618</min_angle>  \x3c!-- -15 degrees --\x3e\n        <max_angle>0.2618</max_angle>   \x3c!-- +15 degrees --\x3e\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>100.0</max>\n      <resolution>0.01</resolution>\n    </range>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.1</stddev>  \x3c!-- \xb110cm noise --\x3e\n    </noise>\n  </ray>\n  <always_on>1</always_on>\n  <update_rate>10</update_rate>  \x3c!-- 10 Hz (matches Velodyne) --\x3e\n  <visualize>true</visualize>\n  <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n    <ros>\n      <remapping>~/out:=points</remapping>\n    </ros>\n    <output_type>sensor_msgs/PointCloud2</output_type>\n    <frame_name>lidar_link</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"rgb-camera",children:"RGB Camera"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="rgb_camera" type="camera">\n  <pose>0 0 0.15 0 0 0</pose>  \x3c!-- On head --\x3e\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.01</near>\n      <far>100.0</far>\n    </clip>\n    \x3c!-- Intrinsic parameters (camera matrix) --\x3e\n    <lens>\n      <intrinsics>\n        <fx>320.0</fx>  \x3c!-- Focal length X --\x3e\n        <fy>320.0</fy>  \x3c!-- Focal length Y --\x3e\n        <cx>320.0</cx>  \x3c!-- Principal point X --\x3e\n        <cy>240.0</cy>  \x3c!-- Principal point Y --\x3e\n        <s>0.0</s>      \x3c!-- Skew --\x3e\n      </intrinsics>\n    </lens>\n  </camera>\n  <always_on>1</always_on>\n  <update_rate>30</update_rate>\n  <visualize>false</visualize>\n  <plugin name="camera_plugin" filename="libgazebo_ros_camera.so">\n    <ros>\n      <namespace>humanoid_robot</namespace>\n      <remapping>image_raw:=rgb/image_raw</remapping>\n      <remapping>camera_info:=rgb/camera_info</remapping>\n    </ros>\n    <camera_name>rgb_camera</camera_name>\n    <frame_name>camera_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-realsense-d435-style",children:"Depth Camera (RealSense D435-style)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth_camera">\n  <pose>0 0 0.15 0 0 0</pose>\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60\xb0 --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>   \x3c!-- 10cm minimum range --\x3e\n      <far>10.0</far>    \x3c!-- 10m maximum range --\x3e\n    </clip>\n  </camera>\n  <always_on>1</always_on>\n  <update_rate>30</update_rate>\n  <visualize>false</visualize>\n  <plugin name="depth_camera_plugin" filename="libgazebo_ros_camera.so">\n    <ros>\n      <namespace>humanoid_robot</namespace>\n      <remapping>image_raw:=depth/image_raw</remapping>\n      <remapping>camera_info:=depth/camera_info</remapping>\n    </ros>\n    <camera_name>depth_camera</camera_name>\n    <frame_name>depth_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"imu-sensor-simulation",children:"IMU Sensor Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"imu-with-realistic-noise",children:"IMU with Realistic Noise"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <pose>0 0 0.3 0 0 0</pose>  \x3c!-- Torso --\x3e\n  <always_on>1</always_on>\n  <update_rate>100</update_rate>  \x3c!-- 100 Hz --\x3e\n  <visualize>false</visualize>\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>  \x3c!-- White noise --\x3e\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.00001</bias_stddev>  \x3c!-- Bias drift --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.01</stddev>  \x3c!-- Larger noise for acceleration --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.01</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.01</stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n  <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">\n    <ros>\n      <namespace>humanoid_robot</namespace>\n      <remapping>imu:=imu/data</remapping>\n    </ros>\n    <initial_orientation_as_reference>false</initial_orientation_as_reference>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion-combining-multiple-sensors",children:"Sensor Fusion: Combining Multiple Sensors"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Concept"}),": Use multiple sensor modalities together to improve robustness."]}),"\n",(0,a.jsxs)(n.p,{children:["Example: ",(0,a.jsx)(n.strong,{children:"LiDAR + Depth Camera"})]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"LiDAR is good for long range but 2D"}),"\n",(0,a.jsx)(n.li,{children:"Depth camera is good for close-range 3D but limited range"}),"\n",(0,a.jsx)(n.li,{children:"Together: Far obstacles (LiDAR) + detailed near objects (depth)"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# File: src/sensor_fusion_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, PointCloud2\nimport numpy as np\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n\n        # Subscribe to both sensors\n        self.lidar_sub = self.create_subscription(LaserScan, '/scan', self.lidar_callback, 10)\n        self.depth_sub = self.create_subscription(Image, '/depth/image_raw', self.depth_callback, 10)\n\n        # Publish fused obstacle map\n        self.map_pub = self.create_publisher(PointCloud2, '/obstacle_map', 10)\n\n    def lidar_callback(self, msg):\n        # Process LiDAR scan\n        pass\n\n    def depth_callback(self, msg):\n        # Process depth image\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorFusionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"validating-simulated-sensors",children:"Validating Simulated Sensors"}),"\n",(0,a.jsx)(n.h3,{id:"comparison-to-real-hardware-specs",children:"Comparison to Real Hardware Specs"}),"\n",(0,a.jsx)(n.p,{children:"RealSense D435 specifications:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Depth range: 0.1m - 10m"}),"\n",(0,a.jsx)(n.li,{children:"Resolution: 640\xd7480"}),"\n",(0,a.jsx)(n.li,{children:"FOV: 87\xb0 \xd7 58\xb0"}),"\n",(0,a.jsx)(n.li,{children:"Noise: ~1-2% of distance"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"RealSense D435 simulation (should match above):"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"<near>0.1</near>"}),", ",(0,a.jsx)(n.code,{children:"<far>10.0</far>"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"<width>640</width>"}),", ",(0,a.jsx)(n.code,{children:"<height>480</height>"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"<horizontal_fov>1.52</horizontal_fov>"})," (87\xb0)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"<stddev>0.01</stddev>"})," (1% noise)"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"validation-script",children:"Validation Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# File: src/validate_sensors.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2, Image\nimport numpy as np\n\nclass SensorValidator(Node):\n    def __init__(self):\n        super().__init__('sensor_validator')\n        self.lidar_sub = self.create_subscription(PointCloud2, '/points', self.validate_lidar, 1)\n        self.stats = {'count': 0, 'mean_range': 0, 'std_range': 0}\n\n    def validate_lidar(self, msg):\n        # Extract point coordinates from PointCloud2\n        points = np.array([(p[0], p[1], p[2]) for p in msg.data])\n        ranges = np.linalg.norm(points, axis=1)\n\n        self.stats['count'] += 1\n        self.stats['mean_range'] = np.mean(ranges)\n        self.stats['std_range'] = np.std(ranges)\n\n        self.get_logger().info(f\"LiDAR: {len(ranges)} points, \"\n                              f\"mean range: {self.stats['mean_range']:.2f}m, \"\n                              f\"std: {self.stats['std_range']:.4f}m\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorValidator()\n    rclpy.spin(node)\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-labs",children:"Hands-On Labs"}),"\n",(0,a.jsx)(n.h3,{id:"lab-1-simulate-lidar-and-visualize-point-clouds",children:"Lab 1: Simulate LiDAR and Visualize Point Clouds"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Objective"}),": Add 3D LiDAR to humanoid, view point clouds in RViz."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Add 3D LiDAR sensor (from section above) to your humanoid world file"}),"\n",(0,a.jsxs)(n.li,{children:["Launch Gazebo: ",(0,a.jsx)(n.code,{children:"gazebo humanoid_with_lidar.sdf"})]}),"\n",(0,a.jsxs)(n.li,{children:["In another terminal, view point cloud:","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"rviz2\n# Add PointCloud2 display, subscribe to /points\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Verification"}),": Point cloud appears in RViz, moves with humanoid"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-2-depth-camera-with-noise",children:"Lab 2: Depth Camera with Noise"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Objective"}),": Configure depth camera with realistic noise, compare noisy vs. ideal."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Add depth camera plugin (from section above)"}),"\n",(0,a.jsxs)(n.li,{children:["View depth image in RViz:","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 run rqt_image_view rqt_image_view\n# Select /humanoid_robot/depth/image_raw\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Measure noise: Run ",(0,a.jsx)(n.code,{children:"validate_sensors.py"})," and check std deviation"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"lab-3-imu-noise-modeling",children:"Lab 3: IMU Noise Modeling"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Objective"}),": Record IMU data while robot is stationary, analyze noise characteristics."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Add IMU plugin to humanoid"}),"\n",(0,a.jsxs)(n.li,{children:["Record 10 seconds of data:","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 bag record /humanoid_robot/imu/data\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Analyze with Python:","\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Plot acceleration noise distribution\npython3 src/analyze_imu.py\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"end-of-chapter-exercises",children:"End-of-Chapter Exercises"}),"\n",(0,a.jsx)(n.h3,{id:"exercise-1-lidar-navigation",children:"Exercise 1: LiDAR Navigation"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Task"}),": Humanoid navigates maze using simulated LiDAR. Detect obstacles at 2m, 5m, 10m."]}),"\n",(0,a.jsx)(n.h3,{id:"exercise-2-depth-camera-noise-analysis",children:"Exercise 2: Depth Camera Noise Analysis"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Task"}),": Configure depth camera with 3 noise levels (0.005m, 0.01m, 0.05m). Measure detection accuracy."]}),"\n",(0,a.jsx)(n.h3,{id:"exercise-3-imu-calibration",children:"Exercise 3: IMU Calibration"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Task"}),": Collect stationary IMU data, calculate bias and noise covariance."]}),"\n",(0,a.jsx)(n.h3,{id:"exercise-4-sensor-fusion-implementation",children:"Exercise 4: Sensor Fusion Implementation"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Task"}),": Fuse LiDAR and depth camera into combined 3D obstacle map."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsxs)(n.p,{children:["\u2705 ",(0,a.jsx)(n.strong,{children:"LiDAR simulation"}),": 2D scans and 3D point clouds with noise"]}),"\n",(0,a.jsxs)(n.p,{children:["\u2705 ",(0,a.jsx)(n.strong,{children:"Camera simulation"}),": RGB and depth cameras with intrinsic parameters"]}),"\n",(0,a.jsxs)(n.p,{children:["\u2705 ",(0,a.jsx)(n.strong,{children:"IMU simulation"}),": Realistic acceleration and angular velocity with noise"]}),"\n",(0,a.jsxs)(n.p,{children:["\u2705 ",(0,a.jsx)(n.strong,{children:"Sensor noise"}),": Gaussian models matching real hardware"]}),"\n",(0,a.jsxs)(n.p,{children:["\u2705 ",(0,a.jsx)(n.strong,{children:"ROS 2 topics"}),": Standard message formats for all sensors"]}),"\n",(0,a.jsxs)(n.p,{children:["\u2705 ",(0,a.jsx)(n.strong,{children:"Validation"}),": Compare simulated to real sensor specs"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"/ai_native-textbook/docs/module-2/chapter-4-unity-high-fidelity-simulation",children:"Chapter 4: High-Fidelity Simulation in Unity"})," covers rendering, dataset export, and high-fidelity visualization."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Estimated time"}),": 4-5 hours"]}),"\n",(0,a.jsx)(n.p,{children:"Ready? Head to Chapter 4!"})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>o});var s=i(6540);const a={},r=s.createContext(a);function l(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);