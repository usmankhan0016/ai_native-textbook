"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[2527],{1543:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4/chapter-2-vision-for-vla","title":"Chapter 2: Vision for VLA \u2013 Building Perception Pipelines","description":"Module 11 | Difficulty 8\u201310 hours","source":"@site/docs/module-4/chapter-2-vision-for-vla.md","sourceDirName":"module-4","slug":"/module-4/chapter-2-vision-for-vla","permalink":"/ai_native-textbook/docs/module-4/chapter-2-vision-for-vla","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Introduction to Vision\u2013Language\u2013Action Robotics","permalink":"/ai_native-textbook/docs/module-4/chapter-1-vla-intro"},"next":{"title":"Chapter 3: Language Planning with Whisper & Large Language Models","permalink":"/ai_native-textbook/docs/module-4/chapter-3-language-planning-whisper-llm"}}');var r=s(4848),t=s(8453);const a={},o="Chapter 2: Vision for VLA \u2013 Building Perception Pipelines",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Part 1: Perception Fundamentals",id:"part-1-perception-fundamentals",level:2},{value:"Perception Pipeline for VLA",id:"perception-pipeline-for-vla",level:3},{value:"Camera &amp; Depth Sensors",id:"camera--depth-sensors",level:3},{value:"Object Detection Models",id:"object-detection-models",level:3},{value:"Object Affordance Detection",id:"object-affordance-detection",level:3},{value:"Scene Graphs",id:"scene-graphs",level:3},{value:"Part 2: Multi-Modal Fusion",id:"part-2-multi-modal-fusion",level:2},{value:"RGB + Depth Fusion",id:"rgb--depth-fusion",level:3},{value:"Multi-View Camera Fusion",id:"multi-view-camera-fusion",level:3},{value:"Affordance Detection",id:"affordance-detection",level:3},{value:"Part 3: ROS 2 Perception Integration",id:"part-3-ros-2-perception-integration",level:2},{value:"ROS 2 Perception Node Architecture",id:"ros-2-perception-node-architecture",level:3},{value:"Part 4: Labs &amp; Exercises",id:"part-4-labs--exercises",level:2},{value:"Lab 1: Detect Objects in a Scene Image (1 hour)",id:"lab-1-detect-objects-in-a-scene-image-1-hour",level:3},{value:"Lab 2: Identify Pickable Objects (1.5 hours)",id:"lab-2-identify-pickable-objects-15-hours",level:3},{value:"Lab 3: Convert Scene to LLM-Readable Format (1 hour)",id:"lab-3-convert-scene-to-llm-readable-format-1-hour",level:3},{value:"End-of-Chapter Exercises",id:"end-of-chapter-exercises",level:3},{value:"Capstone Integration",id:"capstone-integration",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-2-vision-for-vla--building-perception-pipelines",children:"Chapter 2: Vision for VLA \u2013 Building Perception Pipelines"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Module"}),": 4 | ",(0,r.jsx)(n.strong,{children:"Week"}),": 11 | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": Intermediate | ",(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 8\u201310 hours"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Design and implement"})," multi-modal perception pipelines combining RGB, depth, and segmentation data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apply state-of-the-art models"})," (YOLO, Mask R-CNN, SAM, Grounding DINO) for object detection and affordance extraction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrate ROS 2 perception nodes"})," that subscribe to camera and depth topics and publish structured scene information"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Extract affordances"}),' from visual observations to inform LLM reasoning (e.g., "which objects are graspable?")']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fuse multiple sensor modalities"})," (RGB + depth + segmentation) into a scene graph suitable for LLM planning"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object Detection"})," (YOLO): Fast, real-time identification of object classes and bounding boxes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Instance Segmentation"})," (Mask R-CNN): Per-object pixel-level masks enabling precise graspability analysis"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Foundation Models"})," (SAM): Segment Anything Model for zero-shot segmentation of any object"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Grounding Vision-Language Models"})," (Grounding DINO): Link natural language descriptions to visual regions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Affordance Detection"}),": Identifying how objects can be interacted with (graspable, movable, pushable, etc.)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scene Graphs"}),": Structured representations of objects, their properties, and spatial relationships"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB-D Fusion"}),": Combining color information with depth for 3D scene understanding"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-1-perception-fundamentals",children:"Part 1: Perception Fundamentals"}),"\n",(0,r.jsx)(n.h3,{id:"perception-pipeline-for-vla",children:"Perception Pipeline for VLA"}),"\n",(0,r.jsx)(n.p,{children:'A VLA robot must answer: "What is in the environment, where is it, and how can I interact with it?"'}),"\n",(0,r.jsx)(n.p,{children:"The perception pipeline transforms raw sensor data into LLM-readable information:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Sensor Input Layer                                \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                          \u2502\n\u2502  \u2502RGB Camera\u2502  \u2502D-Cam/ToF \u2502  \u2502 (Optional)                          \u2502\n\u2502  \u25021920x1080 \u2502  \u2502640x480   \u2502  \u2502 LiDAR    \u2502                          \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502             \u2502              \u2502\n       \u2193             \u2193              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Model Inference Layer                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502  \u2502 YOLO Detection   \u2502      \u2502 Mask R-CNN       \u2502                    \u2502\n\u2502  \u2502 (Fast detection) \u2502      \u2502 (Instance masks) \u2502                    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2193                          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Feature Extraction Layer                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n\u2502  \u2502 Depth Inference  \u2502      \u2502 Affordance Models\u2502                    \u2502\n\u2502  \u2502 (3D coords)      \u2502      \u2502 (Graspability)   \u2502                    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2193                          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Scene Graph Construction                          \u2502\n\u2502  {                                                                   \u2502\n\u2502    "objects": [                                                      \u2502\n\u2502      {"id": "cup_0", "class": "cup", "color": "red",                \u2502\n\u2502       "location": [0.5, 0.3, 0.8], "graspable": true},              \u2502\n\u2502      {"id": "table_0", "class": "table", "graspable": false}        \u2502\n\u2502    ],                                                                \u2502\n\u2502    "spatial_relationships": [                                        \u2502\n\u2502      {"subject": "cup_0", "predicate": "on", "object": "table_0"}   \u2502\n\u2502    ]                                                                 \u2502\n\u2502  }                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  LLM-Ready Scene Representation                      \u2502\n\u2502  "There is a red cup on the table at position (0.5m, 0.3m, 0.8m).   \u2502\n\u2502   The cup is graspable. The table is not interactive."              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,r.jsx)(n.h3,{id:"camera--depth-sensors",children:"Camera & Depth Sensors"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"RGB Cameras"}),": Standard color images (1920\xd71080 or higher). Provides semantic information (color, text, fine details)."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Depth Sensors"}),": Measure distance from camera to objects. Common types:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo Vision"}),": Uses two RGB cameras + computation to infer depth (e.g., Intel RealSense D435)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time-of-Flight (ToF)"}),": Directly measures light travel time (e.g., Kinect, PMD CamBoard)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"}),": Laser-based scanning (e.g., Livox Mid360, SICK LMS1xx) \u2013 coarse but long-range"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["For household VLA robots, ",(0,r.jsx)(n.strong,{children:"RGB-D pairs"})," (RGB + depth from same viewpoint) are standard."]}),"\n",(0,r.jsx)(n.h3,{id:"object-detection-models",children:"Object Detection Models"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"YOLO (You Only Look Once)"}),": Fast, real-time object detection."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Input: RGB image"}),"\n",(0,r.jsx)(n.li,{children:"Output: Bounding boxes + class labels + confidence scores"}),"\n",(0,r.jsx)(n.li,{children:"Speed: ~30-60 FPS on GPU (fast enough for real-time control)"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy: ~80-90% on common objects (COCO dataset)"}),"\n",(0,r.jsx)(n.li,{children:"Use when: You need fast detection of common objects"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Mask R-CNN"}),": Slower but provides instance masks (pixel-level object boundaries)."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Input: RGB image"}),"\n",(0,r.jsx)(n.li,{children:"Output: Bounding boxes + class labels + per-pixel segmentation masks"}),"\n",(0,r.jsx)(n.li,{children:"Speed: ~5-10 FPS (slower, but more precise)"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy: ~90%+ with high precision"}),"\n",(0,r.jsx)(n.li,{children:"Use when: You need to know exact object shapes for grasping"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SAM (Segment Anything Model)"}),": Zero-shot segmentation of any object."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Input: RGB image + optional prompt (point or bounding box)"}),"\n",(0,r.jsx)(n.li,{children:"Output: Precise mask for any object (even ones not in training data)"}),"\n",(0,r.jsx)(n.li,{children:"Speed: ~50-100ms per mask (acceptable for planning, not real-time control)"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy: Exceptional on novel objects"}),"\n",(0,r.jsx)(n.li,{children:"Use when: You encounter unknown objects not in training data"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Grounding DINO"}),": Link natural language to visual regions."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Input: RGB image + text description"}),"\n",(0,r.jsx)(n.li,{children:"Output: Bounding box + mask for the described region"}),"\n",(0,r.jsx)(n.li,{children:"Speed: ~200ms (good for planning, not control)"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy: Excellent at finding described objects"}),"\n",(0,r.jsx)(n.li,{children:'Use when: LLM outputs "find the red cup" and you need the visual region'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"object-affordance-detection",children:"Object Affordance Detection"}),"\n",(0,r.jsxs)(n.p,{children:["Affordances describe ",(0,r.jsx)(n.strong,{children:"how objects can be interacted with"}),". A robot needs to know:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Is this object ",(0,r.jsx)(n.strong,{children:"graspable"}),"? (Can I pick it up?)"]}),"\n",(0,r.jsxs)(n.li,{children:["Is it ",(0,r.jsx)(n.strong,{children:"movable"}),"? (Can I push it?)"]}),"\n",(0,r.jsxs)(n.li,{children:["Is it ",(0,r.jsx)(n.strong,{children:"fragile"}),"? (Should I use gentle forces?)"]}),"\n",(0,r.jsxs)(n.li,{children:["Is it ",(0,r.jsx)(n.strong,{children:"openable"}),"? (Does it have lids, doors, drawers?)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Affordance detection can be:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Learned from data"}),": Train a classifier on affordance labels (e.g., graspable vs. non-graspable)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rule-based"}),': Use heuristics (e.g., "small objects are graspable, tables are not")']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Semantic"}),': Use LLM reasoning ("glasses are fragile, so handle gently")']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["For the capstone, we'll use a ",(0,r.jsx)(n.strong,{children:"simple learned classifier"})," trained on affordance labels."]}),"\n",(0,r.jsx)(n.h3,{id:"scene-graphs",children:"Scene Graphs"}),"\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"scene graph"})," represents the environment as a structured data format:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "timestamp": "2025-12-07T10:30:00Z",\n  "frame_id": "camera_link",\n  "objects": [\n    {\n      "id": "obj_001",\n      "class": "cup",\n      "color": "red",\n      "confidence": 0.92,\n      "bbox": [450, 300, 550, 400],  // [x_min, y_min, x_max, y_max] in pixels\n      "mask": null,  // Optionally include pixel-level mask\n      "position_3d": [0.45, 0.30, 0.80],  // [x, y, z] in meters (camera frame)\n      "affordances": {\n        "graspable": true,\n        "fragile": false,\n        "movable": true\n      }\n    },\n    {\n      "id": "obj_002",\n      "class": "table",\n      "confidence": 0.98,\n      "position_3d": [0.50, 0.0, 0.00],\n      "affordances": {\n        "graspable": false,\n        "climbable": false,\n        "pushable": false\n      }\n    }\n  ],\n  "relationships": [\n    {\n      "subject": "obj_001",\n      "predicate": "on",\n      "object": "obj_002"\n    }\n  ]\n}\n'})}),"\n",(0,r.jsx)(n.p,{children:"The scene graph can then be converted to natural language for the LLM:"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:'"There is a red cup (graspable) at position (0.45m, 0.30m, 0.80m) on a table at (0.50m, 0.0m, 0.00m)."'}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-2-multi-modal-fusion",children:"Part 2: Multi-Modal Fusion"}),"\n",(0,r.jsx)(n.h3,{id:"rgb--depth-fusion",children:"RGB + Depth Fusion"}),"\n",(0,r.jsx)(n.p,{children:"Combining RGB and depth creates a powerful representation:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB provides"}),": Semantic understanding (what objects are), fine details (colors, text, logos)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth provides"}),": 3D locations, object shapes, surface geometry"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Fusion Challenge"}),": Alignment. RGB and depth images have different resolutions and may be captured from slightly different viewpoints."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": Project depth into RGB frame using camera intrinsics:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: perception/rgb_d_fusion.py\nimport numpy as np\nimport cv2\n\ndef project_depth_to_rgb(depth_image, camera_intrinsics):\n    \"\"\"\n    Project depth image to RGB frame using camera intrinsics.\n\n    Args:\n        depth_image: Depth map (640x480) in meters\n        camera_intrinsics: Camera K matrix (3x3)\n\n    Returns:\n        RGB frame with depth overlay\n    \"\"\"\n    h, w = depth_image.shape\n    fx = camera_intrinsics[0, 0]\n    fy = camera_intrinsics[1, 1]\n    cx = camera_intrinsics[0, 2]\n    cy = camera_intrinsics[1, 2]\n\n    # Create point cloud from depth\n    x = np.arange(w)\n    y = np.arange(h)\n    xx, yy = np.meshgrid(x, y)\n\n    z = depth_image.astype(np.float32) / 1000.0  # Convert mm to m\n    x_3d = (xx - cx) * z / fx\n    y_3d = (yy - cy) * z / fy\n    z_3d = z\n\n    # Filter points (remove zeros, too far)\n    valid = (z > 0) & (z < 5.0)  # 0-5 meters\n\n    return {\n        'x': x_3d[valid],\n        'y': y_3d[valid],\n        'z': z_3d[valid],\n        'valid_mask': valid\n    }\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": For each pixel in the RGB image, we now know:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"2D position (row, column)"}),"\n",(0,r.jsx)(n.li,{children:"3D position (x, y, z in camera frame)"}),"\n",(0,r.jsx)(n.li,{children:"Color (R, G, B from RGB image)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This enables precise object localization for grasping."}),"\n",(0,r.jsx)(n.h3,{id:"multi-view-camera-fusion",children:"Multi-View Camera Fusion"}),"\n",(0,r.jsx)(n.p,{children:"Many robots use multiple cameras for full-body perception (front, side, top). Fusing views requires:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Timestamp Synchronization"}),": Ensure images are captured at nearly the same time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Frame Alignment"}),": Transform objects detected in each camera's frame into a common robot frame"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"De-duplication"}),": Recognize when two detections refer to the same object in the world"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Approach"}),": Use the ",(0,r.jsx)(n.strong,{children:"tf2 library"})," for frame transformations:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# File: perception/multi_view_fusion.py\nimport tf2_ros\nfrom geometry_msgs.msg import TransformStamped\n\nclass MultiViewFusion:\n    def __init__(self, node):\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, node)\n        self.objects = {}  # Global object registry\n\n    def fuse_detection(self, detection, camera_frame):\n        """\n        Fuse a detection from one camera into the global object map.\n\n        Args:\n            detection: Detection (class, bbox, position_3d)\n            camera_frame: Frame ID (e.g., "front_camera_link")\n        """\n        # Transform detection from camera frame to robot base frame\n        transform = self.tf_buffer.lookup_transform(\n            "base_link", camera_frame, rospy.Time(0)\n        )\n\n        # Apply transformation to 3D position\n        position_3d = self._transform_point(\n            detection.position_3d, transform\n        )\n\n        # Check if we\'ve seen this object before\n        object_id = self._find_matching_object(\n            detection.class_name, position_3d\n        )\n\n        if object_id:\n            # Update existing object\n            self.objects[object_id].update(detection, position_3d)\n        else:\n            # Create new object\n            object_id = f"obj_{len(self.objects)}"\n            self.objects[object_id] = DetectedObject(\n                class_name=detection.class_name,\n                position_3d=position_3d,\n                confidence=detection.confidence\n            )\n\n        return object_id\n\n    def _find_matching_object(self, class_name, position_3d, distance_threshold=0.2):\n        """Check if we\'ve seen this object before (within distance threshold)."""\n        for obj_id, obj in self.objects.items():\n            if obj.class_name == class_name:\n                dist = np.linalg.norm(np.array(obj.position_3d) - np.array(position_3d))\n                if dist < distance_threshold:\n                    return obj_id\n        return None\n'})}),"\n",(0,r.jsx)(n.h3,{id:"affordance-detection",children:"Affordance Detection"}),"\n",(0,r.jsx)(n.p,{children:"A simple affordance classifier learns to predict how objects can be interacted with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: perception/affordance_classifier.py\nimport torch\nfrom torchvision import models, transforms\n\nclass AffordanceClassifier:\n    def __init__(self, model_path):\n        # Load pretrained ResNet-50 backbone\n        self.model = models.resnet50(pretrained=True)\n        # Replace final layer for affordance classification\n        num_affordances = 6  # graspable, movable, fragile, openable, pushable, stackable\n        self.model.fc = torch.nn.Linear(2048, num_affordances)\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.eval()\n\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                std=[0.229, 0.224, 0.225])\n        ])\n\n    def predict(self, image, detection_bbox):\n        \"\"\"\n        Predict affordances for an object detected in the image.\n\n        Args:\n            image: RGB image (numpy array)\n            detection_bbox: [x_min, y_min, x_max, y_max] in pixels\n\n        Returns:\n            affordances: {'graspable': 0.95, 'movable': 0.87, ...}\n        \"\"\"\n        # Crop object region from image\n        x_min, y_min, x_max, y_max = detection_bbox\n        object_crop = image[y_min:y_max, x_min:x_max]\n\n        # Preprocess\n        input_tensor = self.transform(object_crop).unsqueeze(0)\n\n        # Inference\n        with torch.no_grad():\n            logits = self.model(input_tensor)\n            probabilities = torch.sigmoid(logits)[0]  # Multi-label (object can be both movable and fragile)\n\n        affordance_names = ['graspable', 'movable', 'fragile', 'openable', 'pushable', 'stackable']\n        return {name: float(prob) for name, prob in zip(affordance_names, probabilities)}\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-3-ros-2-perception-integration",children:"Part 3: ROS 2 Perception Integration"}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-perception-node-architecture",children:"ROS 2 Perception Node Architecture"}),"\n",(0,r.jsx)(n.p,{children:"A perception node in ROS 2 follows this pattern:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# File: ros2_perception_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Point\nfrom std_msgs.msg import Float32MultiArray\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass PerceptionNode(Node):\n    \"\"\"\n    ROS 2 perception node that:\n    1. Subscribes to RGB and depth topics\n    2. Runs detection and segmentation models\n    3. Publishes scene graph (as a JSON message or custom message type)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('perception_node')\n\n        # Subscribers\n        self.rgb_subscription = self.create_subscription(\n            Image, '/camera/color/image_raw', self.rgb_callback, 10\n        )\n        self.depth_subscription = self.create_subscription(\n            Image, '/camera/depth/image_rect_raw', self.depth_callback, 10\n        )\n        self.camera_info_subscription = self.create_subscription(\n            CameraInfo, '/camera/color/camera_info', self.camera_info_callback, 10\n        )\n\n        # Publishers\n        self.scene_graph_publisher = self.create_publisher(String, '/perception/scene_graph', 10)\n        self.detection_image_publisher = self.create_publisher(Image, '/perception/detections_viz', 10)\n\n        # Initialize detector (YOLO)\n        from ultralytics import YOLO\n        self.detector = YOLO('yolov8m.pt')  # Medium model: good balance\n\n        # Bridge for ROS2 <-> OpenCV conversion\n        self.bridge = CvBridge()\n\n        # Storage for latest frames\n        self.latest_rgb = None\n        self.latest_depth = None\n        self.camera_intrinsics = None\n\n    def rgb_callback(self, msg):\n        \"\"\"Process RGB image.\"\"\"\n        try:\n            self.latest_rgb = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n            self.process_frame()\n        except Exception as e:\n            self.get_logger().error(f'RGB callback error: {e}')\n\n    def depth_callback(self, msg):\n        \"\"\"Process depth image.\"\"\"\n        try:\n            self.latest_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n        except Exception as e:\n            self.get_logger().error(f'Depth callback error: {e}')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Store camera intrinsics.\"\"\"\n        self.camera_intrinsics = np.array(msg.K).reshape(3, 3)\n\n    def process_frame(self):\n        \"\"\"Main processing: detect, extract affordances, build scene graph.\"\"\"\n        if self.latest_rgb is None or self.latest_depth is None:\n            return\n\n        # Run YOLO detection\n        results = self.detector(self.latest_rgb)\n        detections = results[0]\n\n        scene_graph = {\n            'timestamp': self.get_clock().now().to_msg(),\n            'objects': []\n        }\n\n        for det in detections.boxes:\n            # Extract bounding box and class\n            x_min, y_min, x_max, y_max = map(int, det.xyxy[0])\n            class_id = int(det.cls[0])\n            confidence = float(det.conf[0])\n            class_name = self.detector.names[class_id]\n\n            # Get 3D position from depth\n            depth_crop = self.latest_depth[y_min:y_max, x_min:x_max]\n            if depth_crop.size > 0:\n                center_depth = np.nanmedian(depth_crop)\n                if np.isfinite(center_depth):\n                    cx = (x_min + x_max) // 2\n                    cy = (y_min + y_max) // 2\n                    x_3d = (cx - self.camera_intrinsics[0, 2]) * center_depth / self.camera_intrinsics[0, 0]\n                    y_3d = (cy - self.camera_intrinsics[1, 2]) * center_depth / self.camera_intrinsics[1, 1]\n                    z_3d = center_depth\n\n                    scene_graph['objects'].append({\n                        'id': f'{class_name}_{len(scene_graph[\"objects\"])}',\n                        'class': class_name,\n                        'confidence': confidence,\n                        'bbox': [x_min, y_min, x_max, y_max],\n                        'position_3d': [float(x_3d), float(y_3d), float(z_3d)],\n                        'affordances': {\n                            'graspable': class_name not in ['table', 'floor', 'wall'],\n                            'movable': class_name in ['cup', 'bottle', 'book', 'toy']\n                        }\n                    })\n\n        # Publish scene graph\n        from std_msgs.msg import String\n        import json\n        msg = String()\n        msg.data = json.dumps(scene_graph)\n        self.scene_graph_publisher.publish(msg)\n\n        # Publish visualization\n        viz_image = self.latest_rgb.copy()\n        for obj in scene_graph['objects']:\n            x_min, y_min, x_max, y_max = obj['bbox']\n            cv2.rectangle(viz_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            cv2.putText(viz_image, f\"{obj['class']} ({obj['confidence']:.2f})\",\n                       (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        viz_msg = self.bridge.cv2_to_imgmsg(viz_image, encoding='bgr8')\n        self.detection_image_publisher.publish(viz_msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = PerceptionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Points"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Subscribes to RGB and depth topics (standard ROS 2 message types)"}),"\n",(0,r.jsx)(n.li,{children:"Runs YOLO detection on RGB"}),"\n",(0,r.jsx)(n.li,{children:"Extracts 3D coordinates using depth + camera intrinsics"}),"\n",(0,r.jsx)(n.li,{children:"Publishes scene graph as JSON (or custom message type)"}),"\n",(0,r.jsx)(n.li,{children:"Includes visualization for debugging"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-4-labs--exercises",children:"Part 4: Labs & Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"lab-1-detect-objects-in-a-scene-image-1-hour",children:"Lab 1: Detect Objects in a Scene Image (1 hour)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Use YOLO to detect objects in an image and evaluate accuracy."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Materials Needed"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Python environment with YOLO and OpenCV installed"}),"\n",(0,r.jsx)(n.li,{children:"Sample images (can use COCO validation set or your own photos)"}),"\n",(0,r.jsx)(n.li,{children:"~1 hour"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Instructions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Install dependencies"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install ultralytics opencv-python numpy\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Load and run YOLO"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\nimport cv2\n\n# Load model\nmodel = YOLO('yolov8m.pt')\n\n# Run inference\nimage_path = 'sample_image.jpg'\nresults = model(image_path)\n\n# Analyze results\nfor detection in results[0].boxes:\n    print(f\"Class: {detection.cls}, Confidence: {detection.conf}\")\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Evaluate accuracy"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Run on 5-10 images"}),"\n",(0,r.jsx)(n.li,{children:"Count true positives (correct detections)"}),"\n",(0,r.jsx)(n.li,{children:"Count false positives (incorrectly detected)"}),"\n",(0,r.jsx)(n.li,{children:"Count false negatives (missed objects)"}),"\n",(0,r.jsx)(n.li,{children:"Calculate: Precision = TP / (TP + FP), Recall = TP / (TP + FN)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visualize results"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Draw bounding boxes on images"}),"\n",(0,r.jsx)(n.li,{children:"Save visualization"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","YOLO successfully detects objects in sample images"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","At least 3 detections analyzed with confidence scores"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Precision and recall calculated from 5+ images"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Visualization saved showing bounding boxes"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 1 hour"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lab-2-identify-pickable-objects-15-hours",children:"Lab 2: Identify Pickable Objects (1.5 hours)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Build a classifier to identify which objects are graspable/pickable."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Materials Needed"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Image dataset with graspable/non-graspable labels (or create your own)"}),"\n",(0,r.jsx)(n.li,{children:"PyTorch and torchvision"}),"\n",(0,r.jsx)(n.li,{children:"~1.5 hours"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Instructions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prepare dataset"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Collect or download object images (e.g., COCO or Google Images)"}),"\n",(0,r.jsx)(n.li,{children:"Label each as graspable (cup, ball, toy) or non-graspable (table, wall, floor)"}),"\n",(0,r.jsx)(n.li,{children:"Split into train (70%) and test (30%)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Train a classifier"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Load pretrained ResNet\nmodel = models.resnet18(pretrained=True)\nmodel.fc = torch.nn.Linear(512, 2)  # Binary classification\n\n# Training loop\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nfor epoch in range(10):\n    for images, labels in train_loader:\n        logits = model(images)\n        loss = loss_fn(logits, labels)\n        loss.backward()\n        optimizer.step()\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Evaluate on test set"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Run on test images"}),"\n",(0,r.jsx)(n.li,{children:"Calculate accuracy, precision, recall"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visualize predictions"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Show images with predicted graspability"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Dataset created with 50+ labeled images"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Model trained with training and validation loss plotted"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test accuracy \u2265 75%"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Confusion matrix or per-class metrics reported"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 1.5 hours"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"lab-3-convert-scene-to-llm-readable-format-1-hour",children:"Lab 3: Convert Scene to LLM-Readable Format (1 hour)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Transform detected objects into a natural language scene description suitable for an LLM."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Materials Needed"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Detected objects (from Lab 1) with bounding boxes and positions"}),"\n",(0,r.jsx)(n.li,{children:"LLM API (mock or real, optional)"}),"\n",(0,r.jsx)(n.li,{children:"~1 hour"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Instructions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Convert detections to structured data"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"detections = [\n    {'class': 'cup', 'color': 'red', 'position': [0.5, 0.3, 0.8], 'graspable': True},\n    {'class': 'table', 'position': [0.5, 0.0, 0.0], 'graspable': False},\n]\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Extract spatial relationships"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"For each pair of objects, determine relationship (on, next_to, behind, in_front_of)"}),"\n",(0,r.jsxs)(n.li,{children:["Example: cup is ",(0,r.jsx)(n.code,{children:"on"})," the table"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Generate natural language description"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def scene_to_text(detections):\n    text = "Scene description:\\n"\n    for det in detections:\n        text += f"- A {det[\'color\']} {det[\'class\']} at position {det[\'position\']}\\n"\n    # Add relationships\n    text += "\\nRelationships:\\n"\n    text += "- The red cup is on the table\\n"\n    return text\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prompt an LLM"})," (optional):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\n\nclient = OpenAI()\nprompt = f"""\n{scene_text}\n\nUser command: "Pick up the red cup"\n\nTask plan (as JSON):\n"""\n\nresponse = client.chat.completions.create(\n    model="gpt-4",\n    messages=[{"role": "user", "content": prompt}]\n)\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Detections converted to structured data format"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Spatial relationships extracted from at least 3 pairs of objects"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Natural language scene description generated"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","(Optional) LLM prompted with scene and returned task decomposition"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 1 hour"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"end-of-chapter-exercises",children:"End-of-Chapter Exercises"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercise 1: Model Comparison"})," (Beginner)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Compare YOLO, Mask R-CNN, and SAM on a sample image"}),"\n",(0,r.jsx)(n.li,{children:"Create a table comparing speed, accuracy, and use cases"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercise 2: Affordance Labeling"})," (Intermediate)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Given a set of objects, design affordance labels"}),"\n",(0,r.jsx)(n.li,{children:"Create a rule-based classifier (if-then rules for graspability)"}),"\n",(0,r.jsx)(n.li,{children:"Test on household objects"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Exercise 3: Scene Graph Construction"})," (Advanced)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Design a complete scene graph format for VLA"}),"\n",(0,r.jsx)(n.li,{children:"Include: objects, affordances, spatial relationships"}),"\n",(0,r.jsx)(n.li,{children:"Implement JSON schema with validation"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"capstone-integration",children:"Capstone Integration"}),"\n",(0,r.jsxs)(n.p,{children:["Chapter 2 teaches you to ",(0,r.jsx)(n.strong,{children:"perceive and understand"})," the environment. In the capstone:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"You'll run a real-time perception pipeline on a simulated scene"}),"\n",(0,r.jsx)(n.li,{children:"The scene graph you build will feed into the LLM (Chapter 3)"}),"\n",(0,r.jsx)(n.li,{children:"The LLM will reason about which objects are graspable, movable, etc."}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 controllers (Chapter 4) will execute the resulting task plan"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Vision is the first pillar of VLA. Robots must perceive their environment to reason about tasks. Key takeaways:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Object detection"})," (YOLO) provides fast, real-time identification"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth fusion"})," enables 3D localization for grasping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Affordance detection"})," teaches robots how to interact with objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scene graphs"})," provide structured, LLM-ready representations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 integration"})," makes perception part of the robot's middleware stack"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:'Redmon, J., & Farhadi, A. (2018). "YOLOv3: An Incremental Improvement." arXiv:1804.02767'}),"\n",(0,r.jsx)(n.li,{children:'He, K., et al. (2017). "Mask R-CNN." ICCV'}),"\n",(0,r.jsx)(n.li,{children:'Kirillov, A., et al. (2023). "Segment Anything." arXiv:2304.02135'}),"\n",(0,r.jsx)(n.li,{children:'Shi, Z., et al. (2024). "Grounding DINO: Marrying DINO with Grounded Pre-Training." arXiv:2401.14207'}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next Chapter"}),": ",(0,r.jsx)(n.a,{href:"/ai_native-textbook/docs/module-4/chapter-3-language-planning-whisper-llm",children:"Chapter 3: Language Planning with Whisper & LLMs"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>o});var i=s(6540);const r={},t=i.createContext(r);function a(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);