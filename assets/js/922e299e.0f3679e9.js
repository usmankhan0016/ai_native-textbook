"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[1356],{7568:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-3/chapter-2-isaac-sim","title":"Chapter 2: Isaac Sim for Photorealistic Robotics Simulation","description":"Duration Intermediate | Week: 8","source":"@site/docs/module-3/chapter-2-isaac-sim.md","sourceDirName":"module-3","slug":"/module-3/chapter-2-isaac-sim","permalink":"/ai_native-textbook/docs/module-3/chapter-2-isaac-sim","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"isaac-sim","permalink":"/ai_native-textbook/docs/tags/isaac-sim"},{"inline":true,"label":"simulation","permalink":"/ai_native-textbook/docs/tags/simulation"},{"inline":true,"label":"physics","permalink":"/ai_native-textbook/docs/tags/physics"},{"inline":true,"label":"sensors","permalink":"/ai_native-textbook/docs/tags/sensors"},{"inline":true,"label":"urdf","permalink":"/ai_native-textbook/docs/tags/urdf"},{"inline":true,"label":"usd","permalink":"/ai_native-textbook/docs/tags/usd"},{"inline":true,"label":"synthetic-data","permalink":"/ai_native-textbook/docs/tags/synthetic-data"}],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"Ch. 2: Isaac Sim Robotics","title":"Chapter 2: Isaac Sim for Photorealistic Robotics Simulation","tags":["isaac-sim","simulation","physics","sensors","urdf","usd","synthetic-data"],"difficulty":"Intermediate","module":3,"week":8,"prerequisites":["Chapter 1: Isaac Introduction","Module 2: Digital Twins","URDF syntax knowledge"],"estimated_time":"5-6 hours","topics":["USD workflows","PhysX 5","sensor simulation","ROS 2 integration","synthetic data export","digital twins"]},"sidebar":"tutorialSidebar","previous":{"title":"Ch. 1: Isaac Platform Intro","permalink":"/ai_native-textbook/docs/module-3/chapter-1-isaac-introduction"},"next":{"title":"Ch. 3: Isaac ROS Perception","permalink":"/ai_native-textbook/docs/module-3/chapter-3-isaac-ros"}}');var t=s(4848),a=s(8453);const r={sidebar_position:2,sidebar_label:"Ch. 2: Isaac Sim Robotics",title:"Chapter 2: Isaac Sim for Photorealistic Robotics Simulation",tags:["isaac-sim","simulation","physics","sensors","urdf","usd","synthetic-data"],difficulty:"Intermediate",module:3,week:8,prerequisites:["Chapter 1: Isaac Introduction","Module 2: Digital Twins","URDF syntax knowledge"],estimated_time:"5-6 hours",topics:["USD workflows","PhysX 5","sensor simulation","ROS 2 integration","synthetic data export","digital twins"]},o="Chapter 2: Isaac Sim for Photorealistic Robotics Simulation",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"<strong>USD (Universal Scene Description)</strong>",id:"usd-universal-scene-description",level:3},{value:"<strong>URDF to USD Conversion</strong>",id:"urdf-to-usd-conversion",level:3},{value:"<strong>PhysX 5</strong>",id:"physx-5",level:3},{value:"<strong>Sensor Simulation</strong>",id:"sensor-simulation",level:3},{value:"<strong>Synthetic Dataset</strong>",id:"synthetic-dataset",level:3},{value:"<strong>Domain Randomization</strong>",id:"domain-randomization",level:3},{value:"<strong>Real-Time Rendering</strong>",id:"real-time-rendering",level:3},{value:"Part 1: USD vs URDF Workflow",id:"part-1-usd-vs-urdf-workflow",level:2},{value:"Why USD Instead of URDF?",id:"why-usd-instead-of-urdf",level:3},{value:"Conversion Workflow",id:"conversion-workflow",level:3},{value:"Part 2: PhysX 5 Physics Configuration",id:"part-2-physx-5-physics-configuration",level:2},{value:"Critical Parameters",id:"critical-parameters",level:3},{value:"Tuning Recipe",id:"tuning-recipe",level:3},{value:"Real-World Comparison Example",id:"real-world-comparison-example",level:3},{value:"Part 3: Sensor Simulation &amp; Realism",id:"part-3-sensor-simulation--realism",level:2},{value:"RGB Camera Simulation",id:"rgb-camera-simulation",level:3},{value:"Depth (RGB-D) Camera",id:"depth-rgb-d-camera",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:3},{value:"IMU (Inertial Measurement Unit)",id:"imu-inertial-measurement-unit",level:3},{value:"Part 4: ROS 2 Integration",id:"part-4-ros-2-integration",level:2},{value:"Part 5: Hands-On Labs",id:"part-5-hands-on-labs",level:2},{value:"Lab 1: Convert URDF to USD and Load Humanoid (1.5 hours)",id:"lab-1-convert-urdf-to-usd-and-load-humanoid-15-hours",level:3},{value:"Lab 2: Configure Sensors &amp; Verify Output (1.5 hours)",id:"lab-2-configure-sensors--verify-output-15-hours",level:3},{value:"Lab 3: Export Synthetic Dataset (2 hours)",id:"lab-3-export-synthetic-dataset-2-hours",level:3},{value:"Part 6: Code Examples",id:"part-6-code-examples",level:2},{value:"Example 1: PhysX Parameter Tuning Script",id:"example-1-physx-parameter-tuning-script",level:3},{value:"Example 2: Sensor Data Recorder",id:"example-2-sensor-data-recorder",level:3},{value:"Example 3: Dataset COCO Export",id:"example-3-dataset-coco-export",level:3},{value:"Part 7: Architecture Diagrams",id:"part-7-architecture-diagrams",level:2},{value:"Isaac Sim to ROS 2 Data Flow",id:"isaac-sim-to-ros-2-data-flow",level:3},{value:"Synthetic Data Generation Pipeline",id:"synthetic-data-generation-pipeline",level:3},{value:"Part 8: End-of-Chapter Exercises",id:"part-8-end-of-chapter-exercises",level:2},{value:"Exercise 1: URDF Validation and Conversion (45 min)",id:"exercise-1-urdf-validation-and-conversion-45-min",level:3},{value:"Exercise 2: Physics Tuning for Realism (1 hour)",id:"exercise-2-physics-tuning-for-realism-1-hour",level:3},{value:"Exercise 3: Multi-Sensor Configuration (1 hour)",id:"exercise-3-multi-sensor-configuration-1-hour",level:3},{value:"Exercise 4: Synthetic Dataset Quality (1.5 hours)",id:"exercise-4-synthetic-dataset-quality-15-hours",level:3},{value:"Part 9: Capstone Integration",id:"part-9-capstone-integration",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Additional Resources",id:"additional-resources",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-2-isaac-sim-for-photorealistic-robotics-simulation",children:"Chapter 2: Isaac Sim for Photorealistic Robotics Simulation"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 5-6 hours | ",(0,t.jsx)(n.strong,{children:"Difficulty"}),": Intermediate | ",(0,t.jsx)(n.strong,{children:"Week"}),": 8"]}),"\n",(0,t.jsx)(n.p,{children:"Master photorealistic physics simulation and synthetic data generation."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By completing this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Convert URDF to USD"})," \u2014 Transform robot descriptions (URDF) into Isaac Sim's native format (USD) and validate in physics engine"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configure PhysX Parameters"})," \u2014 Tune friction, damping, contact thresholds, and solver iterations to match real-world behavior"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simulate Realistic Sensors"})," \u2014 Add RGB-D cameras, LiDAR, IMU sensors with noise models and calibration parameters"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integrate with ROS 2"})," \u2014 Publish sensor data and subscribe to control commands via Isaac ROS bridge"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Export Synthetic Datasets"})," \u2014 Generate 1000+ labeled frames (images, depth, masks) for machine learning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Debug Physics Issues"})," \u2014 Identify and fix instabilities, penetrations, and unrealistic behavior"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Optimize Simulation Speed"})," \u2014 Achieve real-time performance without sacrificing physics accuracy"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automate Data Collection"})," \u2014 Create scripts to generate high-volume synthetic data for training pipelines"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"usd-universal-scene-description",children:(0,t.jsx)(n.strong,{children:"USD (Universal Scene Description)"})}),"\n",(0,t.jsx)(n.p,{children:"Industry standard for 3D scene representation (developed by Pixar). Replaces URDF in Isaac Sim for superior asset management, hierarchical composition, and real-time editing. All Isaac Sim scenes are USD files."}),"\n",(0,t.jsx)(n.h3,{id:"urdf-to-usd-conversion",children:(0,t.jsx)(n.strong,{children:"URDF to USD Conversion"})}),"\n",(0,t.jsx)(n.p,{children:"Process of transforming ROS robot descriptions (URDF XML) into Isaac Sim's native USD format. Preserves kinematics, physics properties, sensor definitions; enables use of existing robot models."}),"\n",(0,t.jsx)(n.h3,{id:"physx-5",children:(0,t.jsx)(n.strong,{children:"PhysX 5"})}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA's GPU-accelerated physics engine used by Isaac Sim. Simulates rigid bodies, constraints, contacts, friction. Parameters (friction, damping, contact offset) must be tuned per robot to match real-world dynamics."}),"\n",(0,t.jsx)(n.h3,{id:"sensor-simulation",children:(0,t.jsx)(n.strong,{children:"Sensor Simulation"})}),"\n",(0,t.jsx)(n.p,{children:"Synthetic generation of camera (RGB, depth), LiDAR, and IMU data in simulation. Includes realistic noise models, occlusion, and distortion matching real sensor hardware."}),"\n",(0,t.jsx)(n.h3,{id:"synthetic-dataset",children:(0,t.jsx)(n.strong,{children:"Synthetic Dataset"})}),"\n",(0,t.jsx)(n.p,{children:"Machine learning training data generated entirely in simulation (not real world). Includes images, annotations (bounding boxes, segmentation, depth), and ground truth labels. Eliminates manual labeling and enables large-scale data generation."}),"\n",(0,t.jsx)(n.h3,{id:"domain-randomization",children:(0,t.jsx)(n.strong,{children:"Domain Randomization"})}),"\n",(0,t.jsx)(n.p,{children:"Technique for improving sim-to-real transfer. Randomizes visual appearance (textures, lighting), physics properties (mass, friction), and scene layout during training. Prevents model from overfitting to simulation artifacts."}),"\n",(0,t.jsx)(n.h3,{id:"real-time-rendering",children:(0,t.jsx)(n.strong,{children:"Real-Time Rendering"})}),"\n",(0,t.jsx)(n.p,{children:"Photorealistic image generation using RTX ray tracing. Critical for synthetic data quality (models trained on synthetic must transfer to real images)."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-1-usd-vs-urdf-workflow",children:"Part 1: USD vs URDF Workflow"}),"\n",(0,t.jsx)(n.h3,{id:"why-usd-instead-of-urdf",children:"Why USD Instead of URDF?"}),"\n",(0,t.jsxs)(n.p,{children:["In Module 2, you used URDF (Unified Robot Description Format) in Gazebo. Isaac Sim uses ",(0,t.jsx)(n.strong,{children:"USD (Universal Scene Description)"})," instead. Why?"]}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Aspect"}),(0,t.jsx)(n.th,{children:"URDF"}),(0,t.jsx)(n.th,{children:"USD"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Format"})}),(0,t.jsx)(n.td,{children:"XML text"}),(0,t.jsx)(n.td,{children:"Structured data + binary assets"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"3D Capability"})}),(0,t.jsx)(n.td,{children:"Minimal (joints, links)"}),(0,t.jsx)(n.td,{children:"Full 3D scene (meshes, materials, lights, cameras)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Real-Time Editing"})}),(0,t.jsx)(n.td,{children:"No (restart simulation)"}),(0,t.jsx)(n.td,{children:"Yes (edit while running)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Asset Reuse"})}),(0,t.jsx)(n.td,{children:"Limited"}),(0,t.jsx)(n.td,{children:"Rich (materials, models, scenes)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Photorealism"})}),(0,t.jsx)(n.td,{children:"Not designed for"}),(0,t.jsx)(n.td,{children:"Native support"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Scalability"})}),(0,t.jsx)(n.td,{children:"Single robot"}),(0,t.jsx)(n.td,{children:"Complex multi-asset scenes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Industry Adoption"})}),(0,t.jsx)(n.td,{children:"ROS-specific"}),(0,t.jsx)(n.td,{children:"Film (Pixar), VFX, professional visualization"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Practical Impact"}),": USD enables you to create production-grade digital twins with photorealistic rendering, while URDF only described kinematics."]}),"\n",(0,t.jsx)(n.h3,{id:"conversion-workflow",children:"Conversion Workflow"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Step 1: Start with URDF                               \u2502\n\u2502  (e.g., humanoid.urdf from robot manufacturer)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Step 2: Validate URDF                                 \u2502\n\u2502  - Check joint definitions, mass, inertia              \u2502\n\u2502  - Verify mesh file paths exist                        \u2502\n\u2502  - Fix common issues (floating base, invalid TF)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Step 3: Convert to USD                                \u2502\n\u2502  - Use Isaac Sim conversion tool or python API         \u2502\n\u2502  - Preserves kinematics, adds physics schema           \u2502\n\u2502  - Generates USD stage file                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Step 4: Load in Isaac Sim                             \u2502\n\u2502  - Add sensors (cameras, LiDAR, IMU)                   \u2502\n\u2502  - Configure physics parameters                        \u2502\n\u2502  - Set up ROS 2 bridge                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Step 5: Validate Physics                              \u2502\n\u2502  - Run simulation, compare behavior to real robot      \u2502\n\u2502  - Tune friction, damping, contact parameters         \u2502\n\u2502  - Iterate until behavior matches                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Step 6: Export Dataset                                \u2502\n\u2502  - Configure camera rendering, export images          \u2502\n\u2502  - Generate annotations (depth, segmentation)         \u2502\n\u2502  - Ready for ML training                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-2-physx-5-physics-configuration",children:"Part 2: PhysX 5 Physics Configuration"}),"\n",(0,t.jsx)(n.p,{children:"PhysX 5 is the heart of Isaac Sim physics. To simulate a humanoid accurately, you need to understand and tune key parameters."}),"\n",(0,t.jsx)(n.h3,{id:"critical-parameters",children:"Critical Parameters"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Friction"})," (how much robot slides)"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Default: 0.5\nRange: 0.0 (ice) \u2192 1.0+ (concrete)\n\nFor humanoid:\n- Shoe contact: 0.6-0.8 (rubber on floor)\n- Joint friction: 0.1-0.3 (internal damping)\n\nEffect: Too low \u2192 robot slides uncontrollably\n        Too high \u2192 robot gets "stuck" on small obstacles\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Damping"})," (how quickly motion stops)"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'Default: 0.0 (no damping)\nRange: 0.0 \u2192 1.0\n\nFor humanoid joints:\n- Angular damping: 0.5-0.9 (reduces oscillation)\n- Linear damping: 0.1-0.3 (air resistance)\n\nEffect: Too low \u2192 joints oscillate forever after impact\n        Too high \u2192 joints move slowly, feel "sluggish"\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Contact Offset"}),' (physics "softness")']}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Default: 0.02 meters\nRange: 0.001 \u2192 0.1\n\nFor humanoid feet:\n- Use 0.01-0.02m for stability\n- Larger value \u2192 feet penetrate ground slightly, more stable\n- Smaller value \u2192 feet stick exactly to ground, but unstable\n\nEffect: Too large \u2192 humanoid floats above ground\n        Too small \u2192 physics becomes unstable\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solver Iterations"})," (accuracy vs. speed)"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Default: 4\nRange: 1 \u2192 32\n\nFor production:\n- Real-time: 4-8 iterations\n- High accuracy: 16+ iterations\n\nEffect: More iterations \u2192 more accurate, slower\n        Fewer iterations \u2192 faster, less accurate\n"})}),"\n",(0,t.jsx)(n.h3,{id:"tuning-recipe",children:"Tuning Recipe"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Start with defaults"}),": PhysX 5 defaults work for many robots"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run simulator"}),": Press play, observe behavior"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Identify issues"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Sliding? \u2192 Increase friction"}),"\n",(0,t.jsx)(n.li,{children:"Oscillating joints? \u2192 Increase damping"}),"\n",(0,t.jsx)(n.li,{children:"Unstable? \u2192 Increase contact offset"}),"\n",(0,t.jsx)(n.li,{children:"Too slow? \u2192 Decrease solver iterations"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Measure metrics"}),": Compare to real robot (gait cycle time, balance, etc.)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Iterate"}),": Adjust one parameter at a time"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"real-world-comparison-example",children:"Real-World Comparison Example"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Parameter"}),(0,t.jsx)(n.th,{children:"Real Robot"}),(0,t.jsx)(n.th,{children:"Isaac Sim (Untuned)"}),(0,t.jsx)(n.th,{children:"Isaac Sim (Tuned)"}),(0,t.jsx)(n.th,{children:"Match?"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Friction Coefficient"}),(0,t.jsx)(n.td,{children:"0.7"}),(0,t.jsx)(n.td,{children:"0.5"}),(0,t.jsx)(n.td,{children:"0.7"}),(0,t.jsx)(n.td,{children:"\u2705 Yes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Gait Cycle Time"}),(0,t.jsx)(n.td,{children:"1.2 sec"}),(0,t.jsx)(n.td,{children:"0.8 sec"}),(0,t.jsx)(n.td,{children:"1.19 sec"}),(0,t.jsx)(n.td,{children:"\u2705 Close"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Joint Damping"}),(0,t.jsx)(n.td,{children:"0.6"}),(0,t.jsx)(n.td,{children:"0.1"}),(0,t.jsx)(n.td,{children:"0.6"}),(0,t.jsx)(n.td,{children:"\u2705 Yes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Standing Stability"}),(0,t.jsx)(n.td,{children:"Rock-solid"}),(0,t.jsx)(n.td,{children:"Wobbly"}),(0,t.jsx)(n.td,{children:"Rock-solid"}),(0,t.jsx)(n.td,{children:"\u2705 Yes"})]})]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-3-sensor-simulation--realism",children:"Part 3: Sensor Simulation & Realism"}),"\n",(0,t.jsx)(n.h3,{id:"rgb-camera-simulation",children:"RGB Camera Simulation"}),"\n",(0,t.jsx)(n.p,{children:"Real RGB camera in Isaac Sim:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Configure camera sensor\ncamera = {\n    "width": 1280,           # Resolution\n    "height": 720,\n    "focal_length": 24.0,    # mm (determines field of view)\n    "intrinsics": {\n        "fx": 954.0,         # Focal length in pixels\n        "fy": 954.0,\n        "cx": 640.0,         # Principal point (center)\n        "cy": 360.0,\n    },\n    "distortion": {\n        "k1": 0.1,           # Radial distortion\n        "k2": 0.02,\n        "p1": 0.001,         # Tangential distortion\n        "p2": 0.001,\n    },\n    "noise": {\n        "gaussian_mean": 0.0,\n        "gaussian_std": 0.01,  # 1% pixel noise\n    },\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What each parameter does"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intrinsics"}),": Convert 3D world coordinates to 2D image pixels"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Distortion"}),": Model barrel/pincushion effects from lens imperfections"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Noise"}),": Add realistic sensor noise (analog gain, pixel noise)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"depth-rgb-d-camera",children:"Depth (RGB-D) Camera"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# RGB-D camera (generates both RGB image AND depth map)\nrgbd_camera = {\n    "rgb": {            # Color image\n        "width": 640,\n        "height": 480,\n    },\n    "depth": {          # Depth map (per-pixel distance)\n        "width": 640,\n        "height": 480,\n        "near_plane": 0.1,      # Minimum distance (meters)\n        "far_plane": 10.0,      # Maximum distance\n        "noise_std": 0.01,      # Depth noise (cm-scale)\n    },\n    "occlusion_support": True,  # Realistic occlusion handling\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why depth matters"}),": RGB images don't tell you distance. Depth cameras (like RealSense) measure per-pixel distance to objects. Essential for 3D perception."]}),"\n",(0,t.jsx)(n.h3,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# LiDAR (360\xb0 laser range finder)\nlidar = {\n    "num_rays": 64,              # 64 laser beams\n    "horizontal_fov": 360.0,     # Full circle\n    "vertical_fov": 26.8,        # Vertical coverage\n    "range": {\n        "min": 0.3,              # Minimum distance\n        "max": 100.0,            # Maximum distance\n    },\n    "noise_std": 0.01,           # Measurement noise\n    "rotation_speed": 10.0,      # Hz (10 rotations/sec)\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why LiDAR matters"}),": Gives 3D point cloud of environment. Better for obstacle avoidance and SLAM than cameras alone."]}),"\n",(0,t.jsx)(n.h3,{id:"imu-inertial-measurement-unit",children:"IMU (Inertial Measurement Unit)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# IMU (acceleration + rotation)\nimu = {\n    "accelerometer": {\n        "range": [-16, 16],      # \xb116 G (gravity units)\n        "noise_std": 0.05,       # Acceleration noise\n    },\n    "gyroscope": {\n        "range": [-500, 500],    # \xb1500 deg/sec\n        "noise_std": 0.1,        # Rotation rate noise\n    },\n    "update_rate": 100.0,        # Hz (100 samples/sec)\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why IMU matters"}),": Measures robot's own acceleration and rotation. Used for balance control, orientation estimation."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-4-ros-2-integration",children:"Part 4: ROS 2 Integration"}),"\n",(0,t.jsx)(n.p,{children:"How sensor data flows from Isaac Sim to your control algorithms:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Isaac Sim Physics Engine\n        \u2193 (every frame)\nCamera captures RGB image\nDepth sensor measures distances\nLiDAR scans environment\nIMU measures acceleration/rotation\n        \u2193\nIsaac ROS Bridge\n        \u2193 (publishes to ROS 2 topics)\n/camera/image_raw           (RGB image)\n/camera/depth/image_raw     (Depth map)\n/lidar/points               (Point cloud)\n/imu/data                   (Accelerometer + Gyroscope)\n        \u2193\nYour ROS 2 Nodes Subscribe\n        \u2193\nObject Detection / SLAM / Control\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Key insight"}),": Same ROS 2 topics work in both simulation AND on real robot (after Chapter 5 deployment)."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-5-hands-on-labs",children:"Part 5: Hands-On Labs"}),"\n",(0,t.jsx)(n.h3,{id:"lab-1-convert-urdf-to-usd-and-load-humanoid-15-hours",children:"Lab 1: Convert URDF to USD and Load Humanoid (1.5 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Take an existing humanoid URDF, convert to USD, load in Isaac Sim."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prerequisites"}),": Chapter 1 complete, Isaac Sim installed"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 1a: Download/Find a Humanoid URDF"})}),"\n",(0,t.jsx)(n.p,{children:"Option 1: Use a publicly available URDF:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Download Boston Dynamics Atlas URDF (public domain)\nwget https://raw.githubusercontent.com/osrf/gazebo_models/master/atlas/model.urdf\n\n# Verify file\ncat model.urdf | head -20\n# Should show: <robot name="atlas"> or similar\n'})}),"\n",(0,t.jsx)(n.p,{children:"Option 2: Use your own humanoid URDF if available"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 1b: Validate URDF Structure"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Check for common URDF issues\npython3 << \'EOF\'\nimport xml.etree.ElementTree as ET\n\nurdf_file = "model.urdf"\ntree = ET.parse(urdf_file)\nroot = tree.getroot()\n\nrobot_name = root.get("name")\njoints = root.findall("joint")\nlinks = root.findall("link")\n\nprint(f"Robot: {robot_name}")\nprint(f"Joints: {len(joints)}")\nprint(f"Links: {len(links)}")\n\n# Check for common issues\nfor joint in joints:\n    joint_name = joint.get("name")\n    joint_type = joint.get("type")\n    if joint_type not in ["revolute", "prismatic", "fixed", "continuous"]:\n        print(f"\u26a0\ufe0f  Unknown joint type: {joint_type}")\nEOF\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 1c: Convert URDF to USD"})}),"\n",(0,t.jsx)(n.p,{children:"Using Isaac Sim Python API:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: convert_urdf_to_usd.py\n"""\nConvert URDF to USD format for Isaac Sim.\n"""\n\nfrom omni.isaac.kit import SimulationApp\nfrom omni.isaac.core.utils.extensions import enable_extension\n\n# Enable USD composition\nenable_extension("omni.usd")\n\nsimulation_app = SimulationApp({"headless": True})\n\nfrom pxr import Usd\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.importer.urdf import _urdf\n\n# Convert URDF to USD\nurdf_path = "/home/user/model.urdf"  # Your URDF file\nusd_output_path = "/home/user/model.usd"\n\n# Import URDF\n_urdf.import_urdf(\n    urdf_path=urdf_path,\n    usd_path=usd_output_path,\n    import_inertia=True,           # Preserve inertia tensors\n    fix_base=False,                 # Allow base to move\n    force_usd_geometry=False,       # Use original mesh format\n)\n\nprint(f"\u2705 Converted URDF \u2192 USD")\nprint(f"   Output: {usd_output_path}")\n\nsimulation_app.close()\n'})}),"\n",(0,t.jsx)(n.p,{children:"Run it:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python3 convert_urdf_to_usd.py\n# Expected: \u2705 Converted URDF \u2192 USD\n#           Output: /home/user/model.usd\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 1d: Load USD in Isaac Sim GUI"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# In Isaac Sim:\n# 1. File \u2192 Open \u2192 Select model.usd\n# 2. Humanoid should appear in viewport\n# 3. Press Play \u2192 Humanoid falls (gravity)\n# 4. Observe physics behavior\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","URDF validated (no parsing errors)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","USD file created (verify file exists)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","USD loads in Isaac Sim without errors"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Humanoid visible in viewport"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Physics simulation runs (no freezes)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Gravity causes humanoid to fall"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"lab-2-configure-sensors--verify-output-15-hours",children:"Lab 2: Configure Sensors & Verify Output (1.5 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Add RGB-D and LiDAR sensors to humanoid, verify data in ROS 2."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prerequisites"}),": Lab 1 complete, humanoid loaded in Isaac Sim"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 2a: Add Camera to Humanoid Head"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: add_sensors.py\n"""\nAdd RGB-D camera and LiDAR to humanoid robot.\n"""\n\nfrom omni.isaac.kit import SimulationApp\nfrom omni.isaac.core import World\nfrom omni.isaac.sensor import Camera, RayCaster\n\nsimulation_app = SimulationApp({"headless": False})\n\nworld = World()\n\n# Load humanoid\nhumanoid_path = "/World/Humanoid"\n# (Assume humanoid already loaded via GUI)\n\n# Add RGB-D camera to humanoid\'s head\ncamera = Camera(\n    prim_path=f"{humanoid_path}/Camera",\n    position=[0.0, 0.0, 0.15],     # 15cm above head\n    orientation=[0.0, 0.0, 0.0, 1.0],\n    width=640,\n    height=480,\n    frequency=30,                   # 30 Hz\n)\n\nprint(f"\u2705 Camera added at {camera.prim_path}")\n\n# Add LiDAR\nlidar = RayCaster(\n    prim_path=f"{humanoid_path}/LiDAR",\n    position=[0.0, 0.0, 0.1],\n    num_rays=64,\n    frequency=10,                   # 10 Hz\n)\n\nprint(f"\u2705 LiDAR added at {lidar.prim_path}")\n\n# Simulate for 100 steps\nfor step in range(100):\n    world.step(render=True)\n    if step % 10 == 0:\n        camera_data = camera.get_current_frame()\n        lidar_data = lidar.get_current_frame()\n        print(f"Step {step}: Camera shape {camera_data[\'rgb\'].shape}, LiDAR points {len(lidar_data)}")\n\nsimulation_app.close()\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 2b: Configure Noise Models"})}),"\n",(0,t.jsx)(n.p,{children:"Add realistic sensor noise:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Sensor noise configuration\ncamera_noise = {\n    "rgb_noise_std": 0.01,       # 1% intensity noise\n    "gaussian_blur": 0.5,         # Slight blur\n}\n\nlidar_noise = {\n    "range_noise_std": 0.02,     # 2cm range noise\n    "intensity_noise": 0.05,      # Intensity variation\n}\n\nimu_noise = {\n    "accel_noise_std": 0.1,      # m/s\xb2 noise\n    "gyro_noise_std": 0.05,      # rad/s noise\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Camera added to humanoid"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","LiDAR added to humanoid"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sensor data accessible in code"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Noise models configured"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Simulation runs without errors"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"lab-3-export-synthetic-dataset-2-hours",children:"Lab 3: Export Synthetic Dataset (2 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Generate 1000+ labeled images for machine learning."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prerequisites"}),": Lab 2 complete, sensors configured"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 3a: Configure Rendering Parameters"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: export_dataset.py\n"""\nExport synthetic dataset from Isaac Sim.\n"""\n\nfrom omni.isaac.kit import SimulationApp\nfrom omni.isaac.core import World\nimport numpy as np\nimport os\nfrom PIL import Image\n\nsimulation_app = SimulationApp({"headless": False})\n\nworld = World()\n\n# Configuration\ndataset_dir = "/tmp/synthetic_dataset"\nos.makedirs(dataset_dir, exist_ok=True)\n\noutput_config = {\n    "num_frames": 1000,           # Total images to generate\n    "rgb_output": f"{dataset_dir}/rgb",\n    "depth_output": f"{dataset_dir}/depth",\n    "mask_output": f"{dataset_dir}/segmentation",\n}\n\nfor dir_path in [output_config["rgb_output"],\n                  output_config["depth_output"],\n                  output_config["mask_output"]]:\n    os.makedirs(dir_path, exist_ok=True)\n\nprint(f"\u2705 Dataset directory: {dataset_dir}")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 3b: Generate Annotated Frames"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Generate 100 frames (reduce for testing, scale to 1000+ for production)\nframe_count = 0\n\nfor step in range(100):\n    world.step(render=True)\n\n    if step % 5 == 0:  # Capture every 5 frames (5x faster than 30 Hz)\n        # Get camera data\n        rgb_image = camera.get_rgb()      # Shape: (480, 640, 3)\n        depth_map = camera.get_depth()    # Shape: (480, 640)\n        segmentation = camera.get_segmentation()  # Instance IDs\n\n        # Save RGB\n        rgb_path = f"{output_config[\'rgb_output\']}/{frame_count:06d}.png"\n        Image.fromarray((rgb_image * 255).astype(np.uint8)).save(rgb_path)\n\n        # Save depth (normalized for visualization)\n        depth_path = f"{output_config[\'depth_output\']}/{frame_count:06d}.png"\n        depth_normalized = ((depth_map - depth_map.min()) /\n                           (depth_map.max() - depth_map.min()) * 255).astype(np.uint8)\n        Image.fromarray(depth_normalized).save(depth_path)\n\n        # Save segmentation\n        mask_path = f"{output_config[\'mask_output\']}/{frame_count:06d}.png"\n        Image.fromarray(segmentation.astype(np.uint8)).save(mask_path)\n\n        frame_count += 1\n\n        if frame_count % 20 == 0:\n            print(f"\u2705 Exported {frame_count} frames")\n\nprint(f"\u2705 Dataset complete: {frame_count} frames in {output_config[\'rgb_output\']}")\n\nsimulation_app.close()\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step 3c: Verify Dataset Quality"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check exported files\nls -lh /tmp/synthetic_dataset/rgb/*.png | head -5\n# Expected: 1000+ PNG files\n\n# Check file sizes\ndu -sh /tmp/synthetic_dataset/\n# Expected: ~500-800 MB for 1000 frames\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Success Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","1000+ RGB images exported"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Depth maps generated for each image"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Segmentation masks saved"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Dataset file sizes reasonable (~500MB for 1000 frames)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Random sample inspection shows realistic data"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-6-code-examples",children:"Part 6: Code Examples"}),"\n",(0,t.jsx)(n.h3,{id:"example-1-physx-parameter-tuning-script",children:"Example 1: PhysX Parameter Tuning Script"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: tune_physics.py\n"""\nIteratively tune PhysX parameters to match real robot behavior.\n"""\n\nimport subprocess\nfrom dataclasses import dataclass\n\n@dataclass\nclass PhysicsParams:\n    friction: float = 0.5\n    damping: float = 0.1\n    contact_offset: float = 0.02\n    solver_iterations: int = 4\n\ndef simulate_and_measure(params: PhysicsParams):\n    """Run simulation with given params, measure behavior."""\n    # This is pseudocode - actual measurement requires running Isaac Sim\n    # and analyzing robot trajectory\n\n    print(f"Testing: friction={params.friction}, damping={params.damping}")\n\n    # Measure metrics\n    metrics = {\n        "gait_stability": 0.8 if params.friction > 0.6 else 0.5,\n        "joint_oscillation": params.damping * 10,\n        "simulation_speed": 60 / params.solver_iterations,  # FPS\n    }\n\n    return metrics\n\ndef tune_physics(target_metrics):\n    """Grid search for best parameters."""\n    best_params = None\n    best_error = float(\'inf\')\n\n    for friction in [0.5, 0.6, 0.7, 0.8]:\n        for damping in [0.1, 0.5, 0.9]:\n            for contact_offset in [0.01, 0.02, 0.05]:\n                params = PhysicsParams(\n                    friction=friction,\n                    damping=damping,\n                    contact_offset=contact_offset,\n                )\n\n                metrics = simulate_and_measure(params)\n                error = sum((metrics[k] - target_metrics.get(k, 0))**2\n                           for k in metrics)\n\n                if error < best_error:\n                    best_error = error\n                    best_params = params\n                    print(f"  \u2705 New best: {best_params}")\n\n    return best_params\n\nif __name__ == "__main__":\n    target = {\n        "gait_stability": 0.95,\n        "joint_oscillation": 5.0,\n        "simulation_speed": 30.0,\n    }\n\n    best = tune_physics(target)\n    print(f"\\n\u2705 Best parameters found:")\n    print(f"   {best}")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-2-sensor-data-recorder",children:"Example 2: Sensor Data Recorder"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: record_sensor_data.py\n"""\nRecord sensor data from Isaac Sim to ROS 2 bag files.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom std_msgs.msg import Header\nimport numpy as np\n\nclass SensorRecorder(Node):\n    """Record Isaac Sim sensor data to ROS 2 topics."""\n\n    def __init__(self):\n        super().__init__(\'sensor_recorder\')\n\n        # Create publishers for each sensor\n        self.rgb_pub = self.create_publisher(Image, \'/camera/rgb\', 10)\n        self.depth_pub = self.create_publisher(Image, \'/camera/depth\', 10)\n        self.lidar_pub = self.create_publisher(PointCloud2, \'/lidar/points\', 10)\n\n        # Timer to publish data\n        self.timer = self.create_timer(0.033, self.publish_sensor_data)  # 30 Hz\n        self.frame_id = 0\n\n    def publish_sensor_data(self):\n        """Publish sensor data (stub - actual data from Isaac Sim)."""\n        # This would be connected to Isaac Sim camera/LiDAR output\n\n        header = Header(stamp=self.get_clock().now().to_msg(),\n                       frame_id="camera_link")\n\n        # Publish RGB image\n        rgb_msg = Image()\n        rgb_msg.header = header\n        rgb_msg.height = 480\n        rgb_msg.width = 640\n        rgb_msg.encoding = "rgb8"\n        # rgb_msg.data = get_rgb_from_isaac_sim()\n\n        self.rgb_pub.publish(rgb_msg)\n\n        if self.frame_id % 10 == 0:\n            self.get_logger().info(f"Published frame {self.frame_id}")\n\n        self.frame_id += 1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    recorder = SensorRecorder()\n    rclpy.spin(recorder)\n    recorder.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"example-3-dataset-coco-export",children:"Example 3: Dataset COCO Export"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# File: export_coco_dataset.py\n"""\nExport Isaac Sim synthetic data in COCO format for ML training.\n"""\n\nimport json\nfrom pathlib import Path\nfrom dataclasses import asdict, dataclass\nfrom typing import List\n\n@dataclass\nclass CocoImage:\n    id: int\n    file_name: str\n    height: int\n    width: int\n\n@dataclass\nclass CocoAnnotation:\n    id: int\n    image_id: int\n    category_id: int\n    bbox: List[float]  # [x, y, width, height]\n    area: float\n    iscrowd: int\n    segmentation: List[List[float]]\n\n@dataclass\nclass CocoCategory:\n    id: int\n    name: str\n\nclass CocoDataset:\n    """COCO format dataset exporter."""\n\n    def __init__(self, output_dir):\n        self.output_dir = Path(output_dir)\n        self.images = []\n        self.annotations = []\n        self.categories = [\n            CocoCategory(1, "humanoid"),\n            CocoCategory(2, "obstacle"),\n            CocoCategory(3, "ground"),\n        ]\n        self.image_id = 0\n        self.annotation_id = 0\n\n    def add_frame(self, filename, objects):\n        """Add image and its annotations."""\n        image = CocoImage(\n            id=self.image_id,\n            file_name=filename,\n            height=480,\n            width=640,\n        )\n        self.images.append(image)\n\n        for obj in objects:\n            annotation = CocoAnnotation(\n                id=self.annotation_id,\n                image_id=self.image_id,\n                category_id=obj["category_id"],\n                bbox=obj["bbox"],\n                area=obj["area"],\n                iscrowd=0,\n                segmentation=[],\n            )\n            self.annotations.append(annotation)\n            self.annotation_id += 1\n\n        self.image_id += 1\n\n    def export(self):\n        """Write COCO JSON annotation file."""\n        coco_dict = {\n            "images": [asdict(img) for img in self.images],\n            "annotations": [asdict(ann) for ann in self.annotations],\n            "categories": [asdict(cat) for cat in self.categories],\n        }\n\n        output_file = self.output_dir / "annotations.json"\n        with open(output_file, \'w\') as f:\n            json.dump(coco_dict, f, indent=2)\n\n        print(f"\u2705 Exported COCO dataset: {output_file}")\n        print(f"   Images: {len(self.images)}")\n        print(f"   Annotations: {len(self.annotations)}")\n\nif __name__ == "__main__":\n    dataset = CocoDataset("/tmp/coco_dataset")\n\n    # Add sample frames\n    for i in range(100):\n        objects = [\n            {"category_id": 1, "bbox": [100, 50, 200, 300], "area": 60000},\n            {"category_id": 2, "bbox": [400, 200, 150, 100], "area": 15000},\n        ]\n        dataset.add_frame(f"img_{i:06d}.png", objects)\n\n    dataset.export()\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-7-architecture-diagrams",children:"Part 7: Architecture Diagrams"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-sim-to-ros-2-data-flow",children:"Isaac Sim to ROS 2 Data Flow"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph LR\n    A["Isaac Sim<br/>Physics Engine"] --\x3e|per frame| B["Sensor Simulation"]\n    B --\x3e|RGB Image| C["Camera Node"]\n    B --\x3e|Depth Map| D["Depth Node"]\n    B --\x3e|Point Cloud| E["LiDAR Node"]\n    B --\x3e|Accel/Gyro| F["IMU Node"]\n    C --\x3e|/camera/image<br/>ROS 2 topic| G["Perception<br/>Pipeline"]\n    D --\x3e|/camera/depth| G\n    E --\x3e|/lidar/points| G\n    F --\x3e|/imu/data| G\n    G --\x3e|/detection<br/>results| H["Control<br/>Node"]\n    H --\x3e|/joint_commands| I["Isaac Sim<br/>Joint Controller"]\n    I --\x3e|next frame| A'}),"\n",(0,t.jsx)(n.h3,{id:"synthetic-data-generation-pipeline",children:"Synthetic Data Generation Pipeline"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TD\n    A["Isaac Sim Scene"] --\x3e|Configure| B["Camera + LiDAR<br/>+ IMU Settings"]\n    B --\x3e|Render| C["RGB Image<br/>+ Depth<br/>+ Segmentation"]\n    C --\x3e|Extract| D["Bounding Boxes<br/>+ Instance IDs<br/>+ 3D Poses"]\n    D --\x3e|Annotate| E["COCO Format<br/>JSON"]\n    C --\x3e|Export| F["PNG Images<br/>+ Depth Maps"]\n    E --\x3e G["ML Training<br/>Dataset"]\n    F --\x3e G\n    G --\x3e|PyTorch| H["Object Detection<br/>Model"]\n    H --\x3e|Eval| I["Accuracy Metrics"]'}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-8-end-of-chapter-exercises",children:"Part 8: End-of-Chapter Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-urdf-validation-and-conversion-45-min",children:"Exercise 1: URDF Validation and Conversion (45 min)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Take a URDF (your own or downloaded), validate it, convert to USD, load in Isaac Sim."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","URDF file obtained (yours or public)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","URDF validation script run, no errors"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","USD conversion completed successfully"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","USD loads in Isaac Sim without errors"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Robot visible and physics simulation runs"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Brief write-up of differences between URDF and USD"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-physics-tuning-for-realism-1-hour",children:"Exercise 2: Physics Tuning for Realism (1 hour)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Load a humanoid, measure physics behavior, tune parameters to match targets."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Targets"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Friction coefficient: 0.7-0.8 (rubber on floor)"}),"\n",(0,t.jsx)(n.li,{children:"Gait stability: No wobbling"}),"\n",(0,t.jsx)(n.li,{children:"Contact behavior: Feet don't penetrate ground"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Record baseline behavior (parameters at defaults)"}),"\n",(0,t.jsx)(n.li,{children:"Identify issues (sliding, instability, oscillation)"}),"\n",(0,t.jsx)(n.li,{children:"Adjust one parameter at a time"}),"\n",(0,t.jsx)(n.li,{children:"Re-test and measure improvement"}),"\n",(0,t.jsx)(n.li,{children:"Document final parameters"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Baseline measurements recorded"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Issues identified"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Parameters tuned iteratively"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Final parameters documented"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Physics behavior noticeably improved"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-multi-sensor-configuration-1-hour",children:"Exercise 3: Multi-Sensor Configuration (1 hour)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Add RGB-D camera, LiDAR, and IMU to humanoid. Verify all sensors output data."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","RGB-D camera added, outputs images and depth"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","LiDAR added, outputs point clouds"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","IMU added, outputs acceleration and rotation"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All three sensors configured with realistic noise"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test code verifies all sensor outputs are accessible"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Brief description of why each sensor matters"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h3,{id:"exercise-4-synthetic-dataset-quality-15-hours",children:"Exercise 4: Synthetic Dataset Quality (1.5 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Generate 500-frame synthetic dataset, evaluate quality for ML training."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Metrics to assess"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Coverage"}),": Does dataset include diverse poses? (standing, walking, falling)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Realism"}),": Do images look photorealistic? Any obvious simulation artifacts?"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Annotation accuracy"}),": Are depth maps correct? Segmentation masks precise?"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"File sizes"}),": Are they reasonable? (~300-500 MB for 500 frames)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","500 RGB images exported"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","500 depth maps exported"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","500 segmentation masks exported"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Random sample inspection (visual check for realism)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","COCO format annotations generated"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Quality assessment report (1-2 pages)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"part-9-capstone-integration",children:"Part 9: Capstone Integration"}),"\n",(0,t.jsx)(n.p,{children:"Your humanoid AI Assistant capstone depends on Chapter 2 work:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Digital Twin Fidelity"})," \u2014 Accurate USD conversion ensures sim-to-real transfer works"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Realism"}),' \u2014 Proper sensor noise models prevent the "reality gap" where sim-trained models fail on real robots']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Synthetic Data"})," \u2014 Large-scale dataset generation (10K+ frames) enables training in Chapter 4"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Physics Accuracy"})," \u2014 Realistic physics ensures learned behaviors transfer to real hardware"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Checkpoint"}),": By end of Chapter 2, your humanoid should:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Have accurate physics matching real-world dynamics"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Have RGB-D + LiDAR + IMU sensors configured"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Be exporting 1000+ synthetic frames ready for ML training"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Be publishable to ROS 2 topics (Chapter 3)"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"You now have a photorealistic digital twin with realistic sensors. In Chapter 3, you'll:"}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Deploy AI Perception"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use Isaac ROS for hardware-accelerated vision"}),"\n",(0,t.jsx)(n.li,{children:"Implement object detection at >30 FPS"}),"\n",(0,t.jsx)(n.li,{children:"Add visual SLAM for autonomous navigation"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Ready? Move to ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/ai_native-textbook/docs/module-3/chapter-3-isaac-ros",children:"Chapter 3: AI Perception with Isaac ROS"})}),"."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Chapter Summary"}),": 5-6 hours | Difficulty: Intermediate | Week 8 Day 2-5"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_core_urdf_importer.html",children:"Isaac Sim URDF Import Guide"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://nvidia-phx.github.io/physx/documentation/",children:"PhysX 5 Parameter Tuning"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://cocodataset.org/#format-data",children:"COCO Dataset Format"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/ros2/common_interfaces/tree/master/sensor_msgs",children:"ROS 2 Sensor Messages"})}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Questions?"})," Check debugging section or ask in Module 3 discussions."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var i=s(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);