"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[8325],{8335:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-2/week-7","title":"Week 7 Practice Guide: Sensors & Unity","description":"Week 7 (5 days of study) | 18-20 hours total | Sensors in Gazebo + Unity high-fidelity rendering","source":"@site/docs/module-2/week-7.md","sourceDirName":"module-2","slug":"/module-2/week-7","permalink":"/ai_native-textbook/docs/module-2/week-7","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"practice","permalink":"/ai_native-textbook/docs/tags/practice"},{"inline":true,"label":"sensors","permalink":"/ai_native-textbook/docs/tags/sensors"},{"inline":true,"label":"unity","permalink":"/ai_native-textbook/docs/tags/unity"},{"inline":true,"label":"week-7","permalink":"/ai_native-textbook/docs/tags/week-7"},{"inline":true,"label":"hands-on","permalink":"/ai_native-textbook/docs/tags/hands-on"}],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"sidebar_label":"Week 7: Sensors & Unity","title":"Week 7 Practice Guide: Sensors & Unity","tags":["practice","sensors","unity","week-7","hands-on"],"difficulty":"Advanced","module":2,"week":7,"prerequisites":["Week 6","Chapters 3-4"],"estimated_time":"18-20 hours","topics":["sensor-simulation","unity-robotics","ros2-communication","rendering","dataset-export"]},"sidebar":"tutorialSidebar","previous":{"title":"Week 6: Gazebo Fundamentals","permalink":"/ai_native-textbook/docs/module-2/week-6"},"next":{"title":"Module 3: The AI-Robot Brain","permalink":"/ai_native-textbook/docs/module-3/"}}');var r=i(4848),a=i(8453);const o={sidebar_position:6,sidebar_label:"Week 7: Sensors & Unity",title:"Week 7 Practice Guide: Sensors & Unity",tags:["practice","sensors","unity","week-7","hands-on"],difficulty:"Advanced",module:2,week:7,prerequisites:["Week 6","Chapters 3-4"],estimated_time:"18-20 hours",topics:["sensor-simulation","unity-robotics","ros2-communication","rendering","dataset-export"]},t="Week 7 Practice Guide: Sensors & Unity",l={},d=[{value:"Week 7 Objectives",id:"week-7-objectives",level:2},{value:"Daily Learning Path",id:"daily-learning-path",level:2},{value:"Monday: Introduction to Sensor Simulation (1.5 hours)",id:"monday-introduction-to-sensor-simulation-15-hours",level:2},{value:"Morning: Understand Sensor Simulation Fundamentals",id:"morning-understand-sensor-simulation-fundamentals",level:3},{value:"Afternoon: Add Basic Sensors to Humanoid",id:"afternoon-add-basic-sensors-to-humanoid",level:3},{value:"Tuesday: LiDAR and Depth Cameras (1.5 hours)",id:"tuesday-lidar-and-depth-cameras-15-hours",level:2},{value:"Morning: Add 3D LiDAR Sensor",id:"morning-add-3d-lidar-sensor",level:3},{value:"Afternoon: Add Depth Camera",id:"afternoon-add-depth-camera",level:3},{value:"Wednesday: RViz Visualization and Sensor Fusion (2 hours)",id:"wednesday-rviz-visualization-and-sensor-fusion-2-hours",level:2},{value:"Morning: Visualize Sensors in RViz",id:"morning-visualize-sensors-in-rviz",level:3},{value:"Afternoon: Analyze Sensor Data",id:"afternoon-analyze-sensor-data",level:3},{value:"Thursday: Set Up Unity and Import Humanoid (2 hours)",id:"thursday-set-up-unity-and-import-humanoid-2-hours",level:2},{value:"Morning: Install and Configure Unity",id:"morning-install-and-configure-unity",level:3},{value:"Afternoon: Import Humanoid and Test Communication",id:"afternoon-import-humanoid-and-test-communication",level:3},{value:"Friday: High-Fidelity Rendering and Dataset Export (2.5 hours)",id:"friday-high-fidelity-rendering-and-dataset-export-25-hours",level:2},{value:"Morning: Configure HDRP for Photorealism",id:"morning-configure-hdrp-for-photorealism",level:3},{value:"Afternoon: Export Training Dataset",id:"afternoon-export-training-dataset",level:3},{value:"Week 7 Exercises (5 independent exercises, 1-2 hours each)",id:"week-7-exercises-5-independent-exercises-1-2-hours-each",level:2},{value:"Exercise 1: Configure LiDAR with Varying Noise",id:"exercise-1-configure-lidar-with-varying-noise",level:3},{value:"Exercise 2: Depth Camera Accuracy Analysis",id:"exercise-2-depth-camera-accuracy-analysis",level:3},{value:"Exercise 3: IMU Calibration",id:"exercise-3-imu-calibration",level:3},{value:"Exercise 4: Unity ROS 2 Two-Way Communication",id:"exercise-4-unity-ros-2-two-way-communication",level:3},{value:"Exercise 5: Sensor Fusion Implementation",id:"exercise-5-sensor-fusion-implementation",level:3},{value:"Week 7 Challenge Projects (2-3 hours each, optional)",id:"week-7-challenge-projects-2-3-hours-each-optional",level:2},{value:"Challenge 1: Sensor-Based Obstacle Avoidance",id:"challenge-1-sensor-based-obstacle-avoidance",level:3},{value:"Challenge 2: High-Fidelity Rendering with Domain Randomization",id:"challenge-2-high-fidelity-rendering-with-domain-randomization",level:3},{value:"Debugging Tips &amp; Tricks",id:"debugging-tips--tricks",level:2},{value:"Issue: ROS 2 topics not appearing from Gazebo sensors",id:"issue-ros-2-topics-not-appearing-from-gazebo-sensors",level:3},{value:"Issue: Unity ROS 2 bridge connection fails",id:"issue-unity-ros-2-bridge-connection-fails",level:3},{value:"Issue: Point cloud visualization is slow in RViz",id:"issue-point-cloud-visualization-is-slow-in-rviz",level:3},{value:"Resources &amp; Links",id:"resources--links",level:2},{value:"Week 7 Summary Checklist",id:"week-7-summary-checklist",level:2},{value:"Module 2 Capstone: Week 8",id:"module-2-capstone-week-8",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"week-7-practice-guide-sensors--unity",children:"Week 7 Practice Guide: Sensors & Unity"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Week 7 (5 days of study) | 18-20 hours total | Sensors in Gazebo + Unity high-fidelity rendering"})}),"\n",(0,r.jsxs)(e.p,{children:["Welcome to Week 7! This week builds on your Week 6 Gazebo experience by adding ",(0,r.jsx)(e.strong,{children:"realistic perception sensors"})," and transitioning to ",(0,r.jsx)(e.strong,{children:"high-fidelity rendering in Unity"}),". By Friday, you'll have a complete Digital Twin combining physics accuracy (Gazebo) and visual realism (Unity)."]}),"\n",(0,r.jsx)(e.h2,{id:"week-7-objectives",children:"Week 7 Objectives"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this week, you will:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"\u2705 Add and configure LiDAR, depth camera, and IMU sensors to your humanoid"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Understand sensor noise models and their impact on algorithms"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Publish sensor data over ROS 2 topics in standard formats"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Visualize sensor data (point clouds, depth images, IMU) in RViz"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Set up Unity for robotics with ROS 2 integration"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Import humanoid model into Unity with materials and physics"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Create photorealistic environments suitable for training data generation"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"daily-learning-path",children:"Daily Learning Path"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Monday        Tuesday       Wednesday     Thursday       Friday\n\u2502             \u2502              \u2502             \u2502              \u2502\nSensors \u2192  LiDAR + \u2192 RViz Visualization \u2192 Unity Setup \u2192 High-Fidelity\nin Gazebo   Depth        & Sensor Fusion    & Import      Rendering\n\u2502             \u2502              \u2502             \u2502              \u2502\n1.5 hours    1.5 hours      2 hours       2 hours       2.5 hours\n"})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"monday-introduction-to-sensor-simulation-15-hours",children:"Monday: Introduction to Sensor Simulation (1.5 hours)"}),"\n",(0,r.jsx)(e.h3,{id:"morning-understand-sensor-simulation-fundamentals",children:"Morning: Understand Sensor Simulation Fundamentals"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Review why sensor simulation matters and what realism looks like."]}),"\n",(0,r.jsx)(e.p,{children:"Key concepts:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor fidelity"}),": How accurately simulation matches real sensors"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Noise models"}),": Gaussian noise that matches real hardware"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Range limits"}),": Minimum and maximum sensor ranges"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Field of view"}),": Angular coverage of the sensor"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Real-world example"}),": RealSense D435 depth camera"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Range: 0.1m to 10m"}),"\n",(0,r.jsx)(e.li,{children:"Noise: ~1-2% of distance"}),"\n",(0,r.jsx)(e.li,{children:"Resolution: 640\xd7480 pixels"}),"\n",(0,r.jsx)(e.li,{children:"FOV: 87\xb0 \xd7 58\xb0"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Our simulation should match these specs!"}),"\n",(0,r.jsx)(e.h3,{id:"afternoon-add-basic-sensors-to-humanoid",children:"Afternoon: Add Basic Sensors to Humanoid"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Create a new world with humanoid + camera + IMU sensors."]}),"\n",(0,r.jsxs)(e.p,{children:["Create file ",(0,r.jsx)(e.code,{children:"humanoid_with_sensors.sdf"})," by copying your Week 6 world and adding sensors to the humanoid link."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Add camera sensor"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Inside humanoid link in head area --\x3e\n<sensor name="rgb_camera" type="camera">\n  <pose>0 0 0.15 0 0 0</pose>\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.01</near>\n      <far>100.0</far>\n    </clip>\n  </camera>\n  <always_on>1</always_on>\n  <update_rate>30</update_rate>\n  <visualize>false</visualize>\n  <plugin name="camera_plugin" filename="libgazebo_ros_camera.so">\n    <ros>\n      <namespace>humanoid_robot</namespace>\n      <remapping>image_raw:=camera/image_raw</remapping>\n    </ros>\n    <camera_name>front_camera</camera_name>\n    <frame_name>camera_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Add IMU sensor"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Inside humanoid link in torso area --\x3e\n<sensor name="imu_sensor" type="imu">\n  <always_on>1</always_on>\n  <update_rate>100</update_rate>\n  <imu>\n    <angular_velocity>\n      <x><noise type="gaussian"><mean>0.0</mean><stddev>0.001</stddev></noise></x>\n      <y><noise type="gaussian"><mean>0.0</mean><stddev>0.001</stddev></noise></y>\n      <z><noise type="gaussian"><mean>0.0</mean><stddev>0.001</stddev></noise></z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x><noise type="gaussian"><mean>0.0</mean><stddev>0.01</stddev></noise></x>\n      <y><noise type="gaussian"><mean>0.0</mean><stddev>0.01</stddev></noise></y>\n      <z><noise type="gaussian"><mean>0.0</mean><stddev>0.01</stddev></noise></z>\n    </linear_acceleration>\n  </imu>\n  <plugin name="imu_plugin" filename="libgazebo_ros_imu_sensor.so">\n    <ros>\n      <namespace>humanoid_robot</namespace>\n      <remapping>imu:=imu/data</remapping>\n    </ros>\n  </plugin>\n</sensor>\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Launch and verify"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"gazebo humanoid_with_sensors.sdf\n\n# In another terminal:\nros2 topic list | grep humanoid_robot\n# Should see: /humanoid_robot/camera/image_raw, /humanoid_robot/imu/data\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Checkpoint"}),": Both sensors publish data without errors"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"tuesday-lidar-and-depth-cameras-15-hours",children:"Tuesday: LiDAR and Depth Cameras (1.5 hours)"}),"\n",(0,r.jsx)(e.h3,{id:"morning-add-3d-lidar-sensor",children:"Morning: Add 3D LiDAR Sensor"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Configure a 3D LiDAR (point cloud) sensor on the humanoid head."]}),"\n",(0,r.jsxs)(e.p,{children:["Edit ",(0,r.jsx)(e.code,{children:"humanoid_with_sensors.sdf"}),", add to humanoid head link:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar_3d" type="ray">\n  <pose>0 0 0.3 0 0 0</pose>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>400</samples>\n        <min_angle>0</min_angle>\n        <max_angle>6.283</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>16</samples>\n        <min_angle>-0.2618</min_angle>\n        <max_angle>0.2618</max_angle>\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>100.0</max>\n      <resolution>0.01</resolution>\n    </range>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.1</stddev>  \x3c!-- 10cm noise --\x3e\n    </noise>\n  </ray>\n  <always_on>1</always_on>\n  <update_rate>10</update_rate>\n  <visualize>true</visualize>\n  <plugin name="lidar_plugin" filename="libgazebo_ros_ray_sensor.so">\n    <ros>\n      <remapping>~/out:=points</remapping>\n    </ros>\n    <output_type>sensor_msgs/PointCloud2</output_type>\n    <frame_name>lidar_link</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,r.jsx)(e.h3,{id:"afternoon-add-depth-camera",children:"Afternoon: Add Depth Camera"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Add a RealSense D435-style depth camera."]}),"\n",(0,r.jsxs)(e.p,{children:["Edit ",(0,r.jsx)(e.code,{children:"humanoid_with_sensors.sdf"}),", add to humanoid head link:"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth_camera">\n  <pose>0.05 0 0.15 0 0 0</pose>\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10.0</far>\n    </clip>\n  </camera>\n  <always_on>1</always_on>\n  <update_rate>30</update_rate>\n  <visualize>false</visualize>\n  <plugin name="depth_plugin" filename="libgazebo_ros_camera.so">\n    <ros>\n      <namespace>humanoid_robot</namespace>\n      <remapping>image_raw:=depth/image_raw</remapping>\n    </ros>\n    <camera_name>depth_camera</camera_name>\n    <frame_name>depth_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Launch and verify"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:'gazebo humanoid_with_sensors.sdf\n\n# Check topics\nros2 topic list | grep -E "depth|lidar|points"\n\n# Visualize point cloud (next section)\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Checkpoint"}),": LiDAR and depth camera topics appear in ",(0,r.jsx)(e.code,{children:"ros2 topic list"})]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"wednesday-rviz-visualization-and-sensor-fusion-2-hours",children:"Wednesday: RViz Visualization and Sensor Fusion (2 hours)"}),"\n",(0,r.jsx)(e.h3,{id:"morning-visualize-sensors-in-rviz",children:"Morning: Visualize Sensors in RViz"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Set up RViz to display all sensor outputs."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Launch RViz"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:'rviz2\n\n# In RViz:\n# 1. File \u2192 Open Config \u2192 select default config\n# 2. Add displays:\n#    a) PointCloud2: subscribe to /points (from LiDAR)\n#    b) Image: subscribe to /camera/image_raw (from RGB camera)\n#    c) Image: subscribe to /depth/image_raw (from depth camera)\n#    d) IMU: subscribe to /imu/data\n# 3. Set Fixed Frame to "base_link"\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Observe"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"LiDAR point cloud should appear as dots around humanoid"}),"\n",(0,r.jsx)(e.li,{children:"RGB camera image should show rendered view from humanoid head"}),"\n",(0,r.jsx)(e.li,{children:"Depth image should show grayscale (brighter = closer)"}),"\n",(0,r.jsx)(e.li,{children:"IMU should show acceleration vectors"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"afternoon-analyze-sensor-data",children:"Afternoon: Analyze Sensor Data"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Write Python script to analyze sensor output and understand noise."]}),"\n",(0,r.jsxs)(e.p,{children:["Create ",(0,r.jsx)(e.code,{children:"analyze_sensors.py"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2, Image\nimport numpy as np\nimport sensor_msgs_py.point_cloud2 as pc2\n\nclass SensorAnalyzer(Node):\n    def __init__(self):\n        super().__init__('sensor_analyzer')\n        self.lidar_sub = self.create_subscription(PointCloud2, '/points', self.analyze_lidar, 1)\n        self.stats = {'count': 0, 'ranges': []}\n\n    def analyze_lidar(self, msg):\n        points = np.array(list(pc2.read_points(msg, field_names=['x', 'y', 'z'], skip_nans=True)))\n        if len(points) > 0:\n            ranges = np.linalg.norm(points, axis=1)\n            self.get_logger().info(f'Points: {len(points)}, Range: '\n                                  f'min={np.min(ranges):.2f}m, '\n                                  f'max={np.max(ranges):.2f}m, '\n                                  f'mean={np.mean(ranges):.2f}m')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorAnalyzer()\n    rclpy.spin(node)\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Run"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"python3 analyze_sensors.py\n# See statistics on LiDAR data\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Checkpoint"}),": You understand sensor data format and can analyze it"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"thursday-set-up-unity-and-import-humanoid-2-hours",children:"Thursday: Set Up Unity and Import Humanoid (2 hours)"}),"\n",(0,r.jsx)(e.h3,{id:"morning-install-and-configure-unity",children:"Morning: Install and Configure Unity"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Install Unity 2022 LTS and set up robotics packages."]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Download ",(0,r.jsx)(e.a,{href:"https://unity.com/download",children:"Unity Hub"})]}),"\n",(0,r.jsx)(e.li,{children:"Install Unity 2022 LTS version"}),"\n",(0,r.jsx)(e.li,{children:"Create new 3D project"}),"\n",(0,r.jsxs)(e.li,{children:["Install ROS2ForUnity:","\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"cd <unity_project>/Assets\ngit clone https://github.com/ROS2ForUnity/ROS2-Unity.git\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["In Unity Editor, set up project:","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Window \u2192 TextMesh Pro \u2192 Import TMP Essentials"}),"\n",(0,r.jsx)(e.li,{children:"Edit \u2192 Project Settings \u2192 Player \u2192 .NET Framework 4.7.1"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"afternoon-import-humanoid-and-test-communication",children:"Afternoon: Import Humanoid and Test Communication"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Import humanoid URDF into Unity and verify ROS 2 communication."]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Add ROS2 Node component"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'Create empty GameObject "ROS2Manager"'}),"\n",(0,r.jsx)(e.li,{children:'Add component "ROS2Node"'}),"\n",(0,r.jsx)(e.li,{children:"Configure ROS Domain ID"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Create joint state publisher script"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'// Assets/Scripts/JointStatePublisher.cs\nusing UnityEngine;\nusing ROS2;\nusing sensor_msgs.msg;\n\npublic class JointStatePublisher : MonoBehaviour\n{\n    private ROS2Node ros2Node;\n    private IPublisher<JointState> pub;\n    private Animator anim;\n\n    void Start()\n    {\n        ros2Node = GetComponent<ROS2Node>();\n        pub = ros2Node.CreatePublisher<JointState>("joint_states");\n        anim = GetComponent<Animator>();\n    }\n\n    void Update()\n    {\n        var msg = new JointState();\n        msg.Header.Stamp = ROS2.Clock.Now;\n        msg.Name = new string[] { "left_hip_x", "right_hip_x" };\n        msg.Position = new double[] { anim.GetFloat("LeftHip"), anim.GetFloat("RightHip") };\n        pub.Publish(msg);\n    }\n}\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Test communication"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Terminal 1: Start Unity simulation\n# In Unity: Play button\n\n# Terminal 2: Monitor ROS 2\nros2 topic echo /joint_states\n# Should see joint state messages from Unity\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Checkpoint"}),": Unity publishes joint states that appear in ROS 2"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"friday-high-fidelity-rendering-and-dataset-export-25-hours",children:"Friday: High-Fidelity Rendering and Dataset Export (2.5 hours)"}),"\n",(0,r.jsx)(e.h3,{id:"morning-configure-hdrp-for-photorealism",children:"Morning: Configure HDRP for Photorealism"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Switch to High-Definition Render Pipeline and configure realistic materials."]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Install HDRP"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Window \u2192 Asset Store"}),"\n",(0,r.jsx)(e.li,{children:'Search "High Definition RP"'}),"\n",(0,r.jsx)(e.li,{children:"Import latest version"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Create HDRP scene"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Assets \u2192 Rendering \u2192 High Definition Render Pipeline Asset"}),"\n",(0,r.jsx)(e.li,{children:"Set as default in Project Settings"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Configure materials"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'// Assets/Scripts/MaterialSetup.cs\nusing UnityEngine;\nusing UnityEngine.Rendering.HighDefinition;\n\npublic class MaterialSetup : MonoBehaviour\n{\n    void Start()\n    {\n        Renderer rend = GetComponent<Renderer>();\n        Material mat = rend.material;\n\n        // PBR material properties\n        mat.SetColor("_BaseColor", new Color(0.5f, 0.5f, 0.5f));\n        mat.SetFloat("_Metallic", 0.3f);   // 0=non-metal, 1=metal\n        mat.SetFloat("_Smoothness", 0.7f); // 0=rough, 1=glossy\n    }\n}\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Set up lighting"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Create Directional Light (sun)"}),"\n",(0,r.jsx)(e.li,{children:"Adjust intensity and rotation for dramatic shadows"}),"\n",(0,r.jsx)(e.li,{children:"Add environment skybox for reflections"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"afternoon-export-training-dataset",children:"Afternoon: Export Training Dataset"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Capture 100 frames with annotations for training data."]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'// Assets/Scripts/DatasetExporter.cs\nusing UnityEngine;\nusing System.IO;\nusing System.Text;\n\npublic class DatasetExporter : MonoBehaviour\n{\n    public Camera captureCamera;\n    public int frameCount = 100;\n    public string outputPath = "Assets/Dataset/";\n\n    void Start()\n    {\n        if (!Directory.Exists(outputPath))\n            Directory.CreateDirectory(outputPath);\n        StartCoroutine(ExportFrames());\n    }\n\n    System.Collections.IEnumerator ExportFrames()\n    {\n        for (int i = 0; i < frameCount; i++)\n        {\n            // Capture screenshot\n            Texture2D tex = ScreenCapture.CaptureScreenshotAsTexture();\n            byte[] imageBytes = tex.EncodeToPNG();\n            string imagePath = $"{outputPath}/frame_{i:04d}.png";\n            File.WriteAllBytes(imagePath, imageBytes);\n\n            // Generate annotation\n            StringBuilder sb = new StringBuilder();\n            sb.AppendLine("{");\n            sb.AppendLine($"  \\"frame_id\\": {i},");\n            sb.AppendLine($"  \\"timestamp\\": {Time.realtimeSinceStartup},");\n            sb.AppendLine($"  \\"humanoid_position\\": [{gameObject.transform.position.x}, "\n                        + $"{gameObject.transform.position.y}, "\n                        + $"{gameObject.transform.position.z}]");\n            sb.AppendLine("}");\n\n            string annoPath = $"{outputPath}/frame_{i:04d}_annotation.json";\n            File.WriteAllText(annoPath, sb.ToString());\n\n            yield return new WaitForEndOfFrame();\n        }\n\n        Debug.Log($"Exported {frameCount} frames to {outputPath}");\n    }\n}\n'})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Verify export"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"ls -la Assets/Dataset/ | wc -l\n# Should show 200 files (100 PNG + 100 JSON)\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Checkpoint"}),": Training dataset successfully exported"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"week-7-exercises-5-independent-exercises-1-2-hours-each",children:"Week 7 Exercises (5 independent exercises, 1-2 hours each)"}),"\n",(0,r.jsx)(e.h3,{id:"exercise-1-configure-lidar-with-varying-noise",children:"Exercise 1: Configure LiDAR with Varying Noise"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Create 3 worlds with different LiDAR noise levels (0.05m, 0.1m, 0.2m). Measure detection accuracy at 10m range."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"All 3 worlds launch successfully"}),"\n",(0,r.jsx)(e.li,{children:"You measure/document noise impact on obstacle detection accuracy"}),"\n",(0,r.jsx)(e.li,{children:"Document findings in a report"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h3,{id:"exercise-2-depth-camera-accuracy-analysis",children:"Exercise 2: Depth Camera Accuracy Analysis"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Place objects at known distances (1m, 3m, 5m) and measure depth camera accuracy."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Capture depth images at 3 distances"}),"\n",(0,r.jsx)(e.li,{children:"Analyze depth values (should match known distances \xb15%)"}),"\n",(0,r.jsx)(e.li,{children:"Explain any measurement errors"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h3,{id:"exercise-3-imu-calibration",children:"Exercise 3: IMU Calibration"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Record 30 seconds of stationary IMU data and calculate bias and noise covariance."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Record IMU data to file"}),"\n",(0,r.jsx)(e.li,{children:"Calculate: mean, std dev, bias for gyro and accelerometer"}),"\n",(0,r.jsx)(e.li,{children:"Report values match real IMU specifications"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h3,{id:"exercise-4-unity-ros-2-two-way-communication",children:"Exercise 4: Unity ROS 2 Two-Way Communication"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Implement command feedback loop:"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Send joint commands from ROS 2 node"}),"\n",(0,r.jsx)(e.li,{children:"Unity receives and applies commands"}),"\n",(0,r.jsx)(e.li,{children:"Unity publishes back updated joint states"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Commands sent from ROS 2 cause humanoid movement in Unity"}),"\n",(0,r.jsx)(e.li,{children:"Updated states published back to ROS 2"}),"\n",(0,r.jsx)(e.li,{children:"Zero message delays or errors"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h3,{id:"exercise-5-sensor-fusion-implementation",children:"Exercise 5: Sensor Fusion Implementation"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Combine LiDAR + depth camera data to create unified 3D obstacle map."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Read both sensor topics in ROS 2 node"}),"\n",(0,r.jsx)(e.li,{children:"Merge point clouds into single coordinate frame"}),"\n",(0,r.jsx)(e.li,{children:"Publish combined obstacle map"}),"\n",(0,r.jsx)(e.li,{children:"Visualize in RViz (should see merged data)"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"week-7-challenge-projects-2-3-hours-each-optional",children:"Week 7 Challenge Projects (2-3 hours each, optional)"}),"\n",(0,r.jsx)(e.h3,{id:"challenge-1-sensor-based-obstacle-avoidance",children:"Challenge 1: Sensor-Based Obstacle Avoidance"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Objective"}),": Navigate humanoid through obstacle course using LiDAR and depth camera."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Implementation"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Create world with 5+ obstacles"}),"\n",(0,r.jsx)(e.li,{children:"Process LiDAR/depth data to detect obstacles"}),"\n",(0,r.jsx)(e.li,{children:"Simple algorithm: move forward if no obstacle, rotate if blocked"}),"\n",(0,r.jsx)(e.li,{children:"Success: complete course without crashes"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h3,{id:"challenge-2-high-fidelity-rendering-with-domain-randomization",children:"Challenge 2: High-Fidelity Rendering with Domain Randomization"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Objective"}),": Export 500-frame dataset with randomized lighting and materials."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Implementation"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Create Unity scene with humanoid and objects"}),"\n",(0,r.jsx)(e.li,{children:"Randomize lighting (sun intensity, color, direction)"}),"\n",(0,r.jsx)(e.li,{children:"Randomize materials (colors, roughness, metallic)"}),"\n",(0,r.jsx)(e.li,{children:"Export dataset with automated domain randomization"}),"\n",(0,r.jsx)(e.li,{children:"Validate annotations are consistent across variations"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"debugging-tips--tricks",children:"Debugging Tips & Tricks"}),"\n",(0,r.jsx)(e.h3,{id:"issue-ros-2-topics-not-appearing-from-gazebo-sensors",children:"Issue: ROS 2 topics not appearing from Gazebo sensors"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Solution"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Check gazebo_ros plugins are installed: ",(0,r.jsx)(e.code,{children:"apt list --installed | grep gazebo-ros"})]}),"\n",(0,r.jsxs)(e.li,{children:["Verify sensor ",(0,r.jsx)(e.code,{children:"<plugin>"})," tags are correct in SDF"]}),"\n",(0,r.jsx)(e.li,{children:"Check ROS_DOMAIN_ID environment variable matches"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"issue-unity-ros-2-bridge-connection-fails",children:"Issue: Unity ROS 2 bridge connection fails"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Solution"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Verify ROS 2 domain IDs match (Gazebo and Unity)"}),"\n",(0,r.jsxs)(e.li,{children:["Check ",(0,r.jsx)(e.code,{children:"source /opt/ros/humble/setup.bash"})," before launching"]}),"\n",(0,r.jsx)(e.li,{children:"Ensure network allows localhost communication"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"issue-point-cloud-visualization-is-slow-in-rviz",children:"Issue: Point cloud visualization is slow in RViz"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Solution"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Reduce LiDAR sample density (fewer points per scan)"}),"\n",(0,r.jsx)(e.li,{children:"Reduce update rate (from 10 Hz to 5 Hz)"}),"\n",(0,r.jsx)(e.li,{children:"Use rviz2 instead of rviz (better performance)"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"resources--links",children:"Resources & Links"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Gazebo Sensor Plugins"}),": ",(0,r.jsx)(e.a,{href:"http://gazebosim.org/tutorials?tut=sensor_configuration",children:"http://gazebosim.org/tutorials?tut=sensor_configuration"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"ROS 2 Sensor Messages"}),": ",(0,r.jsx)(e.a,{href:"http://docs.ros.org/en/humble/Concepts/About-ROS-2-Interfaces.html",children:"http://docs.ros.org/en/humble/Concepts/About-ROS-2-Interfaces.html"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Unity Robotics Documentation"}),": ",(0,r.jsx)(e.a,{href:"https://github.com/Unity-Technologies/Unity-Robotics-Hub",children:"https://github.com/Unity-Technologies/Unity-Robotics-Hub"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"RViz User Guide"}),": ",(0,r.jsx)(e.a,{href:"http://wiki.ros.org/rviz/UserGuide",children:"http://wiki.ros.org/rviz/UserGuide"})]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"week-7-summary-checklist",children:"Week 7 Summary Checklist"}),"\n",(0,r.jsx)(e.p,{children:"By Friday, you should have:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u2705 Added LiDAR, depth camera, and IMU sensors to humanoid"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Visualized all sensor data in RViz"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Understood sensor noise models and their effects"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Set up Unity for robotics with ROS 2 integration"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Imported humanoid into Unity with realistic materials"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Configured HDRP photorealistic rendering"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Exported training dataset with annotations"}),"\n",(0,r.jsx)(e.li,{children:"\u2705 Completed 5 exercises and at least 1 challenge project"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"If you've checked all boxes, you've completed Module 2! \ud83c\udf89"})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"module-2-capstone-week-8",children:"Module 2 Capstone: Week 8"}),"\n",(0,r.jsxs)(e.p,{children:["You now have the skills for the ",(0,r.jsx)(e.strong,{children:"Week 8 Capstone Project"}),", which combines everything:"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Task"}),": Create a complete Digital Twin simulation where your humanoid:"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Spawns in a Gazebo physics-accurate environment"}),"\n",(0,r.jsx)(e.li,{children:"Uses sensors (LiDAR, depth, IMU) for perception"}),"\n",(0,r.jsx)(e.li,{children:"Executes a task (walk, navigate, grasp)"}),"\n",(0,r.jsx)(e.li,{children:"Gets visualized in high-fidelity in Unity"}),"\n",(0,r.jsx)(e.li,{children:"Generates synthetic training data"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Estimated time"}),": 6-8 hours"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Success"}),": Humanoid completes task successfully in both Gazebo and Unity, with annotated dataset exported."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Congratulations on completing Module 2!"})," \ud83d\ude80"]}),"\n",(0,r.jsxs)(e.p,{children:["You're now ready for ",(0,r.jsx)(e.strong,{children:"Module 3: NVIDIA Isaac Platform"}),", where you'll use advanced simulation features, generate massive datasets, and prepare for sim-to-real transfer."]}),"\n",(0,r.jsx)(e.p,{children:"See you in Module 3!"})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>t});var s=i(6540);const r={},a=s.createContext(r);function o(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);