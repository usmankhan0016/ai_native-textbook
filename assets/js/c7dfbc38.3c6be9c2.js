"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[3267],{454:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/index","title":"Module 4: Vision\u2013Language\u2013Action (VLA) Robotics","description":"Capstone Module | Duration Modules 1-3","source":"@site/docs/module-4/index.md","sourceDirName":"module-4","slug":"/module-4/","permalink":"/ai_native-textbook/docs/module-4/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 9-10: AI & Capstone","permalink":"/ai_native-textbook/docs/module-3/week-9-10"},"next":{"title":"Chapter 1: Introduction to Vision\u2013Language\u2013Action Robotics","permalink":"/ai_native-textbook/docs/module-4/chapter-1-vla-intro"}}');var r=i(4848),l=i(8453);const t={},o="Module 4: Vision\u2013Language\u2013Action (VLA) Robotics",a={},c=[{value:"Module Overview",id:"module-overview",level:2},{value:"What You&#39;ll Build",id:"what-youll-build",level:3},{value:"Learning Outcomes",id:"learning-outcomes",level:3},{value:"Module Chapters",id:"module-chapters",level:2},{value:"<strong>Chapter 1: Introduction to Vision-Language-Action Robotics</strong>",id:"chapter-1-introduction-to-vision-language-action-robotics",level:3},{value:"<strong>Chapter 2: Vision for VLA \u2013 Building Perception Pipelines</strong>",id:"chapter-2-vision-for-vla--building-perception-pipelines",level:3},{value:"<strong>Chapter 3: Language Planning with Whisper &amp; Large Language Models</strong>",id:"chapter-3-language-planning-with-whisper--large-language-models",level:3},{value:"<strong>Chapter 4: VLA Control Architecture &amp; Deployment</strong>",id:"chapter-4-vla-control-architecture--deployment",level:3},{value:"<strong>Week 13: VLA Capstone Sprint &amp; Integration</strong>",id:"week-13-vla-capstone-sprint--integration",level:3},{value:"Module Learning Path",id:"module-learning-path",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Required Knowledge (from Modules 1-3)",id:"required-knowledge-from-modules-1-3",level:3},{value:"Recommended Tools",id:"recommended-tools",level:3},{value:"Optional Hardware",id:"optional-hardware",level:3},{value:"Key Concepts You&#39;ll Master",id:"key-concepts-youll-master",level:2},{value:"Vision-Language-Action (VLA)",id:"vision-language-action-vla",level:3},{value:"Multimodal Perception",id:"multimodal-perception",level:3},{value:"LLM-Driven Planning",id:"llm-driven-planning",level:3},{value:"Behavior Trees",id:"behavior-trees",level:3},{value:"Real-Time Control Integration",id:"real-time-control-integration",level:3},{value:"Safety-Critical Systems",id:"safety-critical-systems",level:3},{value:"Technical Stack",id:"technical-stack",level:2},{value:"Success Criteria",id:"success-criteria",level:2},{value:"Resources",id:"resources",level:2},{value:"Official Documentation",id:"official-documentation",level:3},{value:"Key Research Papers",id:"key-research-papers",level:3},{value:"Community Resources",id:"community-resources",level:3},{value:"Grading &amp; Evaluation",id:"grading--evaluation",level:2},{value:"Module Grading",id:"module-grading",level:3},{value:"Capstone Grading",id:"capstone-grading",level:3},{value:"Frequently Asked Questions",id:"frequently-asked-questions",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Contact &amp; Support",id:"contact--support",level:2},{value:"Module 4 Checklist",id:"module-4-checklist",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"module-4-visionlanguageaction-vla-robotics",children:"Module 4: Vision\u2013Language\u2013Action (VLA) Robotics"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capstone Module"})," | ",(0,r.jsx)(n.strong,{children:"Duration"}),": 4 weeks (Weeks 10-13) | ",(0,r.jsx)(n.strong,{children:"Prerequisite"}),": Modules 1-3"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,r.jsxs)(n.p,{children:["Module 4 is the ",(0,r.jsx)(n.strong,{children:"capstone"})," of the Physical AI & Humanoid Robotics Textbook. You'll build a complete Vision-Language-Action (VLA) system\u2014integrating perception, language understanding, and robot control\u2014to create a humanoid that responds to natural language voice commands and executes complex household tasks."]}),"\n",(0,r.jsx)(n.h3,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,r.jsx)(n.p,{children:"By the end of Module 4, your robot will:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perceive"})," its environment using RGB + depth sensors (Chapter 2)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Understand"})," voice commands via speech-to-text (Chapter 3)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reason"})," about tasks using Large Language Models (Chapter 3)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Plan"})," multi-step action sequences (Chapter 3)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Execute"})," navigation and manipulation in real-time (Chapter 4)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recover"})," from failures with automatic replanning (Chapter 4)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Operate safely"})," with watchdogs and emergency stops (Chapter 4)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"After completing Module 4, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Design and implement"})," complete VLA architectures for autonomous humanoids"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrate perception, language, and control"})," into cohesive systems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deploy optimized systems"})," to resource-constrained platforms (Jetson hardware)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explain the role"})," of vision, language, and action in embodied AI"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Evaluate VLA systems"})," against technical rubrics (perception accuracy, planning quality, control robustness, safety)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Build and deploy"})," real-world autonomous robots that understand natural language"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"module-chapters",children:"Module Chapters"}),"\n",(0,r.jsx)(n.h3,{id:"chapter-1-introduction-to-vision-language-action-robotics",children:(0,r.jsx)(n.strong,{children:"Chapter 1: Introduction to Vision-Language-Action Robotics"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Difficulty: Beginner | Time: 8-10 hours"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Understand the VLA paradigm and why it enables autonomous humanoids."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Topics"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"What is VLA? (Three pillars: Vision, Language, Action)"}),"\n",(0,r.jsx)(n.li,{children:"Cognitive robotics vs. classical robotics"}),"\n",(0,r.jsx)(n.li,{children:"Real-world VLA systems (OpenAI RT-2, DeepMind RT-X, NVIDIA VLMs)"}),"\n",(0,r.jsx)(n.li,{children:"Task representation: skills, behaviors, action graphs"}),"\n",(0,r.jsx)(n.li,{children:"Behavior trees for hierarchical task control"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Deliverables"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Lab 1: Analyze a real VLA system (OpenAI Robotics or RT-X)"}),"\n",(0,r.jsx)(n.li,{children:"Lab 2: Design a VLA architecture for household cleanup"}),"\n",(0,r.jsx)(n.li,{children:"Lab 3: Evaluate VLA design trade-offs"}),"\n",(0,r.jsx)(n.li,{children:"3 progressive exercises"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capstone Connection"}),": Establishes the architectural framework for your capstone system."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"chapter-2-vision-for-vla--building-perception-pipelines",children:(0,r.jsx)(n.strong,{children:"Chapter 2: Vision for VLA \u2013 Building Perception Pipelines"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Difficulty: Intermediate | Time: 8-10 hours"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Build real-time perception systems that understand scenes for robot reasoning."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Topics"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Object detection (YOLO, Mask R-CNN, SAM)"}),"\n",(0,r.jsx)(n.li,{children:"Grounding vision-language models (Grounding DINO)"}),"\n",(0,r.jsx)(n.li,{children:"Affordance detection (what objects can the robot interact with?)"}),"\n",(0,r.jsx)(n.li,{children:"RGB-D fusion (combining color and depth)"}),"\n",(0,r.jsx)(n.li,{children:"Multi-view camera fusion"}),"\n",(0,r.jsx)(n.li,{children:"Scene graphs (structured scene representations for LLMs)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Deliverables"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Lab 1: Detect objects in scenes with YOLO (90%+ accuracy)"}),"\n",(0,r.jsx)(n.li,{children:"Lab 2: Identify pickable/graspable objects with affordance classifier"}),"\n",(0,r.jsx)(n.li,{children:"Lab 3: Convert scenes to LLM-readable natural language descriptions"}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 perception node code examples"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capstone Connection"}),": Perception node provides scene understanding for LLM planning."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"chapter-3-language-planning-with-whisper--large-language-models",children:(0,r.jsx)(n.strong,{children:"Chapter 3: Language Planning with Whisper & Large Language Models"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Difficulty: Intermediate | Time: 8-10 hours"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Use language models to decompose voice commands into executable robot skills."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Topics"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Whisper speech-to-text API (voice input with >95% accuracy)"}),"\n",(0,r.jsx)(n.li,{children:"Prompt engineering for robotics (crafting LLM inputs)"}),"\n",(0,r.jsx)(n.li,{children:"Chain-of-thought reasoning (LLM step-by-step task breakdown)"}),"\n",(0,r.jsx)(n.li,{children:"Task decomposition (natural language \u2192 skill sequences)"}),"\n",(0,r.jsx)(n.li,{children:"Mock and real LLM integration"}),"\n",(0,r.jsx)(n.li,{children:"Behavior trees for task planning"}),"\n",(0,r.jsx)(n.li,{children:"Replanning and failure recovery"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Deliverables"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Lab 1: Test Whisper speech recognition on household commands"}),"\n",(0,r.jsx)(n.li,{children:"Lab 2: Decompose tasks using mock LLM (core labs)"}),"\n",(0,r.jsx)(n.li,{children:"Lab 3: Implement replanning logic for failure recovery"}),"\n",(0,r.jsx)(n.li,{children:"Lab 4: Real LLM integration with OpenAI/Claude APIs (bonus)"}),"\n",(0,r.jsx)(n.li,{children:"Code examples for Whisper node, LLM decomposer, replanning engine"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capstone Connection"}),": Planning module converts voice \u2192 task plans for execution."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"chapter-4-vla-control-architecture--deployment",children:(0,r.jsx)(n.strong,{children:"Chapter 4: VLA Control Architecture & Deployment"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Difficulty: Advanced | Time: 8-10 hours"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Integrate perception and planning into a unified control system."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Topics"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ROS 2 action servers (asynchronous goal-oriented communication)"}),"\n",(0,r.jsx)(n.li,{children:"Nav2 navigation stack (autonomous mobile manipulation)"}),"\n",(0,r.jsx)(n.li,{children:"MoveIt manipulation (arm motion planning with collision avoidance)"}),"\n",(0,r.jsx)(n.li,{children:"VLA orchestrator (central controller coordinating all components)"}),"\n",(0,r.jsx)(n.li,{children:"Safety mechanisms (watchdogs, emergency stops)"}),"\n",(0,r.jsx)(n.li,{children:"Failure recovery (replanning on task failures)"}),"\n",(0,r.jsx)(n.li,{children:"Deployment optimization (model quantization, Jetson platforms)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Deliverables"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Lab 1: Build perception-aware navigation"}),"\n",(0,r.jsx)(n.li,{children:"Lab 2: Integrate LLM planning with ROS 2 control"}),"\n",(0,r.jsx)(n.li,{children:"Lab 3: Implement safety watchdogs and emergency stops"}),"\n",(0,r.jsx)(n.li,{children:"Lab 4: Design deployment optimization strategy"}),"\n",(0,r.jsx)(n.li,{children:"VLA orchestrator code"}),"\n",(0,r.jsx)(n.li,{children:"Safety watchdog implementation"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capstone Connection"}),": Control module executes full perception \u2192 planning \u2192 action pipeline."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"week-13-vla-capstone-sprint--integration",children:(0,r.jsx)(n.strong,{children:"Week 13: VLA Capstone Sprint & Integration"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.em,{children:"Difficulty: Advanced | Time: 40-50 hours (2-week sprint)"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Build and evaluate a complete end-to-end VLA system."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Capstone Project"}),': "Embodied AI Household Assistant"']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What You'll Build"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Complete VLA pipeline in simulation (Isaac Lab)"}),"\n",(0,r.jsx)(n.li,{children:"Voice-controlled household task execution"}),"\n",(0,r.jsx)(n.li,{children:"Real-time perception, planning, and control"}),"\n",(0,r.jsx)(n.li,{children:"Safety-critical systems with watchdogs and recovery"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Evaluation Rubric"})," (4 dimensions):"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perception"})," (25%): Object detection accuracy ",(0,r.jsx)(n.code,{children:">90%"}),", affordance extraction, ",(0,r.jsx)(n.code,{children:"<1"}),"00ms` latency"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LLM Planning"})," (25%): Multi-step task decomposition, robust fallbacks, semantic understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Control & Integration"})," (35%): Full pipeline execution, ",(0,r.jsx)(n.code,{children:"<3"}),"3ms` control loop, failure recovery"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety"})," (15%): Watchdogs, emergency stops, collision avoidance, proven stress tests"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Bonus Challenge"}),": Deploy to real Jetson hardware with quantized models and benchmark real-time performance."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Deliverables"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Complete ROS 2 system with all components"}),"\n",(0,r.jsx)(n.li,{children:"Technical documentation and architecture diagrams"}),"\n",(0,r.jsx)(n.li,{children:"Evaluation results with metrics"}),"\n",(0,r.jsx)(n.li,{children:"5-minute demo video"}),"\n",(0,r.jsx)(n.li,{children:"Capstone report"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"module-learning-path",children:"Module Learning Path"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Week 10: Chapters 1-2\n\u251c\u2500 Mon-Wed: Chapter 1 (VLA fundamentals)\n\u2514\u2500 Wed-Fri: Chapter 2 (Perception pipeline)\n\nWeek 11: Chapter 2-3\n\u251c\u2500 Mon-Wed: Chapter 2 (Complete perception)\n\u2514\u2500 Wed-Fri: Chapter 3 (Whisper & LLMs)\n\nWeek 12: Chapters 3-4\n\u251c\u2500 Mon-Wed: Chapter 3 (Complete planning)\n\u2514\u2500 Wed-Fri: Chapter 4 (Control architecture)\n\nWeek 13: Capstone Sprint (Days 1-10)\n\u251c\u2500 Days 1-2: Perception sprint\n\u251c\u2500 Days 3-4: Planning sprint\n\u251c\u2500 Days 5-6: Control integration\n\u251c\u2500 Days 7-8: Full pipeline testing\n\u251c\u2500 Day 9: Evaluation & documentation\n\u2514\u2500 Days 10+: Refinement, bonus features, hardware (optional)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.h3,{id:"required-knowledge-from-modules-1-3",children:"Required Knowledge (from Modules 1-3)"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ROS 2 fundamentals (publishers, subscribers, services, actions)"}),"\n",(0,r.jsx)(n.li,{children:"Digital twin simulation (Gazebo or Isaac Sim)"}),"\n",(0,r.jsx)(n.li,{children:"Python 3.10+ programming"}),"\n",(0,r.jsx)(n.li,{children:"Basic robotics concepts (kinematics, control, navigation)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"recommended-tools",children:"Recommended Tools"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ROS 2 Humble (installed and configured)"}),"\n",(0,r.jsx)(n.li,{children:"NVIDIA Isaac Lab (for simulation) or Gazebo Garden"}),"\n",(0,r.jsx)(n.li,{children:"Python with PyTorch, OpenCV, numpy"}),"\n",(0,r.jsx)(n.li,{children:"Git and GitHub (for version control)"}),"\n",(0,r.jsx)(n.li,{children:"Docker (for reproducible environments)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"optional-hardware",children:"Optional Hardware"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Jetson platform (Orin Nano, Orin NX) for hardware deployment"}),"\n",(0,r.jsx)(n.li,{children:"Real humanoid robot (e.g., Tesla Bot, Figure 01, Digit)"}),"\n",(0,r.jsx)(n.li,{children:"USB microphone for voice input"}),"\n",(0,r.jsx)(n.li,{children:"RGB-D camera (Intel RealSense D435)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-concepts-youll-master",children:"Key Concepts You'll Master"}),"\n",(0,r.jsx)(n.h3,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"}),"\n",(0,r.jsx)(n.p,{children:"An integrated architecture where robots perceive their environment, reason about goals using language, and execute actions in real-time. VLA enables natural language control of complex autonomous behaviors."}),"\n",(0,r.jsx)(n.h3,{id:"multimodal-perception",children:"Multimodal Perception"}),"\n",(0,r.jsx)(n.p,{children:"Combining RGB images, depth maps, and segmentation outputs to build rich scene understanding for robot reasoning."}),"\n",(0,r.jsx)(n.h3,{id:"llm-driven-planning",children:"LLM-Driven Planning"}),"\n",(0,r.jsx)(n.p,{children:"Using Large Language Models to decompose natural language commands into executable robot skill sequences\u2014enabling robots to generalize to novel tasks."}),"\n",(0,r.jsx)(n.h3,{id:"behavior-trees",children:"Behavior Trees"}),"\n",(0,r.jsx)(n.p,{children:"Hierarchical representations of robot behaviors using composable nodes (selectors, sequences, actions)\u2014enabling modular, reusable task control."}),"\n",(0,r.jsx)(n.h3,{id:"real-time-control-integration",children:"Real-Time Control Integration"}),"\n",(0,r.jsxs)(n.p,{children:["Tight synchronization of perception, planning, and control loops with latency budgets (e.g., ",(0,r.jsx)(n.code,{children:"<3"}),"3ms for 30 Hz real-time operation)."]}),"\n",(0,r.jsx)(n.h3,{id:"safety-critical-systems",children:"Safety-Critical Systems"}),"\n",(0,r.jsx)(n.p,{children:"Watchdogs, emergency stops, and failure recovery mechanisms that ensure safe operation in human environments."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"technical-stack",children:"Technical Stack"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Technologies"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Perception"})}),(0,r.jsx)(n.td,{children:"YOLO, Mask R-CNN, SAM, Grounding DINO"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Speech-to-Text"})}),(0,r.jsx)(n.td,{children:"OpenAI Whisper API"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Language Models"})}),(0,r.jsx)(n.td,{children:"GPT-4, Claude, open-source LLMs"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Robot Control"})}),(0,r.jsx)(n.td,{children:"ROS 2 Humble, Nav2, MoveIt"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Simulation"})}),(0,r.jsx)(n.td,{children:"NVIDIA Isaac Lab, Gazebo Garden"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Deployment"})}),(0,r.jsx)(n.td,{children:"Jetson Orin (TensorRT), Docker"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Programming"})}),(0,r.jsx)(n.td,{children:"Python 3.10+, C++ (optional)"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"success-criteria",children:"Success Criteria"}),"\n",(0,r.jsx)(n.p,{children:"By completing Module 4, you will have:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Implemented a complete VLA system (perception + planning + control)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Demonstrated end-to-end task execution from voice commands"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Evaluated your system against a rigorous technical rubric"}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 Deployed to simulation with real-time performance (",(0,r.jsx)(n.code,{children:"<5"}),"0ms control loop)"]}),"\n",(0,r.jsx)(n.li,{children:"\u2705 (Bonus) Deployed to physical Jetson hardware"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Documented your system architecture and evaluation results"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Presented your capstone project with demo video and report"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,r.jsx)(n.h3,{id:"official-documentation",children:"Official Documentation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"ROS 2 Humble Docs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://isaac-sim.github.io/",children:"NVIDIA Isaac Lab Docs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://moveit.readthedocs.io/",children:"MoveIt2 Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://nav2.org/",children:"Nav2 Documentation"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"key-research-papers",children:"Key Research Papers"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Brohan et al. (2023): "RT-2: Vision-Language-Action Models" - arXiv:2307.15818'}),"\n",(0,r.jsx)(n.li,{children:'Padalkar et al. (2024): "RT-X: Robotics Transformer" - DeepMind Blog'}),"\n",(0,r.jsx)(n.li,{children:'Kirillov et al. (2023): "Segment Anything" - arXiv:2304.02135'}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"community-resources",children:"Community Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["ROS2 Discourse: ",(0,r.jsx)(n.a,{href:"https://discourse.ros.org/",children:"https://discourse.ros.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Robotics Stack Exchange: ",(0,r.jsx)(n.a,{href:"https://robotics.stackexchange.com/",children:"https://robotics.stackexchange.com/"})]}),"\n",(0,r.jsx)(n.li,{children:"GitHub: ROS2, Isaac Lab, MoveIt projects"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"grading--evaluation",children:"Grading & Evaluation"}),"\n",(0,r.jsx)(n.h3,{id:"module-grading",children:"Module Grading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chapter Assignments"})," (40%): Labs, exercises, comprehension"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Capstone Project"})," (60%):","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Perception subsystem (25% of capstone)"}),"\n",(0,r.jsx)(n.li,{children:"Planning subsystem (25% of capstone)"}),"\n",(0,r.jsx)(n.li,{children:"Control & integration (35% of capstone)"}),"\n",(0,r.jsx)(n.li,{children:"Safety mechanisms (15% of capstone)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"capstone-grading",children:"Capstone Grading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Excellent"})," (A: 95-100%): All systems functional, advanced features, robust deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Good"})," (B: 85-94%): Core systems work, most features implemented"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fair"})," (C: 75-84%): Basic functionality, limited features"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Below Fair"})," (D/F: ",(0,r.jsx)(n.code,{children:"<7"}),"5%): Incomplete or non-functional"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"frequently-asked-questions",children:"Frequently Asked Questions"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: Do I need real robot hardware?"}),"\nA: No! The capstone can be completed entirely in simulation. Hardware is optional (bonus challenge)."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: What if I've never worked with LLMs before?"}),"\nA: Chapter 3 teaches prompting from scratch. No prior ML experience required."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: How much time should I spend per week?"}),"\nA: Plan 40-50 hours over 4 weeks (~10-12 hours per week). The capstone sprint (Week 13) is intensive."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: Can I work in teams?"}),"\nA: Yes! Capstone projects can be team efforts (2-3 people). Specify team composition in submission."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Q: What if my perception accuracy isn't >90%?"}),"\nA: The rubric rewards effort and reasoning. Document your challenges and proposed solutions."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Start with Chapter 1"})," to understand VLA concepts and architectures"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Build perception"})," (Chapter 2) and test on sample scenes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement planning"})," (Chapter 3) with mock LLM first"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrate control"})," (Chapter 4) to create the full pipeline"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Execute the capstone sprint"})," (Week 13) to build and evaluate your system"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"contact--support",children:"Contact & Support"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Instructors"}),": Available for office hours and Q&A"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Course Discord"}),": Real-time help and peer discussion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Discussion Board"}),": Async questions and answers"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"module-4-checklist",children:"Module 4 Checklist"}),"\n",(0,r.jsx)(n.p,{children:"As you complete each chapter, mark these off:"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Chapter 1: VLA Fundamentals (Learn the concepts)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Chapter 2: Perception (Build object detection + scene understanding)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Chapter 3: Language Planning (Implement Whisper + LLM decomposition)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Chapter 4: Control Architecture (Wire everything together)"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Week 13 Capstone: Build, evaluate, and present your VLA system"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Welcome to Module 4: Vision-Language-Action Robotics!"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"This is where it all comes together."})," You'll build state-of-the-art autonomous systems that understand natural language and execute complex real-world tasks. Let's get started!"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"/ai_native-textbook/docs/module-4/chapter-1-vla-intro",children:"Start with Chapter 1: Introduction to Vision-Language-Action Robotics \u2192"})})})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(6540);const r={},l=s.createContext(r);function t(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);