"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_textbook=self.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[7223],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},9184:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/chapter-3-language-planning-whisper-llm","title":"Chapter 3: Language Planning with Whisper & Large Language Models","description":"Module 12 | Difficulty 8\u201310 hours","source":"@site/docs/module-4/chapter-3-language-planning-whisper-llm.md","sourceDirName":"module-4","slug":"/module-4/chapter-3-language-planning-whisper-llm","permalink":"/ai_native-textbook/docs/module-4/chapter-3-language-planning-whisper-llm","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Vision for VLA \u2013 Building Perception Pipelines","permalink":"/ai_native-textbook/docs/module-4/chapter-2-vision-for-vla"},"next":{"title":"Chapter 4: VLA Control Architecture & Deployment","permalink":"/ai_native-textbook/docs/module-4/chapter-4-vla-control-architecture"}}');var r=i(4848),t=i(8453);const o={},a="Chapter 3: Language Planning with Whisper & Large Language Models",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Part 1: Speech Recognition with Whisper",id:"part-1-speech-recognition-with-whisper",level:2},{value:"Whisper Overview",id:"whisper-overview",level:3},{value:"Whisper Architecture",id:"whisper-architecture",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:3},{value:"Handling Noisy Input",id:"handling-noisy-input",level:3},{value:"Part 2: Large Language Models for Task Decomposition",id:"part-2-large-language-models-for-task-decomposition",level:2},{value:"LLMs for Robotics",id:"llms-for-robotics",level:3},{value:"Prompting Strategies",id:"prompting-strategies",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-3-language-planning-with-whisper--large-language-models",children:"Chapter 3: Language Planning with Whisper & Large Language Models"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Module"}),": 4 | ",(0,r.jsx)(n.strong,{children:"Week"}),": 12 | ",(0,r.jsx)(n.strong,{children:"Difficulty"}),": Intermediate | ",(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 8\u201310 hours"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"After completing this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrate Whisper speech-to-text"})," API to convert voice commands to text"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Design effective LLM prompts"})," for robotics task decomposition using chain-of-thought reasoning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Parse LLM outputs"})," into executable skill sequences and detect ambiguities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement task decomposition"})," that converts natural language to structured action plans"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Design and implement replanning logic"})," to recover from task failures and adapt to dynamic environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explain behavior trees"})," as hierarchical task representations and implement basic behavior tree engines"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speech Recognition"})," (Whisper): Converting audio to text with >95% accuracy across languages and accents"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Prompt Engineering"}),": Crafting LLM inputs to reliably produce structured, robotics-compatible outputs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chain-of-Thought Reasoning"}),": Breaking complex tasks into step-by-step reasoning before execution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Task Decomposition"}),": Converting high-level goals into low-level executable skills"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Behavior Trees"}),": Hierarchical, modular representations of robot behaviors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Replanning"}),": Detecting failures and generating alternative action sequences"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tool Use"}),": Teaching LLMs to call functions (e.g., moveto, pick, place) as part of reasoning"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-1-speech-recognition-with-whisper",children:"Part 1: Speech Recognition with Whisper"}),"\n",(0,r.jsx)(n.h3,{id:"whisper-overview",children:"Whisper Overview"}),"\n",(0,r.jsx)(n.p,{children:"Whisper is OpenAI's speech-to-text model that achieves robust, multi-lingual speech recognition. Key features:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": >95% on clear speech, ~80% on noisy audio (significantly better than before ~2024)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Languages"}),": Trained on 99 languages (multilingual capability)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Sizes"}),": tiny (39M) \u2192 base (74M) \u2192 small (244M) \u2192 medium (769M) \u2192 large (1.5B)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency"}),":","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Tiny/base: ~100ms per 30 seconds of audio (real-time capable)"}),"\n",(0,r.jsx)(n.li,{children:"Large: ~500ms per 30 seconds (sufficient for planning, not control-loop speed)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": API pricing ~$0.02 per minute of audio (free if self-hosted)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"whisper-architecture",children:"Whisper Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'Audio Input (16kHz PCM, mono)\n    \u2193\n[Mel Spectrogram Encoder]  \u2192 512-dim features per 20ms\n    \u2193\n[Transformer Encoder]  \u2192 Processes temporal context\n    \u2193\n[BOS token] \u2192 Start decoding\n    \u2193\n[Transformer Decoder]  \u2192 Generates text tokens autoregressively\n    \u2193\nText Output ("pick up the red cup")\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Whisper's key innovation: ",(0,r.jsx)(n.strong,{children:"Trained on weakly-supervised data from the web"})," (video captions), making it robust to diverse audio conditions, accents, and background noise."]}),"\n",(0,r.jsx)(n.h3,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# File: ros2_whisper_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Audio  # Audio message type\nfrom std_msgs.msg import String\nimport numpy as np\nfrom openai import OpenAI\nimport threading\n\nclass WhisperNode(Node):\n    """\n    ROS 2 node that:\n    1. Subscribes to audio topic\n    2. Buffers audio (e.g., 5 seconds)\n    3. Sends to Whisper API\n    4. Publishes transcribed text\n    """\n\n    def __init__(self):\n        super().__init__(\'whisper_node\')\n\n        # Initialize OpenAI client\n        self.client = OpenAI(api_key=os.getenv(\'OPENAI_API_KEY\'))\n\n        # Subscribers\n        self.audio_subscription = self.create_subscription(\n            String,  # In practice, you\'d use a custom Audio message\n            \'/microphone/audio_bytes\',\n            self.audio_callback,\n            10\n        )\n\n        # Publishers\n        self.transcription_publisher = self.create_publisher(\n            String, \'/whisper/transcription\', 10\n        )\n\n        # Audio buffer\n        self.audio_buffer = np.array([], dtype=np.int16)\n        self.sample_rate = 16000  # 16 kHz\n        self.buffer_duration = 5.0  # 5 seconds\n        self.buffer_size = int(self.sample_rate * self.buffer_duration)\n\n        self.get_logger().info("Whisper node initialized")\n\n    def audio_callback(self, msg):\n        """Buffer incoming audio."""\n        # Parse audio from message (format depends on message type)\n        # For simplicity, assume msg.data is base64-encoded PCM\n        import base64\n        audio_bytes = base64.b64decode(msg.data)\n        audio_chunk = np.frombuffer(audio_bytes, dtype=np.int16)\n\n        # Append to buffer\n        self.audio_buffer = np.append(self.audio_buffer, audio_chunk)\n\n        # If buffer is full, process\n        if len(self.audio_buffer) >= self.buffer_size:\n            self._process_audio_buffer()\n\n    def _process_audio_buffer(self):\n        """Send buffered audio to Whisper API."""\n        # Convert to audio format (WAV file in memory)\n        import io\n        import wave\n\n        buffer_to_process = self.audio_buffer[:self.buffer_size]\n        self.audio_buffer = self.audio_buffer[self.buffer_size:]  # Clear buffer\n\n        # Create WAV file\n        wav_buffer = io.BytesIO()\n        with wave.open(wav_buffer, \'wb\') as wav_file:\n            wav_file.setnchannels(1)  # Mono\n            wav_file.setsampwidth(2)  # 16-bit\n            wav_file.setframerate(self.sample_rate)\n            wav_file.writeframes(buffer_to_process.tobytes())\n\n        wav_buffer.seek(0)\n\n        # Send to Whisper API\n        try:\n            response = self.client.audio.transcriptions.create(\n                model="whisper-1",\n                file=wav_buffer,\n                language="en"\n            )\n\n            transcript = response.text\n            self.get_logger().info(f"Transcribed: {transcript}")\n\n            # Publish\n            msg = String()\n            msg.data = transcript\n            self.transcription_publisher.publish(msg)\n\n        except Exception as e:\n            self.get_logger().error(f"Whisper API error: {e}")\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"handling-noisy-input",children:"Handling Noisy Input"}),"\n",(0,r.jsx)(n.p,{children:"Whisper is robust, but can still misrecognize. Strategies:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Confidence Thresholding"}),": Only accept high-confidence transcriptions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fallback Mechanisms"}),": If uncertain, ask user for confirmation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context"}),": Use previous commands to correct likely mistakes"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def transcribe_with_fallback(audio_bytes, confidence_threshold=0.8):\n    """\n    Transcribe with fallback to user confirmation.\n    """\n    # Get Whisper transcription\n    transcript = whisper_api.transcribe(audio_bytes)\n\n    # If confidence is low, ask user\n    if transcript.confidence < confidence_threshold:\n        return f"Did you say: {transcript.text}? (yes/no)"\n    else:\n        return transcript.text\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"part-2-large-language-models-for-task-decomposition",children:"Part 2: Large Language Models for Task Decomposition"}),"\n",(0,r.jsx)(n.h3,{id:"llms-for-robotics",children:"LLMs for Robotics"}),"\n",(0,r.jsx)(n.p,{children:"Large Language Models (GPT-4, Claude, etc.) excel at task reasoning. For robotics:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Strengths"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Natural language understanding (parsing varied commands)"}),"\n",(0,r.jsx)(n.li,{children:"Reasoning (breaking complex tasks into steps)"}),"\n",(0,r.jsx)(n.li,{children:"Generalization (handling novel scenarios)"}),"\n",(0,r.jsx)(n.li,{children:"Flexibility (no pre-programmed task library needed)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Limitations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Latency (seconds per response, too slow for control loops)"}),"\n",(0,r.jsx)(n.li,{children:"Cost (API calls)"}),"\n",(0,r.jsx)(n.li,{children:"Hallucination (may propose infeasible actions)"}),"\n",(0,r.jsx)(n.li,{children:"Context length (can't maintain very long task histories)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Best Practice"}),": Use LLMs for ",(0,r.jsx)(n.strong,{children:"planning and high-level reasoning"}),", not for real-time control."]}),"\n",(0,r.jsx)(n.h3,{id:"prompting-strategies",children:"Prompting Strategies"}),"\n",(0,r.jsx)(n.p,{children:"Effective prompts for robot task decomposition include:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"System Prompt"}),": Define the robot's capabilities and constraints"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context"}),": Describe the scene (from perception module)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Task"}),": The user's command"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output Format"}),": Structured JSON with step-by-step actions"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example System Prompt"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'You are a task planner for a humanoid robot. The robot has the following capabilities:\n\nSkills:\n- MoveTo(location): Navigate to a 2D position\n- Pick(object, gripper_force): Grasp an object\n- Place(location, release_speed): Release object at location\n- Search(object_name): Find object in environment\n- Open(object): Open doors, drawers, containers\n- Close(object): Close doors, drawers\n\nConstraints:\n- Robot can only carry one object at a time\n- Gripper force must be: soft (0.5), moderate (1.0), or firm (1.5)\n- Cannot move through walls or locked doors\n- Max reach distance: 1.0m from base\n\nWhen given a user command, decompose it into a sequence of skills.\nReturn JSON format: {"steps": [{"skill": "...", "params": {...}}, ...]}\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Example Prompt Session"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'System: [System prompt above]\n\nUser: "Pick up the red cup from the table and place it on the shelf"\n\nContext: Scene has: red cup at (0.5, 0.3), table at (0.5, 0.0), shelf at (0.8, 1.0)\n\nTask: Decompose the command into robot skills.\n'})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);